
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['FedMPQ-baseline', 'FedMPQ-KD α=0.5,T=2 (default)', 'FedMPQ-KD α=1.0,T=2 (high-KD)', 'FedKD-global-teacher', 'Q-FedAvg']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "FedMPQ suffers a noticeable accuracy drop when the number of local epochs per communication round is small (e.g., 1).  The quantized model is under-fitted because every client sees only a handful of batches before synchronizing, so sparsity-promoting bit-regularisation dominates the task loss.\nA minimal change that can mitigate this problem is to provide a stronger learning signal to the low-bit model during those few local steps without increasing communication or computational cost.",
    "Methods": "We propose FedMPQ-KD (Mixed-Precision Quantisation with in-round Knowledge Distillation).\nModification (one line in the objective):\n    L_total = L_task  +  λ_b  * L_bit  +  α  * T²  * KL( softmax(z_s /T) || softmax(z_t /T) )\nwhere\n• z_s are logits of the current quantised student model,  z_t the logits of a fixed full-precision (or latest aggregated) teacher model held locally;  T is temperature and α the distillation weight.\n\nProcedure per client (changes in bold):\n1. Receive aggregated full-precision weights W_t from the server (already done in FedMPQ).\n2. Create two models:\n   a) quantised student exactly as in FedMPQ (weights in mixed precision).\n   b) ****freeze a copy of W_t in full precision as teacher****.\n3. For E local epochs, optimise the student with the extended loss above.  The teacher only produces logits; no back-prop.\n4. Send student weight updates as usual.\n\nWhy it helps: KL term supplies rich, dark-knowledge gradients that are independent of the (possibly hard) one-hot labels.  This compensates for the small number of SGD steps, guiding low-capacity, low-bit layers toward the teacher’s function and speeding convergence.  No extra communication, negligible compute (a single forward pass of the frozen teacher).",
    "Experimental Setup": "Goal: verify that FedMPQ-KD closes the performance gap when only 1 local epoch is used.\n• Dataset: CIFAR-10 (10 clients, α=0.5 Dirichlet split).\n• Network: ResNet-20.\n• Budgets: average 4-bit, mixed across layers as in the original paper.\n• Baselines:  (1) FedMPQ (original)  (2) FedMPQ-KD (ours).\n• Hyper-parameters:  λ_b =0.01 (unchanged),  α=0.5,  T=2.\n• Training: 50 communication rounds, 1 local epoch, batch-size 64, SGD lr 0.1.\n• Metric: global test accuracy after every round.\nExpected observation window: accuracy vs. rounds and final accuracy.",
    "Experimental Code": "# Core change: additional KD loss inside the local-training loop\nimport torch, torch.nn as nn, torch.nn.functional as F\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    \"\"\"KL divergence with temperature.\"\"\"\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\n\nclass LocalTrainer:\n    def __init__(self, student, teacher, dataloader, lr=0.1, lambda_b=0.01, alpha=0.5):\n        self.student = student  # quantised weights already applied\n        self.teacher = teacher.eval()  # full-precision copy, frozen\n        self.opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n        self.dl = dataloader\n        self.lambda_b = lambda_b\n        self.alpha = alpha\n\n    def train_one_epoch(self):\n        self.student.train()\n        for x, y in self.dl:\n            logits_s = self.student(x)\n            logits_t = self.teacher(x)\n            loss_task = F.cross_entropy(logits_s, y)\n            loss_bit  = self.student.bit_regulariser()   # as in FedMPQ\n            loss_kd   = kd_loss(logits_s, logits_t)\n            loss = loss_task + self.lambda_b*loss_bit + self.alpha*loss_kd\n            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "Expected Result": "With only 1 local epoch, FedMPQ reaches ≈82% CIFAR-10 accuracy after 50 rounds (reported drop of ~3-4% from full baseline).  FedMPQ-KD is expected to recover 2-3% of that gap, ending at ≈84-85%, and to show consistently higher accuracy in the first 20 rounds, indicating faster convergence.",
    "Expected Conclusion": "A single extra knowledge-distillation term supplies richer gradients to the quantised student during scarce local training, reducing under-fitting without additional communication or heavy computation.  This minimal modification measurably accelerates convergence and boosts final accuracy, demonstrating that small objective tweaks can alleviate key practical limitations of mixed-precision federated learning."
}

# Experimental Design
- Strategy: Objective: Build a single, coherent framework that will let every subsequent experiment answer one overarching question – does FedMPQ-KD provide a strictly better accuracy-vs-cost trade-off than competing quantised FL schemes across realistic operating conditions?

1. Core Hypotheses to Validate
   a. Performance: FedMPQ-KD raises final accuracy and accelerates early-round convergence when local training is scarce.
   b. Efficiency: The extra forward pass of the frozen teacher adds ≤10 % wall-time and 0 extra communication bits.
   c. Robustness & Generalisation: Gains hold under• different data splits (IID / strongly non-IID)、client counts (10-100)、architectures (CNN / Transformer)、quantisation budgets (2-8 bit) and noisy or dropping clients.
   d. Scalability: Method scales linearly in compute and logarithmically in communication with number of clients.

2. Comparison Palette (identical across all experiments)
   • Baselines: (1) FedMPQ (original) (2) Full-precision FedAvg (upper bound) (3) SOTA low-bit FL alternatives (FedPQ, FedKD, Q-FedAvg).
   • Ablations: i) –KD (λ_b+L_task only) ii) varying α, T iii) updating vs freezing the teacher iv) local vs global teacher refresh frequency.

3. Evaluation Angles
   3.1 Quantitative Performance
       – Top-1 accuracy vs communication rounds (primary)
       – Area-under-curve (AUC_acc) to capture convergence speed
       – Final accuracy gap to full-precision
   3.2 Cost Metrics
       – Client-side FLOPs & wall-clock per round (measured on A100)
       – Communication bits / round (should be unchanged)
       – Peak GPU VRAM & host RAM
   3.3 Robustness Metrics
       – Accuracy variance across 3 random seeds
       – Degradation (% drop) under extreme non-IID, stragglers, label noise
   3.4 Qualitative / Diagnostic
       – t-SNE of penultimate-layer features (student vs teacher)
       – Bit-width utilisation histograms
       – Gradient-norm and loss-landscape snapshots to illustrate stronger signals.

4. Experimental Axes (each future experiment picks one axis while keeping the rest fixed)
   A. Local compute budget: E∈{1,2,5}
   B. Quantisation budget: avg bits ∈{2,4,8}
   C. Data heterogeneity: Dirichlet α∈{0.1,0.5,∞}
   D. Model family: ResNet-20, MobileNet-V2, ViT-Small
   E. Scale: clients ∈{10,50,100}; participation rate 10-100 %
   F. Failure modes: 20 % clients randomly drop per round; 10 % label noise.

5. Success Criteria (must hold in ≥80 % of experimental settings)
   • +≥2 % absolute test accuracy OR ≥25 % smaller gap to full-precision compared with best quantised baseline, p<0.05 (t-test over seeds).
   • No statistically significant increase in communication volume.
   • Added wall-time ≤10 % per round on A100.
   • Under worst-case non-IID, KD variant still outperforms FedMPQ by ≥1 %.

6. Protocol & Reproducibility
   • Same optimiser, LR schedule, batch size across methods; only α and T tuned once on a small validation pool and kept fixed.
   • Three independent seeds; report mean±std.
   • Use PyTorch w/ deterministic flags; log hardware utilisation via NVIDIA-smi and PyTorch profiler.
   • Release code & logs; each run stores JSON metadata capturing hyper-params, seed, commit hash.

7. Resource Awareness
   • Single NVIDIA A100 (80 GB) easily fits teacher + student (≤4 GB each). Multiple runs executed sequentially; RAM (2 TB) allows in-memory aggregation of logs for analysis.

By following this common strategy—fixed baselines, uniform metrics, multifaceted validation axes, and rigid success criteria—each forthcoming experiment will contribute a comparable slice of evidence, together establishing the robustness, efficiency and overall superiority of FedMPQ-KD.

# Current Experiment (to generate code for)
- Experiment ID: exp-2-robust-eff
- Description: Objective / Hypothesis: Stress-test FedMPQ-KD under harsh FL conditions (100 clients, extreme non-IID, client dropouts) and quantify efficiency gains on lightweight models. We expect KD variants to maintain ≥1 % accuracy lead and stable convergence with negligible cost overhead.

Models:
 • Student: MobileNet-V2 (avg 4-bit mixed precision).
 • Teacher: frozen copy of latest aggregated full-precision MobileNet-V2 (updated every round).
 • Alternative baseline: FedKD – classical KD that exchanges softened logits globally (communication overhead) – to highlight zero-cost advantage of in-round KD.

Datasets:
 • CIFAR-100 (100 classes) to test generalisation difficulty.
Pre-processing: standard CIFAR-100 normalisation, random crop 32→28, horizontal flip.
Data heterogeneity: Dirichlet α = 0.1 (strongly non-IID); plus 20 % of clients randomly drop out each round (stragglers).

Data split & sampling:
 • Each client keeps its shard; 5 % of local data for val, rest train.
 • Global test set untouched.

Training schedule:
 • Local epochs E = 2, batch 64, AdamW lr 3e-4.
 • Total 100 communication rounds, participation rate 20 % (random).
Seeds: 3; metrics averaged.
Checkpoint selection: best-val per seed (val accuracy on each client averaged) then test.

Evaluation metrics:
 • Primary – global top-1 test accuracy.
 • Secondary – convergence AUC_acc, robustness drop (%) when 20 % label noise injected at rnd 60, client-level accuracy variance (std across clients), FLOPs/round, GPU memory, wall-time, communication bits.
 • Calibration – Expected Calibration Error (ECE).

Robustness procedures:
 1. Noise injection: at round 60 flip 20 % of labels on 10 % random active clients; measure Δaccuracy.
 2. Distribution shift: after training, evaluate on CIFAR-100-C (corrupted) severity 3.
 3. Adversarial: FGSM ε=2/255 on test set to compare robustness scores.

Hyperparameter sensitivity: KD variants are re-run with α∈{0.25,0.5,1.0} on 1 seed; learning-rate sweep 1e-4–1e-3 to ensure fairness; results in appendix.

Efficiency analysis: record
 • Training wall-clock per round with PyTorch profiler
 • Inference latency (teacher + student) on 1 k images
 • Peak VRAM
 • FLOPs via fvcore
 • Communication volume – model update size; highlight that FedKD baseline sends extra logits (≈38 MB/client/round).

Example code excerpt for dropout handling:
```
active = random.sample(clients, int(len(clients)*0.2))
for c in active:
    if random.random()<0.2:  # straggler
        continue
    c.local_train()
```
Resource projection: 100 MobileNet-V2 students (quantised) sequentially processed -> 8 h total on single A100; fits 80 GB VRAM if 2 processes run concurrently.
Success criteria: Under all stressors FedMPQ-KD default ≥1 % absolute accuracy over FedMPQ and ≥25 % lower comm cost than FedKD; added wall-time ≤8 %.
- Run Variations: ['FedMPQ-baseline', 'FedMPQ-KD α=0.5,T=2 (default)', 'FedMPQ-KD α=1.0,T=2 (high-KD)', 'FedKD-global-teacher', 'Q-FedAvg']

# Base Code
{'train_py': 'import argparse\nimport json\nimport os\nimport shutil\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\n\nfrom .model import (FedMPQKDWrapper, FedMPQWrapper, get_base_model,\n                    kd_loss)\nfrom .preprocess import Preprocessor\n\n\ndef set_seed(seed: int):\n    """Make experiments deterministic as far as possible."""\n    import random\n    import torch.backends.cudnn as cudnn\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\ndef average_state_dicts(state_dicts: List[Dict[str, torch.Tensor]]):\n    """FedAvg aggregation (simple mean of model parameters)."""\n    avg_state = {}\n    for k in state_dicts[0]:\n        avg_state[k] = torch.stack([sd[k].float() for sd in state_dicts], 0).mean(0)\n    return avg_state\n\n\ndef evaluate(model: torch.nn.Module, dataloader, device):\n    """Return accuracy of MODEL on DATALOADER."""\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\ndef train_client(model_wrapper_cls, global_state, teacher_state, dataloader, device, cfg):\n    """Single client\'s local training.\n\n    Args:\n        model_wrapper_cls: FedMPQWrapper or FedMPQKDWrapper\n        global_state: parameters broadcast from server (full-precision)\n        teacher_state: state dict for teacher (only KD variant uses it)\n        dataloader: client data loader\n        device: torch device\n        cfg: experiment config dict\n    Returns:\n        state_dict after local optimisation\n    """\n    base_model = get_base_model(cfg["model"]["architecture"], cfg["data"]["num_classes"])\n    base_model.load_state_dict(global_state)\n    base_model = base_model.to(device)\n\n    if model_wrapper_cls is FedMPQKDWrapper:\n        teacher_model = get_base_model(cfg["model"]["architecture"], cfg["data"]["num_classes"])\n        teacher_model.load_state_dict(teacher_state)\n        teacher_model = teacher_model.to(device)\n        wrapper = model_wrapper_cls(base_model, teacher_model, cfg)\n    else:\n        wrapper = model_wrapper_cls(base_model, cfg)\n\n    wrapper.train()\n    optimizer = torch.optim.SGD(wrapper.parameters(), lr=cfg["training"]["lr"], momentum=0.9)\n\n    for _ in range(cfg["training"]["local_epochs"]):\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits_student, loss_bit = wrapper.forward_with_bit_loss(x)\n            loss_task = F.cross_entropy(logits_student, y)\n            if model_wrapper_cls is FedMPQKDWrapper:\n                logits_teacher = wrapper.teacher(x)\n                loss_kd = kd_loss(logits_student, logits_teacher, T=cfg["kd_params"]["T"])\n                loss = loss_task + cfg["model"]["lambda_b"] * loss_bit + cfg["kd_params"]["alpha"] * loss_kd\n            else:\n                loss = loss_task + cfg["model"]["lambda_b"] * loss_bit\n            loss.backward()\n            optimizer.step()\n    return wrapper.base_model.state_dict()\n\n\ndef run_experiment(cfg: Dict, results_dir: Path):\n    """Main federated loop for one run variation."""\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # 1. Data\n    preproc = Preprocessor(cfg)\n    client_loaders, test_loader = preproc.get_data_loaders()\n    num_clients = len(client_loaders)\n\n    # 2. Global model\n    global_model = get_base_model(cfg["model"]["architecture"], cfg["data"]["num_classes"])\n    global_model = global_model.to(device)\n\n    # 3. Training metadata containers\n    history = []\n\n    for round_idx in range(cfg["training"]["num_rounds"]):\n        round_client_states = []\n        teacher_state = global_model.state_dict()\n        for client_id, cl_loader in client_loaders.items():\n            model_wrapper_cls = FedMPQKDWrapper if cfg["model"]["name"].lower() == "fedmpq_kd" else FedMPQWrapper\n            client_state = train_client(model_wrapper_cls,\n                                        global_model.state_dict(),\n                                        teacher_state,\n                                        cl_loader,\n                                        device,\n                                        cfg)\n            round_client_states.append(client_state)\n        # Aggregate\n        new_global_state = average_state_dicts(round_client_states)\n        global_model.load_state_dict(new_global_state)\n\n        # Evaluate\n        acc = evaluate(global_model, test_loader, device)\n        history.append({"round": round_idx + 1, "test_accuracy": acc})\n        print(json.dumps({"run_id": cfg["run_id"], "round": round_idx + 1, "test_accuracy": acc}))\n\n    # Persist\n    results_path = results_dir / "results.json"\n    with results_path.open("w") as f:\n        json.dump(history, f, indent=2)\n    # save final model\n    torch.save(global_model.state_dict(), results_dir / "final_model.pt")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="Run a single experiment variation (client-server FL loop)")\n    parser.add_argument("--config", required=True, help="Path to run variation YAML config")\n    parser.add_argument("--results-dir", required=True, help="Directory to store outputs for this run")\n    args = parser.parse_args()\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    Path(args.results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Echo description first\n    exp_desc = cfg.get("description", "No description provided – please fill in.")\n    print("=" * 80)\n    print("Experiment description (run_id = {}):\\n{}".format(cfg["run_id"], exp_desc))\n    print("=" * 80)\n\n    # Ensure config itself is stored for reproducibility\n    shutil.copy(args.config, Path(args.results_dir) / "config.yaml")\n\n    set_seed(cfg["training"].get("seed", 0))\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': 'import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics as sk_metrics\n\nplt.style.use("seaborn-v0_8-paper")\n\n\ndef load_history(path: Path) -> pd.DataFrame:\n    with path.open() as f:\n        data = json.load(f)\n    return pd.DataFrame(data)\n\n\ndef auc_accuracy(df: pd.DataFrame) -> float:\n    """Area under the accuracy-vs-rounds curve (trapezoidal)."""\n    return sk_metrics.auc(df["round"], df["test_accuracy"])\n\n\ndef make_line_plot(histories: Dict[str, pd.DataFrame], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    for run_id, df in histories.items():\n        ax.plot(df["round"], df["test_accuracy"], label=run_id)\n        ax.annotate(f"{df[\'test_accuracy\'].iat[-1]*100:.2f}%",\n                    xy=(df[\'round\'].iat[-1], df[\'test_accuracy\'].iat[-1]),\n                    textcoords="offset points", xytext=(0, 5))\n    ax.set_xlabel("Communication round")\n    ax.set_ylabel("Test Accuracy")\n    ax.legend()\n    plt.tight_layout()\n    img_dir = results_dir / "images"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / "accuracy_over_rounds.pdf"\n    fig.savefig(fig_path, bbox_inches="tight")\n    print(f"Saved figure {fig_path.relative_to(results_dir)}")\n    plt.close(fig)\n\n\ndef make_bar_plot(final_acc: Dict[str, float], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    run_ids = list(final_acc.keys())\n    values = [final_acc[r] for r in run_ids]\n    bars = ax.bar(run_ids, values)\n    ax.set_ylabel("Final Test Accuracy")\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.001, f"{val*100:.2f}%", ha="center", va="bottom")\n    plt.tight_layout()\n    img_dir = results_dir / "images"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / "final_accuracy.pdf"\n    fig.savefig(fig_path, bbox_inches="tight")\n    print(f"Saved figure {fig_path.relative_to(results_dir)}")\n    plt.close(fig)\n\n\ndef consolidate_metrics(histories: Dict[str, pd.DataFrame]):\n    metrics = {}\n    for run_id, df in histories.items():\n        metrics[run_id] = {\n            "final_accuracy": float(df["test_accuracy"].iat[-1]),\n            "best_accuracy": float(df["test_accuracy"].max()),\n            "auc_accuracy": float(auc_accuracy(df)),\n            "num_rounds": int(df["round"].iat[-1]),\n        }\n    return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="Evaluate & compare results of all experiment variations")\n    parser.add_argument("--results-dir", required=True, help="Root directory containing per-run sub-directories")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    # Collect histories\n    histories = {}\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / "results.json"\n        if res_file.exists():\n            histories[run_dir.name] = load_history(res_file)\n\n    if not histories:\n        print("No results.json files found – nothing to evaluate.")\n        return\n\n    # Produce figures\n    make_line_plot(histories, results_dir)\n    final_acc = {k: v["test_accuracy"].iat[-1] for k, v in histories.items()}\n    make_bar_plot(final_acc, results_dir)\n\n    # Consolidated metrics\n    metrics = consolidate_metrics(histories)\n    print(json.dumps({"comparison_metrics": metrics}, indent=2))\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '"""Common data-handling utilities with dataset placeholders.\n\nThis module implements\n    • Dirichlet non-IID splitting\n    • Synthetic dataset for smoke tests\n    • Helper that returns (client_id ⇒ DataLoader)  + global test loader\n\nDataset-specific loading logic should be added in the derived step by\nregistering loaders in DATASET_LOADERS.\n"""\n\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import datasets, transforms\n\n\nclass SyntheticImageDataset(Dataset):\n    """Random images & labels for smoke testing.  32×32×3."""\n\n    def __init__(self, n: int = 1024, num_classes: int = 10):\n        self.n = n\n        self.num_classes = num_classes\n        self.x = torch.randn(n, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n,))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# Registry for dataset-specific loaders.  New datasets should register here.\nDATASET_LOADERS = {}\n\n\ndef register_dataset(name):\n    def decorator(fn):\n        DATASET_LOADERS[name.lower()] = fn\n        return fn\n\n    return decorator\n\n\n@register_dataset("synthetic")\ndef _load_synthetic(cfg):\n    total = cfg["data"].get("total_samples", 1024)\n    ds = SyntheticImageDataset(n=total, num_classes=cfg["data"].get("num_classes", 10))\n    return ds, cfg["data"].get("num_classes", 10)\n\n\n@register_dataset("cifar10")\ndef _load_cifar10(cfg):\n    tfm = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    root = Path(cfg.get("data_dir", "./data"))\n    trainset = datasets.CIFAR10(root=str(root), train=True, download=True, transform=tfm)\n    testset = datasets.CIFAR10(root=str(root), train=False, download=True, transform=tfm)\n    cfg["data"]["num_classes"] = 10\n    return (trainset, testset), 10\n\n\nclass Preprocessor:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.dataset_name = cfg["dataset"]["name"].lower()\n        if self.dataset_name not in DATASET_LOADERS:\n            raise NotImplementedError(\n                f"Dataset \'{self.dataset_name}\' not registered. Add loader in preprocess.py register_dataset decorator.")\n\n    def dirichlet_split(self, labels: List[int], num_clients: int, alpha: float) -> List[List[int]]:\n        """Dirichlet partitioning over class labels as in FL research."""\n        labels = np.array(labels)\n        num_classes = len(np.unique(labels))\n        class_indices = [np.where(labels == y)[0] for y in range(num_classes)]\n        client_indices = [[] for _ in range(num_clients)]\n        for c, idxs in enumerate(class_indices):\n            np.random.shuffle(idxs)\n            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n            proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n            split = np.split(idxs, proportions)\n            for cid, part in enumerate(split):\n                client_indices[cid].extend(part.tolist())\n        return client_indices\n\n    def get_data_loaders(self) -> Tuple[Dict[int, DataLoader], DataLoader]:\n        batch_size = self.cfg["dataset"]["batch_size"]\n        num_clients = self.cfg["dataset"]["num_clients"]\n        alpha = self.cfg["dataset"].get("alpha", math.inf)\n\n        dataset_obj, num_classes = DATASET_LOADERS[self.dataset_name](self.cfg)\n        self.cfg.setdefault("data", {})\n        self.cfg["data"]["num_classes"] = num_classes\n\n        # Handle datasets that return tuple(train,test) vs single dataset\n        if isinstance(dataset_obj, tuple):\n            trainset, testset = dataset_obj\n        else:\n            trainset = dataset_obj\n            testset = dataset_obj  # synthetic reuse for eval\n\n        # Partition trainset\n        if alpha == math.inf:\n            # iid split\n            indices = np.arange(len(trainset))\n            np.random.shuffle(indices)\n            splits = np.array_split(indices, num_clients)\n        else:\n            labels = [trainset[i][1] for i in range(len(trainset))]\n            splits = self.dirichlet_split(labels, num_clients, alpha)\n\n        client_loaders = {}\n        for cid, idxs in enumerate(splits):\n            subset = Subset(trainset, idxs)\n            client_loaders[cid] = DataLoader(subset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n        test_loader = DataLoader(testset, batch_size=256, shuffle=False)\n        return client_loaders, test_loader\n', 'model_py': '"""Model definitions and wrappers implementing FedMPQ & FedMPQ-KD logic."""\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n###############################################################################\n# Utility functions                                                            #\n###############################################################################\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction="batchmean") * (T * T)\n\n\ndef quantize_tensor(t: torch.Tensor, num_bits: int = 8):\n    if num_bits >= 32:\n        return tensor  # No quantisation needed\n    qmin = -2 ** (num_bits - 1)\n    qmax = 2 ** (num_bits - 1) - 1\n    min_val, max_val = tensor.min(), tensor.max()\n    scale = (max_val - min_val) / float(qmax - qmin + 1e-8)\n    zp = qmin - torch.round(min_val / scale)\n    qt = torch.clamp(torch.round(tensor / scale + zp), qmin, qmax)\n    return (qt - zp) * scale\n\n\n###############################################################################\n# Tiny CNN (default when architecture placeholder not replaced)                #\n###############################################################################\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nBASE_MODEL_FACTORY = {\n    "tiny_cnn": TinyCNN,\n    "MODEL_PLACEHOLDER": None,  # PLACEHOLDER: Replace with specific model class (e.g., ResNet20)\n}\n\n\ndef get_base_model(name: str, num_classes: int):\n    name = name.lower()\n    if name not in BASE_MODEL_FACTORY or BASE_MODEL_FACTORY[name] is None:\n        raise NotImplementedError(f"Model architecture \'{name}\' not implemented.  Replace placeholder in model.py.")\n    return BASE_MODEL_FACTORY[name](num_classes=num_classes)\n\n\n###############################################################################\n# FedMPQ wrappers                                                              #\n###############################################################################\n\n\nclass FedMPQWrapper(nn.Module):\n    """Wrap a base model with weight quantisation & bit-regularisation."""\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n        self.bits = cfg["model"].get("bits", 8)\n        self.lambda_b = cfg["model"].get("lambda_b", 0.01)\n\n    def forward_with_bit_loss(self, x):\n        # Fake quantise weights on-the-fly\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        # Restore original weights to keep gradients correct\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n\n        bit_loss = self.bit_regulariser()\n        return logits, bit_loss\n\n    def forward(self, x):\n        logits, _ = self.forward_with_bit_loss(x)\n        return logits\n\n    def bit_regulariser(self):\n        reg = 0.0\n        for p in self.base_model.parameters():\n            reg = reg + torch.sum(torch.abs(p)) / p.numel()\n        return reg\n\n\nclass FedMPQKDWrapper(FedMPQWrapper):\n    def __init__(self, base_model: nn.Module, teacher_model: nn.Module, cfg: Dict):\n        super().__init__(base_model, cfg)\n        self.teacher = teacher_model.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n', 'main_py': 'import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nSCRIPT_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = SCRIPT_DIR.parent / "config"\n\n\nclass Tee:\n    """Tee helper duplicating a stream into a log file while echoing to console."""\n\n    def __init__(self, log_path: Path, stream):\n        self.file = log_path.open("w")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.stream.write(data)\n        self.file.flush()\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n\ndef run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    with stdout_path.open("w") as out_f, stderr_path.open("w") as err_f:\n        proc = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream output live\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f"Subprocess \'{\' \'.join(cmd)}\' exited with {proc.returncode}")\n\n\ndef load_config(path: Path):\n    return yaml.safe_load(path.read_text())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="Experiment orchestrator – runs all variations & evaluation.")\n    parser.add_argument("--smoke-test", action="store_true", help="Run variations defined in smoke_test.yaml")\n    parser.add_argument("--full-experiment", action="store_true", help="Run variations in full_experiment.yaml")\n    parser.add_argument("--results-dir", required=True, help="Directory to store all outputs")\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error("Exactly one of --smoke-test or --full-experiment must be supplied.")\n\n    cfg_file = CONFIG_DIR / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n    cfg = load_config(cfg_file)\n    experiments = cfg.get("experiments", [])\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for exp in experiments:\n        run_id = exp["run_id"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Write individual run config\n        run_cfg_path = run_dir / "config.yaml"\n        with run_cfg_path.open("w") as f:\n            yaml.dump(exp, f)\n\n        print("Launching run_id =", run_id)\n        cmd = [sys.executable, "-m", "src.train", "--config", str(run_cfg_path), "--results-dir", str(run_dir)]\n        stdout_log = run_dir / "stdout.log"\n        stderr_log = run_dir / "stderr.log"\n        run_subprocess(cmd, cwd=SCRIPT_DIR.parent, stdout_path=stdout_log, stderr_path=stderr_log)\n\n    # After all runs – evaluation\n    print("All runs finished.  Initiating evaluation…")\n    eval_cmd = [sys.executable, "-m", "src.evaluate", "--results-dir", str(results_root)]\n    run_subprocess(eval_cmd, cwd=SCRIPT_DIR.parent, stdout_path=results_root / "evaluate_stdout.log", stderr_path=results_root / "evaluate_stderr.log")\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[build-system]\nrequires = ["setuptools", "wheel"]\n\n[project]\nname = "fedmpq_kd_framework"\nversion = "0.1.0"\ndependencies = [\n    "torch>=2.0.0",\n    "torchvision>=0.15",\n    "matplotlib>=3.8",\n    "seaborn>=0.13",\n    "numpy>=1.23",\n    "pandas>=2.1",\n    "pyyaml>=6.0",\n    "scikit-learn>=1.3",\n    "tqdm>=4.66",\n]\n', 'smoke_test_yaml': 'experiments:\n  - run_id: baseline_smoke\n    description: |\n      Smoke-test run of FedMPQ baseline with synthetic data.\n      2 clients, 2 communication rounds, tiny CNN.\n    dataset:\n      name: synthetic          # Uses built-in synthetic dataset\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 1\n\n  - run_id: kd_smoke\n    description: |\n      Smoke-test run of FedMPQ-KD with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq_kd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 2\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n', 'full_experiment_yaml': 'experiments:\n  # PLACEHOLDER: Populate with real variations in derived-specific step\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: "# PLACEHOLDER: Detailed description of the experimental condition"\n    dataset:\n      name: DATASET_PLACEHOLDER          # e.g., cifar10\n      num_clients: SPECIFIC_CONFIG_PLACEHOLDER\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER             # fedmpq / fedmpq_kd / baseline etc.\n      architecture: ARCHITECTURE_PLACEHOLDER  # resnet20 etc.\n      bits: SPECIFIC_CONFIG_PLACEHOLDER\n      lambda_b: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      num_rounds: SPECIFIC_CONFIG_PLACEHOLDER\n      local_epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      seed: SPECIFIC_CONFIG_PLACEHOLDER\n    kd_params:\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      T: SPECIFIC_CONFIG_PLACEHOLDER\n  # Add more experiments as needed\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: timm/mobilenetv2_100.ra_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('mobilenetv2_100.ra_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'mobilenetv2_100.ra_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 16, 112, 112])
    #  torch.Size([1, 24, 56, 56])
    #  torch.Size([1, 32, 28, 28])
    #  torch.Size([1, 96, 14, 14])
    #  torch.Size([1, 320, 7, 7])

    print(o.shape)

from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'mobilenetv2_100.ra_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 1280, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: uoft-cs/cifar10





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "import argparse\nimport json\nimport shutil\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\n\nfrom .model import (\n    FedMPQKDWrapper,\n    FedMPQWrapper,\n    get_base_model,\n    kd_loss,\n)\nfrom .preprocess import Preprocessor\n\n\n################################################################################\n# Helper functions                                                             #\n################################################################################\n\ndef set_seed(seed: int):\n    \"\"\"Make experiments deterministic (as much as possible).\"\"\"\n    import random\n    import torch.backends.cudnn as cudnn\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\ndef average_state_dicts(state_dicts: List[Dict[str, torch.Tensor]]):\n    \"\"\"FedAvg aggregation (simple mean of model parameters).\"\"\"\n    avg_state = {}\n    for k in state_dicts[0]:\n        avg_state[k] = (\n            torch.stack([sd[k].float() for sd in state_dicts], 0).mean(0).type(state_dicts[0][k].dtype)\n        )\n    return avg_state\n\n\ndef evaluate(model: torch.nn.Module, dataloader, device):\n    \"\"\"Return accuracy of MODEL on DATALOADER.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\n################################################################################\n# Client training                                                             #\n################################################################################\n\ndef _make_optimizer(params, cfg):\n    opt_name = cfg[\"training\"].get(\"optimizer\", \"sgd\").lower()\n    lr = cfg[\"training\"][\"lr\"]\n    if opt_name == \"adamw\":\n        weight_decay = cfg[\"training\"].get(\"weight_decay\", 1e-4)\n        return torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n    # Default: SGD\n    momentum = cfg[\"training\"].get(\"momentum\", 0.9)\n    weight_decay = cfg[\"training\"].get(\"weight_decay\", 5e-4)\n    return torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n\n\ndef train_client(\n    model_wrapper_cls,\n    global_state,\n    teacher_state,\n    dataloader,\n    device,\n    cfg,\n):\n    \"\"\"Single client's local training step.\"\"\"\n    # Instantiate student\n    base_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    base_model.load_state_dict(global_state)\n    base_model.to(device)\n\n    # Teacher (only for KD-style wrappers)\n    if model_wrapper_cls is FedMPQKDWrapper:\n        teacher_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n        teacher_model.load_state_dict(teacher_state)\n        teacher_model.to(device)\n        wrapper = model_wrapper_cls(base_model, teacher_model, cfg)\n    else:\n        wrapper = model_wrapper_cls(base_model, cfg)\n\n    wrapper.train()\n    optimizer = _make_optimizer(wrapper.parameters(), cfg)\n\n    kd_cfg = cfg.get(\"kd_params\", {\"alpha\": 0.5, \"T\": 2.0})\n\n    for _ in range(cfg[\"training\"][\"local_epochs\"]):\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits_student, loss_bit = wrapper.forward_with_bit_loss(x)\n            loss_task = F.cross_entropy(logits_student, y)\n            if model_wrapper_cls is FedMPQKDWrapper:\n                with torch.no_grad():\n                    logits_teacher = wrapper.teacher(x)\n                loss_kd = kd_loss(logits_student, logits_teacher, T=kd_cfg[\"T\"])\n                loss = (\n                    loss_task\n                    + cfg[\"model\"].get(\"lambda_b\", 0.0) * loss_bit\n                    + kd_cfg[\"alpha\"] * loss_kd\n                )\n            else:\n                loss = loss_task + cfg[\"model\"].get(\"lambda_b\", 0.0) * loss_bit\n            loss.backward()\n            optimizer.step()\n    # Return student weights\n    return wrapper.base_model.state_dict()\n\n\n################################################################################\n# Federated loop                                                              #\n################################################################################\n\ndef run_experiment(cfg: Dict, results_dir: Path):\n    \"\"\"Main federated training loop.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Data\n    preproc = Preprocessor(cfg)\n    client_loaders, test_loader = preproc.get_data_loaders()\n    num_clients = len(client_loaders)\n    participation_rate = cfg[\"dataset\"].get(\"participation_rate\", 1.0)\n\n    # 2. Global model\n    global_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    global_model.to(device)\n\n    # 3. Training history\n    history = []\n\n    for round_idx in range(cfg[\"training\"][\"num_rounds\"]):\n        round_client_states = []\n        teacher_state = global_model.state_dict()\n\n        # Sample active clients\n        num_active = max(1, int(num_clients * participation_rate))\n        active_clients = np.random.choice(list(client_loaders.keys()), num_active, replace=False)\n\n        for cid in active_clients:\n            # 20% chance of dropout (straggler) if configured\n            dropout_prob = cfg[\"dataset\"].get(\"dropout_prob\", 0.0)\n            if np.random.rand() < dropout_prob:\n                continue  # skip this client\n\n            cl_loader = client_loaders[cid]\n            wrapper_cls = (\n                FedMPQKDWrapper if \"kd\" in cfg[\"model\"][\"name\"].lower() else FedMPQWrapper\n            )\n            client_state = train_client(\n                wrapper_cls,\n                global_model.state_dict(),\n                teacher_state,\n                cl_loader,\n                device,\n                cfg,\n            )\n            round_client_states.append(client_state)\n\n        # Guard against all clients dropped out\n        if not round_client_states:\n            print(f\"[WARN] All clients dropped in round {round_idx + 1}. Skipping aggregation.\")\n            continue\n\n        # Aggregate\n        new_global_state = average_state_dicts(round_client_states)\n        global_model.load_state_dict(new_global_state)\n\n        # Evaluate\n        acc = evaluate(global_model, test_loader, device)\n        history.append({\"round\": round_idx + 1, \"test_accuracy\": acc})\n        print(json.dumps({\"run_id\": cfg[\"run_id\"], \"round\": round_idx + 1, \"test_accuracy\": acc}))\n\n    # Persist results\n    results_path = results_dir / \"results.json\"\n    with results_path.open(\"w\") as f:\n        json.dump(history, f, indent=2)\n\n    torch.save(global_model.state_dict(), results_dir / \"final_model.pt\")\n\n\n################################################################################\n# CLI                                                                         #\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Run a single experiment variation (client-server FL loop)\"\n    )\n    parser.add_argument(\"--config\", required=True, help=\"Path to variation YAML config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Where to store outputs for this run\")\n    args = parser.parse_args()\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    Path(args.results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Echo description\n    print(\"=\" * 80)\n    print(f\"Experiment description (run_id = {cfg['run_id']}):\\n{cfg.get('description', '')}\")\n    print(\"=\" * 80)\n\n    # Copy config for reproducibility\n    shutil.copy(args.config, Path(args.results_dir) / \"config.yaml\")\n\n    set_seed(cfg[\"training\"].get(\"seed\", 0))\n    start = time.time()\n    run_experiment(cfg, Path(args.results_dir))\n    print(f\"Run {cfg['run_id']} completed in {(time.time() - start)/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics as sk_metrics\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n\ndef load_history(path: Path) -> pd.DataFrame:\n    with path.open() as f:\n        data = json.load(f)\n    return pd.DataFrame(data)\n\n\ndef auc_accuracy(df: pd.DataFrame) -> float:\n    \"\"\"Area under the accuracy-vs-rounds curve (trapezoidal).\"\"\"\n    return sk_metrics.auc(df[\"round\"], df[\"test_accuracy\"])\n\n\ndef make_line_plot(histories: Dict[str, pd.DataFrame], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    for run_id, df in histories.items():\n        ax.plot(df[\"round\"], df[\"test_accuracy\"], label=run_id)\n        ax.annotate(f\"{df['test_accuracy'].iat[-1]*100:.2f}%\",\n                    xy=(df['round'].iat[-1], df['test_accuracy'].iat[-1]),\n                    textcoords=\"offset points\", xytext=(0, 5))\n    ax.set_xlabel(\"Communication round\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.legend()\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"accuracy_over_rounds.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef make_bar_plot(final_acc: Dict[str, float], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    run_ids = list(final_acc.keys())\n    values = [final_acc[r] for r in run_ids]\n    bars = ax.bar(run_ids, values)\n    ax.set_ylabel(\"Final Test Accuracy\")\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.001, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"final_accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef consolidate_metrics(histories: Dict[str, pd.DataFrame]):\n    metrics = {}\n    for run_id, df in histories.items():\n        metrics[run_id] = {\n            \"final_accuracy\": float(df[\"test_accuracy\"].iat[-1]),\n            \"best_accuracy\": float(df[\"test_accuracy\"].max()),\n            \"auc_accuracy\": float(auc_accuracy(df)),\n            \"num_rounds\": int(df[\"round\"].iat[-1]),\n        }\n    return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate & compare results of all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Root directory containing per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    # Collect histories\n    histories = {}\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            histories[run_dir.name] = load_history(res_file)\n\n    if not histories:\n        print(\"No results.json files found – nothing to evaluate.\")\n        return\n\n    # Produce figures\n    make_line_plot(histories, results_dir)\n    final_acc = {k: v[\"test_accuracy\"].iat[-1] for k, v in histories.items()}\n    make_bar_plot(final_acc, results_dir)\n\n    # Consolidated metrics\n    metrics = consolidate_metrics(histories)\n    print(json.dumps({\"comparison_metrics\": metrics}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Data-handling utilities.\n\nSupports:\n• Synthetic random images (for smoke tests)\n• CIFAR-10 / CIFAR-100 with torchvision\n\nProvides `Preprocessor` that performs Dirichlet client partitioning and returns\n    client_id -> DataLoader  (train)\n    + global test DataLoader\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import datasets, transforms\n\n\n################################################################################\n# Synthetic dataset (tiny, fast)                                               #\n################################################################################\n\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"Random images & labels for smoke testing.  32×32×3.\"\"\"\n\n    def __init__(self, n: int = 1024, num_classes: int = 10):\n        self.n = n\n        self.num_classes = num_classes\n        self.x = torch.randn(n, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n,))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n################################################################################\n# Dataset loader registry                                                      #\n################################################################################\n\nDATASET_LOADERS: Dict[str, callable] = {}\n\n\ndef register_dataset(name):\n    def decorator(fn):\n        DATASET_LOADERS[name.lower()] = fn\n        return fn\n\n    return decorator\n\n\n@register_dataset(\"synthetic\")\ndef _load_synthetic(cfg):\n    total = cfg[\"data\"].get(\"total_samples\", 1024)\n    num_classes = cfg[\"data\"].get(\"num_classes\", 10)\n    ds = SyntheticImageDataset(n=total, num_classes=num_classes)\n    return ds, num_classes\n\n\n################################################################################\n# CIFAR-10 / CIFAR-100 loaders                                                 #\n################################################################################\n\n_COMMON_CIFAR_MEAN = {\n    10: (0.4914, 0.4822, 0.4465),\n    100: (0.5071, 0.4867, 0.4408),\n}\n_COMMON_CIFAR_STD = {\n    10: (0.2023, 0.1994, 0.2010),\n    100: (0.2675, 0.2565, 0.2761),\n}\n\n\ndef _cifar_transforms(num_classes: int):\n    mean = _COMMON_CIFAR_MEAN[num_classes]\n    std = _COMMON_CIFAR_STD[num_classes]\n    train_tfm = transforms.Compose(\n        [\n            transforms.RandomCrop(28, padding=4),  # 32 -> 28 as spec\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    test_tfm = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n    )\n    return train_tfm, test_tfm\n\n\n@register_dataset(\"cifar10\")\ndef _load_cifar10(cfg):\n    root = Path(cfg.get(\"data_dir\", \"./data\"))\n    train_tfm, test_tfm = _cifar_transforms(10)\n    trainset = datasets.CIFAR10(root=str(root), train=True, transform=train_tfm, download=True)\n    testset = datasets.CIFAR10(root=str(root), train=False, transform=test_tfm, download=True)\n    return (trainset, testset), 10\n\n\n@register_dataset(\"cifar100\")\ndef _load_cifar100(cfg):\n    root = Path(cfg.get(\"data_dir\", \"./data\"))\n    train_tfm, test_tfm = _cifar_transforms(100)\n    trainset = datasets.CIFAR100(root=str(root), train=True, transform=train_tfm, download=True)\n    testset = datasets.CIFAR100(root=str(root), train=False, transform=test_tfm, download=True)\n    return (trainset, testset), 100\n\n\n################################################################################\n# Preprocessor                                                                 #\n################################################################################\n\n\nclass Preprocessor:\n    \"\"\"Prepare per-client DataLoaders & global test loader.\"\"\"\n\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.dataset_name = cfg[\"dataset\"][\"name\"].lower()\n        if self.dataset_name not in DATASET_LOADERS:\n            raise NotImplementedError(\n                f\"Dataset '{self.dataset_name}' not registered. Add a loader via @register_dataset decorator.\"\n            )\n\n    # ---------------- Dirichlet Split ---------------- #\n    def _dirichlet_split(self, labels: List[int], num_clients: int, alpha: float):\n        labels = np.array(labels)\n        num_classes = len(np.unique(labels))\n        # Pre-compute indices per class\n        class_indices = [np.where(labels == c)[0] for c in range(num_classes)]\n        client_indices = [[] for _ in range(num_clients)]\n        for c, idxs in enumerate(class_indices):\n            np.random.shuffle(idxs)\n            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n            # Balance to ensure each client gets at least one sample per class\n            proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n            splits = np.split(idxs, proportions)\n            for cid, part in enumerate(splits):\n                client_indices[cid].extend(part.tolist())\n        return client_indices\n\n    # ---------------- Public API ---------------- #\n    def get_data_loaders(self) -> Tuple[Dict[int, DataLoader], DataLoader]:\n        batch_size = self.cfg[\"dataset\"][\"batch_size\"]\n        num_clients = self.cfg[\"dataset\"][\"num_clients\"]\n        alpha = self.cfg[\"dataset\"].get(\"alpha\", math.inf)\n\n        dataset_obj, num_classes = DATASET_LOADERS[self.dataset_name](self.cfg)\n        # Store for downstream use\n        self.cfg.setdefault(\"data\", {})\n        self.cfg[\"data\"][\"num_classes\"] = num_classes\n\n        # Handle (train, test) tuple or single dataset\n        if isinstance(dataset_obj, tuple):\n            trainset, testset = dataset_obj\n        else:\n            trainset = dataset_obj\n            testset = dataset_obj\n\n        # Create client splits\n        if alpha == math.inf:\n            indices = np.arange(len(trainset))\n            np.random.shuffle(indices)\n            splits = np.array_split(indices, num_clients)\n        else:\n            labels = [trainset[i][1] for i in range(len(trainset))]\n            splits = self._dirichlet_split(labels, num_clients, alpha)\n\n        client_loaders: Dict[int, DataLoader] = {}\n        for cid, idx in enumerate(splits):\n            subset = Subset(trainset, idx)\n            client_loaders[cid] = DataLoader(\n                subset,\n                batch_size=batch_size,\n                shuffle=True,\n                drop_last=len(subset) >= batch_size,\n                num_workers=2,\n            )\n\n        test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n        return client_loaders, test_loader\n",
    "model_py": "\"\"\"Model architectures and wrappers for FedMPQ & FedMPQ-KD.\"\"\"\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    import timm  # Lightweight, pure-python model zoo\nexcept ImportError:  # pragma: no cover\n    timm = None\n\n###############################################################################\n# KD loss                                                                      #\n###############################################################################\n\ndef kd_loss(logits_student, logits_teacher, T: float = 2.0):\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction=\"batchmean\") * (T * T)\n\n\n###############################################################################\n# Quantisation helpers                                                         #\n###############################################################################\n\ndef quantize_tensor(t: torch.Tensor, num_bits: int = 8):\n    \"\"\"Symmetric uniform fake-quantisation (per-tensor).\"\"\"\n    if num_bits >= 32:\n        return t  # No quantisation\n\n    qmin = -(2 ** (num_bits - 1))\n    qmax = 2 ** (num_bits - 1) - 1\n    min_val, max_val = t.min(), t.max()\n    # Avoid degenerate scale\n    scale = (max_val - min_val) / float(qmax - qmin) if max_val != min_val else torch.tensor(1.0, device=t.device)\n    zp = qmin - torch.round(min_val / scale)\n    qt = torch.clamp(torch.round(t / scale + zp), qmin, qmax)\n    return (qt - zp) * scale\n\n\n###############################################################################\n# Tiny CNN for smoke tests                                                     #\n###############################################################################\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.flatten(1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n###############################################################################\n# ResNet-20 (CIFAR)                                                            #\n###############################################################################\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return F.relu(out)\n\n\nclass ResNetCIFAR(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super().__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = nn.Linear(64, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for s in strides:\n            layers.append(block(self.in_planes, planes, s))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        return self.linear(out)\n\n\ndef ResNet20(num_classes=10):\n    return ResNetCIFAR(BasicBlock, [3, 3, 3], num_classes=num_classes)\n\n\n###############################################################################\n# MobileNet-V2 (via timm)                                                      #\n###############################################################################\n\n\ndef MobileNetV2(num_classes=10):\n    if timm is None:\n        raise ImportError(\"timm is required for MobileNetV2 – please install timm>=0.9.12\")\n    return timm.create_model(\"mobilenetv2_100.ra_in1k\", pretrained=False, num_classes=num_classes)\n\n\n###############################################################################\n# Model factory                                                                #\n###############################################################################\n\nBASE_MODEL_FACTORY = {\n    \"tiny_cnn\": TinyCNN,\n    \"resnet20\": ResNet20,\n    \"mobilenetv2\": MobileNetV2,\n}\n\n\ndef get_base_model(name: str, num_classes: int):\n    name = name.lower()\n    if name not in BASE_MODEL_FACTORY:\n        raise NotImplementedError(f\"Model architecture '{name}' not implemented.\")\n    return BASE_MODEL_FACTORY[name](num_classes=num_classes)\n\n\n###############################################################################\n# Wrappers                                                                     #\n###############################################################################\n\n\nclass FedMPQWrapper(nn.Module):\n    \"\"\"Wraps a base model with (fake) quantisation + bit regulariser.\"\"\"\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n        self.bits = cfg[\"model\"].get(\"bits\", 32)\n        self.lambda_b = cfg[\"model\"].get(\"lambda_b\", 0.0)\n\n    # --------------------------------------------------------\n    def forward_with_bit_loss(self, x):\n        # Fake quantise weights (in-place) then restore\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        # Restore fp32 weights for gradient flow\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n\n        bit_loss = self.bit_regulariser() if self.lambda_b > 0 else torch.tensor(0.0, device=x.device)\n        return logits, bit_loss\n\n    # --------------------------------------------------------\n    def forward(self, x):\n        logits, _ = self.forward_with_bit_loss(x)\n        return logits\n\n    # --------------------------------------------------------\n    def bit_regulariser(self):\n        reg = 0.0\n        for p in self.base_model.parameters():\n            reg += torch.mean(torch.abs(p))\n        return reg\n\n\nclass FedMPQKDWrapper(FedMPQWrapper):\n    \"\"\"Extends FedMPQ with knowledge-distillation to a frozen teacher.\"\"\"\n\n    def __init__(self, base_model: nn.Module, teacher_model: nn.Module, cfg: Dict):\n        super().__init__(base_model, cfg)\n        self.teacher = teacher_model.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n",
    "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nSCRIPT_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = SCRIPT_DIR.parent / \"config\"\n\n\nclass Tee:\n    \"\"\"Tee helper duplicating a stream into a log file while echoing to console.\"\"\"\n\n    def __init__(self, log_path: Path, stream):\n        self.file = log_path.open(\"w\")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.stream.write(data)\n        self.file.flush()\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n\ndef run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"w\") as out_f, stderr_path.open(\"w\") as err_f:\n        proc = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream output live\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess '{' '.join(cmd)}' exited with {proc.returncode}\")\n\n\ndef load_config(path: Path):\n    return yaml.safe_load(path.read_text())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator – runs all variations & evaluation.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error(\"Exactly one of --smoke-test or --full-experiment must be supplied.\")\n\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    cfg = load_config(cfg_file)\n    experiments = cfg.get(\"experiments\", [])\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Write individual run config\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.dump(exp, f)\n\n        print(\"Launching run_id =\", run_id)\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(run_dir)]\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        run_subprocess(cmd, cwd=SCRIPT_DIR.parent, stdout_path=stdout_log, stderr_path=stderr_log)\n\n    # After all runs – evaluation\n    print(\"All runs finished.  Initiating evaluation…\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    run_subprocess(eval_cmd, cwd=SCRIPT_DIR.parent, stdout_path=results_root / \"evaluate_stdout.log\", stderr_path=results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"fedmpq_kd_framework\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15\",\n    \"timm>=0.9.12\",\n    \"matplotlib>=3.8\",\n    \"seaborn>=0.13\",\n    \"numpy>=1.23\",\n    \"pandas>=2.1\",\n    \"pyyaml>=6.0\",\n    \"scikit-learn>=1.3\",\n    \"tqdm>=4.66\",\n]\n",
    "smoke_test_yaml": "experiments:\n  - run_id: FedMPQ-baseline_smoke\n    description: |\n      Smoke-test of FedMPQ baseline with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n      participation_rate: 1.0\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      optimizer: sgd\n      seed: 1\n\n  - run_id: FedMPQ-KD_default_smoke\n    description: |\n      Smoke-test of FedMPQ-KD (α=0.5, T=2) with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n      participation_rate: 1.0\n    model:\n      name: fedmpq_kd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      optimizer: sgd\n      seed: 2\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n\n  - run_id: FedMPQ-KD_high_smoke\n    description: |\n      Smoke-test of FedMPQ-KD high distillation weight (α=1.0).\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n      participation_rate: 1.0\n    model:\n      name: fedmpq_kd_high\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      optimizer: sgd\n      seed: 3\n    kd_params:\n      alpha: 1.0\n      T: 2.0\n\n  - run_id: FedKD_smoke\n    description: |\n      Smoke-test of FedKD global teacher variant.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n      participation_rate: 1.0\n    model:\n      name: fedkd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      optimizer: sgd\n      seed: 4\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n\n  - run_id: Q-FedAvg_smoke\n    description: |\n      Smoke-test of Q-FedAvg (full precision, no quantisation).\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n      participation_rate: 1.0\n    model:\n      name: q_fedavg\n      architecture: tiny_cnn\n      bits: 32\n      lambda_b: 0.0\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      optimizer: sgd\n      seed: 5\n",
    "full_experiment_yaml": "experiments:\n  - run_id: FedMPQ-baseline\n    description: |\n      Original FedMPQ baseline. Mixed-precision quantised MobileNet-V2 (avg 4-bit).\n      100 clients, extreme non-IID (Dirichlet α=0.1). No KD.\n    dataset:\n      name: cifar100\n      num_clients: 100\n      alpha: 0.1\n      batch_size: 64\n      participation_rate: 0.2         # 20% participation\n      dropout_prob: 0.2               # 20% stragglers\n    model:\n      name: fedmpq\n      architecture: mobilenetv2\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 100\n      local_epochs: 2\n      lr: 0.05\n      optimizer: sgd\n      momentum: 0.9\n      seed: 42\n\n  - run_id: FedMPQ-KD_default\n    description: |\n      Proposed FedMPQ-KD with α=0.5, T=2. Same setup as baseline.\n    dataset:\n      name: cifar100\n      num_clients: 100\n      alpha: 0.1\n      batch_size: 64\n      participation_rate: 0.2\n      dropout_prob: 0.2\n    model:\n      name: fedmpq_kd\n      architecture: mobilenetv2\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 100\n      local_epochs: 2\n      lr: 0.05\n      optimizer: sgd\n      momentum: 0.9\n      seed: 43\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n\n  - run_id: FedMPQ-KD_high\n    description: |\n      FedMPQ-KD with higher distillation weight α=1.0.\n    dataset:\n      name: cifar100\n      num_clients: 100\n      alpha: 0.1\n      batch_size: 64\n      participation_rate: 0.2\n      dropout_prob: 0.2\n    model:\n      name: fedmpq_kd_high\n      architecture: mobilenetv2\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 100\n      local_epochs: 2\n      lr: 0.05\n      optimizer: sgd\n      momentum: 0.9\n      seed: 44\n    kd_params:\n      alpha: 1.0\n      T: 2.0\n\n  - run_id: FedKD-global-teacher\n    description: |\n      Classical FedKD variant using global full-precision teacher.\n    dataset:\n      name: cifar100\n      num_clients: 100\n      alpha: 0.1\n      batch_size: 64\n      participation_rate: 0.2\n      dropout_prob: 0.2\n    model:\n      name: fedkd\n      architecture: mobilenetv2\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 100\n      local_epochs: 2\n      lr: 0.05\n      optimizer: sgd\n      momentum: 0.9\n      seed: 45\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n\n  - run_id: Q-FedAvg\n    description: |\n      Q-FedAvg full-precision baseline (no quantisation, λ_b=0).\n    dataset:\n      name: cifar100\n      num_clients: 100\n      alpha: 0.1\n      batch_size: 64\n      participation_rate: 0.2\n      dropout_prob: 0.2\n    model:\n      name: q_fedavg\n      architecture: mobilenetv2\n      bits: 32\n      lambda_b: 0.0\n    training:\n      num_rounds: 100\n      local_epochs: 2\n      lr: 0.05\n      optimizer: sgd\n      momentum: 0.9\n      seed: 46\n    # no kd_params required for FedAvg baseline\n  # ---------------------------------------------------------------------------\n  # End of experiment list\n  # ---------------------------------------------------------------------------\n"
}
