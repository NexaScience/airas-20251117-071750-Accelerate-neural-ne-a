
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "FedMPQ suffers a noticeable accuracy drop when the number of local epochs per communication round is small (e.g., 1).  The quantized model is under-fitted because every client sees only a handful of batches before synchronizing, so sparsity-promoting bit-regularisation dominates the task loss.\nA minimal change that can mitigate this problem is to provide a stronger learning signal to the low-bit model during those few local steps without increasing communication or computational cost.",
    "Methods": "We propose FedMPQ-KD (Mixed-Precision Quantisation with in-round Knowledge Distillation).\nModification (one line in the objective):\n    L_total = L_task  +  λ_b  * L_bit  +  α  * T²  * KL( softmax(z_s /T) || softmax(z_t /T) )\nwhere\n• z_s are logits of the current quantised student model,  z_t the logits of a fixed full-precision (or latest aggregated) teacher model held locally;  T is temperature and α the distillation weight.\n\nProcedure per client (changes in bold):\n1. Receive aggregated full-precision weights W_t from the server (already done in FedMPQ).\n2. Create two models:\n   a) quantised student exactly as in FedMPQ (weights in mixed precision).\n   b) ****freeze a copy of W_t in full precision as teacher****.\n3. For E local epochs, optimise the student with the extended loss above.  The teacher only produces logits; no back-prop.\n4. Send student weight updates as usual.\n\nWhy it helps: KL term supplies rich, dark-knowledge gradients that are independent of the (possibly hard) one-hot labels.  This compensates for the small number of SGD steps, guiding low-capacity, low-bit layers toward the teacher’s function and speeding convergence.  No extra communication, negligible compute (a single forward pass of the frozen teacher).",
    "Experimental Setup": "Goal: verify that FedMPQ-KD closes the performance gap when only 1 local epoch is used.\n• Dataset: CIFAR-10 (10 clients, α=0.5 Dirichlet split).\n• Network: ResNet-20.\n• Budgets: average 4-bit, mixed across layers as in the original paper.\n• Baselines:  (1) FedMPQ (original)  (2) FedMPQ-KD (ours).\n• Hyper-parameters:  λ_b =0.01 (unchanged),  α=0.5,  T=2.\n• Training: 50 communication rounds, 1 local epoch, batch-size 64, SGD lr 0.1.\n• Metric: global test accuracy after every round.\nExpected observation window: accuracy vs. rounds and final accuracy.",
    "Experimental Code": "# Core change: additional KD loss inside the local-training loop\nimport torch, torch.nn as nn, torch.nn.functional as F\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    \"\"\"KL divergence with temperature.\"\"\"\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\n\nclass LocalTrainer:\n    def __init__(self, student, teacher, dataloader, lr=0.1, lambda_b=0.01, alpha=0.5):\n        self.student = student  # quantised weights already applied\n        self.teacher = teacher.eval()  # full-precision copy, frozen\n        self.opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n        self.dl = dataloader\n        self.lambda_b = lambda_b\n        self.alpha = alpha\n\n    def train_one_epoch(self):\n        self.student.train()\n        for x, y in self.dl:\n            logits_s = self.student(x)\n            logits_t = self.teacher(x)\n            loss_task = F.cross_entropy(logits_s, y)\n            loss_bit  = self.student.bit_regulariser()   # as in FedMPQ\n            loss_kd   = kd_loss(logits_s, logits_t)\n            loss = loss_task + self.lambda_b*loss_bit + self.alpha*loss_kd\n            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "Expected Result": "With only 1 local epoch, FedMPQ reaches ≈82% CIFAR-10 accuracy after 50 rounds (reported drop of ~3-4% from full baseline).  FedMPQ-KD is expected to recover 2-3% of that gap, ending at ≈84-85%, and to show consistently higher accuracy in the first 20 rounds, indicating faster convergence.",
    "Expected Conclusion": "A single extra knowledge-distillation term supplies richer gradients to the quantised student during scarce local training, reducing under-fitting without additional communication or heavy computation.  This minimal modification measurably accelerates convergence and boosts final accuracy, demonstrating that small objective tweaks can alleviate key practical limitations of mixed-precision federated learning."
}

# Experimental Design
## Experiment Strategy
Objective: Build a single, coherent framework that will let every subsequent experiment answer one overarching question – does FedMPQ-KD provide a strictly better accuracy-vs-cost trade-off than competing quantised FL schemes across realistic operating conditions?

1. Core Hypotheses to Validate
   a. Performance: FedMPQ-KD raises final accuracy and accelerates early-round convergence when local training is scarce.
   b. Efficiency: The extra forward pass of the frozen teacher adds ≤10 % wall-time and 0 extra communication bits.
   c. Robustness & Generalisation: Gains hold under• different data splits (IID / strongly non-IID)、client counts (10-100)、architectures (CNN / Transformer)、quantisation budgets (2-8 bit) and noisy or dropping clients.
   d. Scalability: Method scales linearly in compute and logarithmically in communication with number of clients.

2. Comparison Palette (identical across all experiments)
   • Baselines: (1) FedMPQ (original) (2) Full-precision FedAvg (upper bound) (3) SOTA low-bit FL alternatives (FedPQ, FedKD, Q-FedAvg).
   • Ablations: i) –KD (λ_b+L_task only) ii) varying α, T iii) updating vs freezing the teacher iv) local vs global teacher refresh frequency.

3. Evaluation Angles
   3.1 Quantitative Performance
       – Top-1 accuracy vs communication rounds (primary)
       – Area-under-curve (AUC_acc) to capture convergence speed
       – Final accuracy gap to full-precision
   3.2 Cost Metrics
       – Client-side FLOPs & wall-clock per round (measured on A100)
       – Communication bits / round (should be unchanged)
       – Peak GPU VRAM & host RAM
   3.3 Robustness Metrics
       – Accuracy variance across 3 random seeds
       – Degradation (% drop) under extreme non-IID, stragglers, label noise
   3.4 Qualitative / Diagnostic
       – t-SNE of penultimate-layer features (student vs teacher)
       – Bit-width utilisation histograms
       – Gradient-norm and loss-landscape snapshots to illustrate stronger signals.

4. Experimental Axes (each future experiment picks one axis while keeping the rest fixed)
   A. Local compute budget: E∈{1,2,5}
   B. Quantisation budget: avg bits ∈{2,4,8}
   C. Data heterogeneity: Dirichlet α∈{0.1,0.5,∞}
   D. Model family: ResNet-20, MobileNet-V2, ViT-Small
   E. Scale: clients ∈{10,50,100}; participation rate 10-100 %
   F. Failure modes: 20 % clients randomly drop per round; 10 % label noise.

5. Success Criteria (must hold in ≥80 % of experimental settings)
   • +≥2 % absolute test accuracy OR ≥25 % smaller gap to full-precision compared with best quantised baseline, p<0.05 (t-test over seeds).
   • No statistically significant increase in communication volume.
   • Added wall-time ≤10 % per round on A100.
   • Under worst-case non-IID, KD variant still outperforms FedMPQ by ≥1 %.

6. Protocol & Reproducibility
   • Same optimiser, LR schedule, batch size across methods; only α and T tuned once on a small validation pool and kept fixed.
   • Three independent seeds; report mean±std.
   • Use PyTorch w/ deterministic flags; log hardware utilisation via NVIDIA-smi and PyTorch profiler.
   • Release code & logs; each run stores JSON metadata capturing hyper-params, seed, commit hash.

7. Resource Awareness
   • Single NVIDIA A100 (80 GB) easily fits teacher + student (≤4 GB each). Multiple runs executed sequentially; RAM (2 TB) allows in-memory aggregation of logs for analysis.

By following this common strategy—fixed baselines, uniform metrics, multifaceted validation axes, and rigid success criteria—each forthcoming experiment will contribute a comparable slice of evidence, together establishing the robustness, efficiency and overall superiority of FedMPQ-KD.

# Base Code (Reference Foundation)
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics as sk_metrics\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n\ndef load_history(path: Path) -\u003e pd.DataFrame:\n    with path.open() as f:\n        data = json.load(f)\n    return pd.DataFrame(data)\n\n\ndef auc_accuracy(df: pd.DataFrame) -\u003e float:\n    \"\"\"Area under the accuracy-vs-rounds curve (trapezoidal).\"\"\"\n    return sk_metrics.auc(df[\"round\"], df[\"test_accuracy\"])\n\n\ndef make_line_plot(histories: Dict[str, pd.DataFrame], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    for run_id, df in histories.items():\n        ax.plot(df[\"round\"], df[\"test_accuracy\"], label=run_id)\n        ax.annotate(f\"{df[\u0027test_accuracy\u0027].iat[-1]*100:.2f}%\",\n                    xy=(df[\u0027round\u0027].iat[-1], df[\u0027test_accuracy\u0027].iat[-1]),\n                    textcoords=\"offset points\", xytext=(0, 5))\n    ax.set_xlabel(\"Communication round\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.legend()\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"accuracy_over_rounds.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef make_bar_plot(final_acc: Dict[str, float], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    run_ids = list(final_acc.keys())\n    values = [final_acc[r] for r in run_ids]\n    bars = ax.bar(run_ids, values)\n    ax.set_ylabel(\"Final Test Accuracy\")\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.001, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"final_accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef consolidate_metrics(histories: Dict[str, pd.DataFrame]):\n    metrics = {}\n    for run_id, df in histories.items():\n        metrics[run_id] = {\n            \"final_accuracy\": float(df[\"test_accuracy\"].iat[-1]),\n            \"best_accuracy\": float(df[\"test_accuracy\"].max()),\n            \"auc_accuracy\": float(auc_accuracy(df)),\n            \"num_rounds\": int(df[\"round\"].iat[-1]),\n        }\n    return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate \u0026 compare results of all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Root directory containing per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    # Collect histories\n    histories = {}\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            histories[run_dir.name] = load_history(res_file)\n\n    if not histories:\n        print(\"No results.json files found \u2013 nothing to evaluate.\")\n        return\n\n    # Produce figures\n    make_line_plot(histories, results_dir)\n    final_acc = {k: v[\"test_accuracy\"].iat[-1] for k, v in histories.items()}\n    make_bar_plot(final_acc, results_dir)\n\n    # Consolidated metrics\n    metrics = consolidate_metrics(histories)\n    print(json.dumps({\"comparison_metrics\": metrics}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "experiments:\n  # PLACEHOLDER: Populate with real variations in derived-specific step\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: \"# PLACEHOLDER: Detailed description of the experimental condition\"\n    dataset:\n      name: DATASET_PLACEHOLDER          # e.g., cifar10\n      num_clients: SPECIFIC_CONFIG_PLACEHOLDER\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER             # fedmpq / fedmpq_kd / baseline etc.\n      architecture: ARCHITECTURE_PLACEHOLDER  # resnet20 etc.\n      bits: SPECIFIC_CONFIG_PLACEHOLDER\n      lambda_b: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      num_rounds: SPECIFIC_CONFIG_PLACEHOLDER\n      local_epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      seed: SPECIFIC_CONFIG_PLACEHOLDER\n    kd_params:\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      T: SPECIFIC_CONFIG_PLACEHOLDER\n  # Add more experiments as needed\n", "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nSCRIPT_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = SCRIPT_DIR.parent / \"config\"\n\n\nclass Tee:\n    \"\"\"Tee helper duplicating a stream into a log file while echoing to console.\"\"\"\n\n    def __init__(self, log_path: Path, stream):\n        self.file = log_path.open(\"w\")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.stream.write(data)\n        self.file.flush()\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n\ndef run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"w\") as out_f, stderr_path.open(\"w\") as err_f:\n        proc = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream output live\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess \u0027{\u0027 \u0027.join(cmd)}\u0027 exited with {proc.returncode}\")\n\n\ndef load_config(path: Path):\n    return yaml.safe_load(path.read_text())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator \u2013 runs all variations \u0026 evaluation.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error(\"Exactly one of --smoke-test or --full-experiment must be supplied.\")\n\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    cfg = load_config(cfg_file)\n    experiments = cfg.get(\"experiments\", [])\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Write individual run config\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.dump(exp, f)\n\n        print(\"Launching run_id =\", run_id)\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(run_dir)]\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        run_subprocess(cmd, cwd=SCRIPT_DIR.parent, stdout_path=stdout_log, stderr_path=stderr_log)\n\n    # After all runs \u2013 evaluation\n    print(\"All runs finished.  Initiating evaluation\u2026\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    run_subprocess(eval_cmd, cwd=SCRIPT_DIR.parent, stdout_path=results_root / \"evaluate_stdout.log\", stderr_path=results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model definitions and wrappers implementing FedMPQ \u0026 FedMPQ-KD logic.\"\"\"\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n###############################################################################\n# Utility functions                                                            #\n###############################################################################\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction=\"batchmean\") * (T * T)\n\n\ndef quantize_tensor(t: torch.Tensor, num_bits: int = 8):\n    if num_bits \u003e= 32:\n        return tensor  # No quantisation needed\n    qmin = -2 ** (num_bits - 1)\n    qmax = 2 ** (num_bits - 1) - 1\n    min_val, max_val = tensor.min(), tensor.max()\n    scale = (max_val - min_val) / float(qmax - qmin + 1e-8)\n    zp = qmin - torch.round(min_val / scale)\n    qt = torch.clamp(torch.round(tensor / scale + zp), qmin, qmax)\n    return (qt - zp) * scale\n\n\n###############################################################################\n# Tiny CNN (default when architecture placeholder not replaced)                #\n###############################################################################\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nBASE_MODEL_FACTORY = {\n    \"tiny_cnn\": TinyCNN,\n    \"MODEL_PLACEHOLDER\": None,  # PLACEHOLDER: Replace with specific model class (e.g., ResNet20)\n}\n\n\ndef get_base_model(name: str, num_classes: int):\n    name = name.lower()\n    if name not in BASE_MODEL_FACTORY or BASE_MODEL_FACTORY[name] is None:\n        raise NotImplementedError(f\"Model architecture \u0027{name}\u0027 not implemented.  Replace placeholder in model.py.\")\n    return BASE_MODEL_FACTORY[name](num_classes=num_classes)\n\n\n###############################################################################\n# FedMPQ wrappers                                                              #\n###############################################################################\n\n\nclass FedMPQWrapper(nn.Module):\n    \"\"\"Wrap a base model with weight quantisation \u0026 bit-regularisation.\"\"\"\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n        self.bits = cfg[\"model\"].get(\"bits\", 8)\n        self.lambda_b = cfg[\"model\"].get(\"lambda_b\", 0.01)\n\n    def forward_with_bit_loss(self, x):\n        # Fake quantise weights on-the-fly\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        # Restore original weights to keep gradients correct\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n\n        bit_loss = self.bit_regulariser()\n        return logits, bit_loss\n\n    def forward(self, x):\n        logits, _ = self.forward_with_bit_loss(x)\n        return logits\n\n    def bit_regulariser(self):\n        reg = 0.0\n        for p in self.base_model.parameters():\n            reg = reg + torch.sum(torch.abs(p)) / p.numel()\n        return reg\n\n\nclass FedMPQKDWrapper(FedMPQWrapper):\n    def __init__(self, base_model: nn.Module, teacher_model: nn.Module, cfg: Dict):\n        super().__init__(base_model, cfg)\n        self.teacher = teacher_model.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n", "preprocess_py": "\"\"\"Common data-handling utilities with dataset placeholders.\n\nThis module implements\n    \u2022 Dirichlet non-IID splitting\n    \u2022 Synthetic dataset for smoke tests\n    \u2022 Helper that returns (client_id \u21d2 DataLoader)  + global test loader\n\nDataset-specific loading logic should be added in the derived step by\nregistering loaders in DATASET_LOADERS.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import datasets, transforms\n\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"Random images \u0026 labels for smoke testing.  32\u00d732\u00d73.\"\"\"\n\n    def __init__(self, n: int = 1024, num_classes: int = 10):\n        self.n = n\n        self.num_classes = num_classes\n        self.x = torch.randn(n, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n,))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# Registry for dataset-specific loaders.  New datasets should register here.\nDATASET_LOADERS = {}\n\n\ndef register_dataset(name):\n    def decorator(fn):\n        DATASET_LOADERS[name.lower()] = fn\n        return fn\n\n    return decorator\n\n\n@register_dataset(\"synthetic\")\ndef _load_synthetic(cfg):\n    total = cfg[\"data\"].get(\"total_samples\", 1024)\n    ds = SyntheticImageDataset(n=total, num_classes=cfg[\"data\"].get(\"num_classes\", 10))\n    return ds, cfg[\"data\"].get(\"num_classes\", 10)\n\n\n@register_dataset(\"cifar10\")\ndef _load_cifar10(cfg):\n    tfm = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    root = Path(cfg.get(\"data_dir\", \"./data\"))\n    trainset = datasets.CIFAR10(root=str(root), train=True, download=True, transform=tfm)\n    testset = datasets.CIFAR10(root=str(root), train=False, download=True, transform=tfm)\n    cfg[\"data\"][\"num_classes\"] = 10\n    return (trainset, testset), 10\n\n\nclass Preprocessor:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.dataset_name = cfg[\"dataset\"][\"name\"].lower()\n        if self.dataset_name not in DATASET_LOADERS:\n            raise NotImplementedError(\n                f\"Dataset \u0027{self.dataset_name}\u0027 not registered. Add loader in preprocess.py register_dataset decorator.\")\n\n    def dirichlet_split(self, labels: List[int], num_clients: int, alpha: float) -\u003e List[List[int]]:\n        \"\"\"Dirichlet partitioning over class labels as in FL research.\"\"\"\n        labels = np.array(labels)\n        num_classes = len(np.unique(labels))\n        class_indices = [np.where(labels == y)[0] for y in range(num_classes)]\n        client_indices = [[] for _ in range(num_clients)]\n        for c, idxs in enumerate(class_indices):\n            np.random.shuffle(idxs)\n            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n            proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n            split = np.split(idxs, proportions)\n            for cid, part in enumerate(split):\n                client_indices[cid].extend(part.tolist())\n        return client_indices\n\n    def get_data_loaders(self) -\u003e Tuple[Dict[int, DataLoader], DataLoader]:\n        batch_size = self.cfg[\"dataset\"][\"batch_size\"]\n        num_clients = self.cfg[\"dataset\"][\"num_clients\"]\n        alpha = self.cfg[\"dataset\"].get(\"alpha\", math.inf)\n\n        dataset_obj, num_classes = DATASET_LOADERS[self.dataset_name](self.cfg)\n        self.cfg.setdefault(\"data\", {})\n        self.cfg[\"data\"][\"num_classes\"] = num_classes\n\n        # Handle datasets that return tuple(train,test) vs single dataset\n        if isinstance(dataset_obj, tuple):\n            trainset, testset = dataset_obj\n        else:\n            trainset = dataset_obj\n            testset = dataset_obj  # synthetic reuse for eval\n\n        # Partition trainset\n        if alpha == math.inf:\n            # iid split\n            indices = np.arange(len(trainset))\n            np.random.shuffle(indices)\n            splits = np.array_split(indices, num_clients)\n        else:\n            labels = [trainset[i][1] for i in range(len(trainset))]\n            splits = self.dirichlet_split(labels, num_clients, alpha)\n\n        client_loaders = {}\n        for cid, idxs in enumerate(splits):\n            subset = Subset(trainset, idxs)\n            client_loaders[cid] = DataLoader(subset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n        test_loader = DataLoader(testset, batch_size=256, shuffle=False)\n        return client_loaders, test_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"fedmpq_kd_framework\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"torchvision\u003e=0.15\",\n    \"matplotlib\u003e=3.8\",\n    \"seaborn\u003e=0.13\",\n    \"numpy\u003e=1.23\",\n    \"pandas\u003e=2.1\",\n    \"pyyaml\u003e=6.0\",\n    \"scikit-learn\u003e=1.3\",\n    \"tqdm\u003e=4.66\",\n]\n", "smoke_test_yaml": "experiments:\n  - run_id: baseline_smoke\n    description: |\n      Smoke-test run of FedMPQ baseline with synthetic data.\n      2 clients, 2 communication rounds, tiny CNN.\n    dataset:\n      name: synthetic          # Uses built-in synthetic dataset\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 1\n\n  - run_id: kd_smoke\n    description: |\n      Smoke-test run of FedMPQ-KD with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq_kd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 2\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n", "train_py": "import argparse\nimport json\nimport os\nimport shutil\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\n\nfrom .model import (FedMPQKDWrapper, FedMPQWrapper, get_base_model,\n                    kd_loss)\nfrom .preprocess import Preprocessor\n\n\ndef set_seed(seed: int):\n    \"\"\"Make experiments deterministic as far as possible.\"\"\"\n    import random\n    import torch.backends.cudnn as cudnn\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\ndef average_state_dicts(state_dicts: List[Dict[str, torch.Tensor]]):\n    \"\"\"FedAvg aggregation (simple mean of model parameters).\"\"\"\n    avg_state = {}\n    for k in state_dicts[0]:\n        avg_state[k] = torch.stack([sd[k].float() for sd in state_dicts], 0).mean(0)\n    return avg_state\n\n\ndef evaluate(model: torch.nn.Module, dataloader, device):\n    \"\"\"Return accuracy of MODEL on DATALOADER.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\ndef train_client(model_wrapper_cls, global_state, teacher_state, dataloader, device, cfg):\n    \"\"\"Single client\u0027s local training.\n\n    Args:\n        model_wrapper_cls: FedMPQWrapper or FedMPQKDWrapper\n        global_state: parameters broadcast from server (full-precision)\n        teacher_state: state dict for teacher (only KD variant uses it)\n        dataloader: client data loader\n        device: torch device\n        cfg: experiment config dict\n    Returns:\n        state_dict after local optimisation\n    \"\"\"\n    base_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    base_model.load_state_dict(global_state)\n    base_model = base_model.to(device)\n\n    if model_wrapper_cls is FedMPQKDWrapper:\n        teacher_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n        teacher_model.load_state_dict(teacher_state)\n        teacher_model = teacher_model.to(device)\n        wrapper = model_wrapper_cls(base_model, teacher_model, cfg)\n    else:\n        wrapper = model_wrapper_cls(base_model, cfg)\n\n    wrapper.train()\n    optimizer = torch.optim.SGD(wrapper.parameters(), lr=cfg[\"training\"][\"lr\"], momentum=0.9)\n\n    for _ in range(cfg[\"training\"][\"local_epochs\"]):\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits_student, loss_bit = wrapper.forward_with_bit_loss(x)\n            loss_task = F.cross_entropy(logits_student, y)\n            if model_wrapper_cls is FedMPQKDWrapper:\n                logits_teacher = wrapper.teacher(x)\n                loss_kd = kd_loss(logits_student, logits_teacher, T=cfg[\"kd_params\"][\"T\"])\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit + cfg[\"kd_params\"][\"alpha\"] * loss_kd\n            else:\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit\n            loss.backward()\n            optimizer.step()\n    return wrapper.base_model.state_dict()\n\n\ndef run_experiment(cfg: Dict, results_dir: Path):\n    \"\"\"Main federated loop for one run variation.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Data\n    preproc = Preprocessor(cfg)\n    client_loaders, test_loader = preproc.get_data_loaders()\n    num_clients = len(client_loaders)\n\n    # 2. Global model\n    global_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    global_model = global_model.to(device)\n\n    # 3. Training metadata containers\n    history = []\n\n    for round_idx in range(cfg[\"training\"][\"num_rounds\"]):\n        round_client_states = []\n        teacher_state = global_model.state_dict()\n        for client_id, cl_loader in client_loaders.items():\n            model_wrapper_cls = FedMPQKDWrapper if cfg[\"model\"][\"name\"].lower() == \"fedmpq_kd\" else FedMPQWrapper\n            client_state = train_client(model_wrapper_cls,\n                                        global_model.state_dict(),\n                                        teacher_state,\n                                        cl_loader,\n                                        device,\n                                        cfg)\n            round_client_states.append(client_state)\n        # Aggregate\n        new_global_state = average_state_dicts(round_client_states)\n        global_model.load_state_dict(new_global_state)\n\n        # Evaluate\n        acc = evaluate(global_model, test_loader, device)\n        history.append({\"round\": round_idx + 1, \"test_accuracy\": acc})\n        print(json.dumps({\"run_id\": cfg[\"run_id\"], \"round\": round_idx + 1, \"test_accuracy\": acc}))\n\n    # Persist\n    results_path = results_dir / \"results.json\"\n    with results_path.open(\"w\") as f:\n        json.dump(history, f, indent=2)\n    # save final model\n    torch.save(global_model.state_dict(), results_dir / \"final_model.pt\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation (client-server FL loop)\")\n    parser.add_argument(\"--config\", required=True, help=\"Path to run variation YAML config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store outputs for this run\")\n    args = parser.parse_args()\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    Path(args.results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Echo description first\n    exp_desc = cfg.get(\"description\", \"No description provided \u2013 please fill in.\")\n    print(\"=\" * 80)\n    print(\"Experiment description (run_id = {}):\\n{}\".format(cfg[\"run_id\"], exp_desc))\n    print(\"=\" * 80)\n\n    # Ensure config itself is stored for reproducibility\n    shutil.copy(args.config, Path(args.results_dir) / \"config.yaml\")\n\n    set_seed(cfg[\"training\"].get(\"seed\", 0))\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-1-core-perf
- Description: Objective / Hypothesis: Quantitatively verify that FedMPQ-KD closes ≥25 % of the accuracy gap to full-precision while adding ≤10 % wall-time when local compute per round is scarce (E=1). Ablations (-KD) will show the necessity of the new distillation term.

Models:
 • Student: ResNet-20 (mixed-precision: layer-wise 2–8 bit, avg 4 bit).
 • Teacher (for KD): full-precision ResNet-20 snapshot received from server at the start of the round. Frozen during local training.
 • Baselines/SOTA: original FedMPQ implementation, FedPQ (vector quantisation), full-precision FedAvg upper bound.

Dataset:
 • CIFAR-10, 32×32 RGB.
Pre-processing: per-channel mean/std normalisation, random 32→28 crop & horizontal flip on train only.
Split: 10 clients, Dirichlet α = 0.5 non-IID. Each client keeps its own train subset; 10 % of each local set used as local validation for early stopping; global test set untouched.

Training protocol:
 • Local epochs E = 1, batch 64, SGD lr 0.1, momentum 0.9, cosine decay.
 • 50 communication rounds, 3 independent seeds.
 • Early stopping not allowed; evaluate after each round.
Averaging: report mean±std across seeds; primary score is final top-1 test accuracy, secondary AUC_acc (trapezoidal area under accuracy-vs-round curve).
Selection criterion in plots: last checkpoint.

Cost measurement: client-side wall-time and FLOPs captured with PyTorch profiler, VRAM via nvprof; communication bits measured from update tensor sizes.

Hyper-param analysis: run FedMPQ-KD with α∈{0.25,0.5,1.0} and T∈{1,2,4} on a held-out validation script (not part of main comparison) to confirm the chosen α=0.5, T=2 is near-optimal.

Metrics: Primary – top-1 accuracy; Secondary – AUC_acc, gap_to_full-precision, FLOPs/round, wall-time/round, comm_bits/round, ECE calibration error.

Robustness hooks (recorded but not baseline-plotted): repeat whole experiment once with label-noise 10 % to verify ranking stability.

Example code snippet (local training loop excerpt):
```
for x,y in dl:
    logits_s = student(x)
    logits_t = teacher(x)          # frozen fp32
    loss = ce(logits_s,y) + 0.01*student.bit_reg() + 0.5*kd_loss(logits_s, logits_t, T=2)
    loss.backward(); opt.step()
```
Resource estimate on A100:   GPU RAM 4 GB per process; wall-time per round 15 s full-precision, 10 s compressed, 11 s w/ KD (≈+9 %).
Success criterion: FedMPQ-KD ≥84 % accuracy (≥+2 % over FedMPQ) with ≤11 s/round.
- Run Variations: ['full-precision-FedAvg', 'FedMPQ-baseline', 'FedPQ-SOTA', 'FedMPQ-KD (ours)', 'FedMPQ-noKD-ablation']

# Derived Experiment Code (To be validated)

{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics as sk_metrics\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n\ndef load_history(path: Path) -\u003e pd.DataFrame:\n    with path.open() as f:\n        data = json.load(f)\n    return pd.DataFrame(data)\n\n\ndef auc_accuracy(df: pd.DataFrame) -\u003e float:\n    \"\"\"Area under the accuracy-vs-rounds curve (trapezoidal).\"\"\"\n    return sk_metrics.auc(df[\"round\"], df[\"test_accuracy\"])\n\n\ndef make_line_plot(histories: Dict[str, pd.DataFrame], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    for run_id, df in histories.items():\n        ax.plot(df[\"round\"], df[\"test_accuracy\"], label=run_id)\n        ax.annotate(f\"{df[\u0027test_accuracy\u0027].iat[-1]*100:.2f}%\",\n                    xy=(df[\u0027round\u0027].iat[-1], df[\u0027test_accuracy\u0027].iat[-1]),\n                    textcoords=\"offset points\", xytext=(0, 5))\n    ax.set_xlabel(\"Communication round\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.legend()\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"accuracy_over_rounds.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef make_bar_plot(final_acc: Dict[str, float], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    run_ids = list(final_acc.keys())\n    values = [final_acc[r] for r in run_ids]\n    bars = ax.bar(run_ids, values)\n    ax.set_ylabel(\"Final Test Accuracy\")\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.001, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"final_accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef consolidate_metrics(histories: Dict[str, pd.DataFrame]):\n    metrics = {}\n    for run_id, df in histories.items():\n        metrics[run_id] = {\n            \"final_accuracy\": float(df[\"test_accuracy\"].iat[-1]),\n            \"best_accuracy\": float(df[\"test_accuracy\"].max()),\n            \"auc_accuracy\": float(auc_accuracy(df)),\n            \"num_rounds\": int(df[\"round\"].iat[-1]),\n        }\n    return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate \u0026 compare results of all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Root directory containing per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    # Collect histories\n    histories = {}\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            histories[run_dir.name] = load_history(res_file)\n\n    if not histories:\n        print(\"No results.json files found \u2013 nothing to evaluate.\")\n        return\n\n    # Produce figures\n    make_line_plot(histories, results_dir)\n    final_acc = {k: v[\"test_accuracy\"].iat[-1] for k, v in histories.items()}\n    make_bar_plot(final_acc, results_dir)\n\n    # Consolidated metrics\n    metrics = consolidate_metrics(histories)\n    print(json.dumps({\"comparison_metrics\": metrics}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "experiments:\n  - run_id: full-precision-FedAvg\n    description: |-\n      Full-precision upper-bound using vanilla FedAvg with ResNet-20.\n    dataset:\n      name: cifar10\n      num_clients: 10\n      alpha: 0.5\n      batch_size: 64\n    model:\n      name: fedavg\n      architecture: resnet20\n      bits: 32   # no quantisation\n      lambda_b: 0.0\n    training:\n      num_rounds: 50\n      local_epochs: 1\n      lr: 0.1\n      seed: 0\n\n  - run_id: FedMPQ-baseline\n    description: |-\n      Original FedMPQ with 4-bit mixed-precision weights and bit-regularisation.\n    dataset:\n      name: cifar10\n      num_clients: 10\n      alpha: 0.5\n      batch_size: 64\n    model:\n      name: fedmpq\n      architecture: resnet20\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 50\n      local_epochs: 1\n      lr: 0.1\n      seed: 1\n\n  - run_id: FedPQ-SOTA\n    description: |-\n      FedPQ \u2013 vector-quantisation baseline, here approximated by uniform 4-bit quantisation without bit-regulariser.\n    dataset:\n      name: cifar10\n      num_clients: 10\n      alpha: 0.5\n      batch_size: 64\n    model:\n      name: fedpq\n      architecture: resnet20\n      bits: 4\n      lambda_b: 0.0\n    training:\n      num_rounds: 50\n      local_epochs: 1\n      lr: 0.1\n      seed: 2\n\n  - run_id: FedMPQ-KD (ours)\n    description: |-\n      Proposed FedMPQ-KD with in-round knowledge-distillation (\u03b1=0.5, T=2).\n    dataset:\n      name: cifar10\n      num_clients: 10\n      alpha: 0.5\n      batch_size: 64\n    model:\n      name: fedmpq_kd\n      architecture: resnet20\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 50\n      local_epochs: 1\n      lr: 0.1\n      seed: 3\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n\n  - run_id: FedMPQ-noKD-ablation\n    description: |-\n      Ablation of our method without the KD loss (equivalent to FedMPQ baseline configuration).\n    dataset:\n      name: cifar10\n      num_clients: 10\n      alpha: 0.5\n      batch_size: 64\n    model:\n      name: fedmpq\n      architecture: resnet20\n      bits: 4\n      lambda_b: 0.01\n    training:\n      num_rounds: 50\n      local_epochs: 1\n      lr: 0.1\n      seed: 4\n    # No kd_params because KD is disabled\n\n# End of file\n", "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nSCRIPT_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = SCRIPT_DIR.parent / \"config\"\n\n\nclass Tee:\n    \"\"\"Tee helper duplicating a stream into a log file while echoing to console.\"\"\"\n\n    def __init__(self, log_path: Path, stream):\n        self.file = log_path.open(\"w\")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.stream.write(data)\n        self.file.flush()\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n\ndef run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"w\") as out_f, stderr_path.open(\"w\") as err_f:\n        proc = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream output live\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess \u0027{\u0027 \u0027.join(cmd)}\u0027 exited with {proc.returncode}\")\n\n\ndef load_config(path: Path):\n    return yaml.safe_load(path.read_text())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator \u2013 runs all variations \u0026 evaluation.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error(\"Exactly one of --smoke-test or --full-experiment must be supplied.\")\n\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    cfg = load_config(cfg_file)\n    experiments = cfg.get(\"experiments\", [])\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Write individual run config\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.dump(exp, f)\n\n        print(\"Launching run_id =\", run_id)\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(run_dir)]\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        run_subprocess(cmd, cwd=SCRIPT_DIR.parent, stdout_path=stdout_log, stderr_path=stderr_log)\n\n    # After all runs \u2013 evaluation\n    print(\"All runs finished.  Initiating evaluation\u2026\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    run_subprocess(eval_cmd, cwd=SCRIPT_DIR.parent, stdout_path=results_root / \"evaluate_stdout.log\", stderr_path=results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model architectures and FL-specific wrappers (FedAvg, FedMPQ, FedMPQ-KD, FedPQ).\"\"\"\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n###############################################################################\n# Knowledge-distillation helper                                               #\n###############################################################################\n\ndef kd_loss(logits_student: torch.Tensor, logits_teacher: torch.Tensor, T: float = 2.0):\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction=\"batchmean\") * (T * T)\n\n\n###############################################################################\n# Quantisation utilities                                                      #\n###############################################################################\n\ndef quantize_tensor(tensor: torch.Tensor, num_bits: int = 8):\n    \"\"\"Symmetric uniform quantisation to signed integers.\"\"\"\n    if num_bits \u003e= 32:\n        return tensor  # effectively no quantisation\n    qmax = 2 ** (num_bits - 1) - 1\n    max_val = tensor.abs().max() + 1e-8  # avoid division by zero\n    scale = max_val / qmax\n    qt = torch.round(tensor / scale).clamp(-qmax, qmax)\n    return qt * scale\n\n\n###############################################################################\n# Base models                                                                 #\n###############################################################################\n\n# --- Tiny CNN (used in smoke tests) ---------------------------------------- #\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\n# --- ResNet-20 for CIFAR-10 ------------------------------------------------- #\n\n\ndef _conv3x3(in_planes, planes, stride=1):\n    return nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass _BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super().__init__()\n        self.conv1 = _conv3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = _conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Identity()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return F.relu(out)\n\n\nclass ResNet_CIFAR(nn.Module):\n    def __init__(self, num_blocks: List[int], num_classes: int = 10):\n        super().__init__()\n        self.in_planes = 16\n\n        self.conv1 = _conv3x3(3, 16)\n        self.bn1 = nn.BatchNorm2d(16)\n\n        self.layer1 = self._make_layer(16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(64, num_blocks[2], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64 * _BasicBlock.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n\n    def _make_layer(self, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for s in strides:\n            layers.append(_BasicBlock(self.in_planes, planes, s))\n            self.in_planes = planes * _BasicBlock.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        return self.fc(out)\n\n\ndef resnet20(num_classes=10):\n    return ResNet_CIFAR([3, 3, 3], num_classes=num_classes)\n\n\n# --------------------------------------------------------------------------- #\n\nBASE_MODEL_FACTORY = {\n    \"tiny_cnn\": TinyCNN,\n    \"resnet20\": resnet20,\n}\n\n\ndef get_base_model(name: str, num_classes: int):\n    name = name.lower()\n    if name not in BASE_MODEL_FACTORY:\n        raise NotImplementedError(f\"Unknown architecture \u0027{name}\u0027. Available: {list(BASE_MODEL_FACTORY)}\")\n    return BASE_MODEL_FACTORY[name](num_classes=num_classes)\n\n\n###############################################################################\n# Wrappers                                                                     #\n###############################################################################\n\n\nclass FedMPQWrapper(nn.Module):\n    \"\"\"Mixed-precision quantisation + bit-regularisation (original FedMPQ).\"\"\"\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n        self.bits = cfg[\"model\"].get(\"bits\", 8)\n\n    # -------------------- FL specific helpers ----------------------------- #\n\n    def bit_regulariser(self):\n        reg = 0.0\n        for p in self.base_model.parameters():\n            reg = reg + torch.mean(torch.abs(p))\n        return reg / len(list(self.base_model.parameters()))\n\n    def forward_with_bit_loss(self, x):\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n        bit_loss = self.bit_regulariser()\n        return logits, bit_loss\n\n    # Mandatory for PyTorch optimizers\n    def forward(self, x):\n        logits, _ = self.forward_with_bit_loss(x)\n        return logits\n\n\nclass FedMPQKDWrapper(FedMPQWrapper):\n    \"\"\"FedMPQ with in-round knowledge-distillation.\"\"\"\n\n    def __init__(self, base_model: nn.Module, teacher_model: nn.Module, cfg: Dict):\n        super().__init__(base_model, cfg)\n        self.teacher = teacher_model.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n\n\nclass FedPQWrapper(FedMPQWrapper):\n    \"\"\"Simplified implementation of FedPQ (product quantisation). Here we mimic the\n    effect by uniform quantisation without the bit-regulariser.\"\"\"\n\n    def forward_with_bit_loss(self, x):\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n        return logits, torch.tensor(0.0, device=logits.device)\n\n\nclass FedAvgWrapper(nn.Module):\n    \"\"\"Full-precision FedAvg wrapper (no quantisation, no bit loss).\"\"\"\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n\n    def forward_with_bit_loss(self, x):\n        return self.base_model(x), torch.tensor(0.0, device=x.device)\n\n    def forward(self, x):\n        return self.base_model(x)\n", "preprocess_py": "\"\"\"Data-handling utilities: Dirichlet partitioning, preprocessing and loaders.\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import datasets, transforms\n\n\n###############################################################################\n# Synthetic dataset (for smoke tests)                                          #\n###############################################################################\n\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"Random images \u0026 labels. 32\u00d732\u00d73.\"\"\"\n\n    def __init__(self, n: int = 1024, num_classes: int = 10):\n        self.n = n\n        self.num_classes = num_classes\n        self.x = torch.randn(n, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n,))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n###############################################################################\n# Dataset registry                                                             #\n###############################################################################\n\n\nDATASET_LOADERS: Dict[str, callable] = {}\n\n\ndef register_dataset(name):\n    def _decorator(fn):\n        DATASET_LOADERS[name.lower()] = fn\n        return fn\n\n    return _decorator\n\n\n@register_dataset(\"synthetic\")\ndef _load_synthetic(cfg):\n    total = cfg[\"dataset\"].get(\"total_samples\", 1024)\n    ds = SyntheticImageDataset(n=total, num_classes=cfg[\"dataset\"].get(\"num_classes\", 10))\n    return (ds, ds), 10  # train \u0026 test identical here\n\n\n@register_dataset(\"cifar10\")\ndef _load_cifar10(cfg):\n    mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n    train_tf = transforms.Compose(\n        [\n            transforms.RandomCrop(28),  # 32 \u2192 28 crop as per spec\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    test_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n\n    root = Path(cfg.get(\"data_dir\", \"./data\"))\n    trainset = datasets.CIFAR10(root=str(root), train=True, download=True, transform=train_tf)\n    testset = datasets.CIFAR10(root=str(root), train=False, download=True, transform=test_tf)\n    return (trainset, testset), 10\n\n\n###############################################################################\n# Preprocessor class                                                           #\n###############################################################################\n\n\nclass Preprocessor:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.dataset_name = cfg[\"dataset\"][\"name\"].lower()\n        if self.dataset_name not in DATASET_LOADERS:\n            raise NotImplementedError(\n                f\"Dataset \u0027{self.dataset_name}\u0027 not registered. Add loader via @register_dataset decorator.\"\n            )\n\n    # ------------------------ splitting utilities ------------------------- #\n\n    def _dirichlet_split(self, labels: List[int], num_clients: int, alpha: float):\n        labels = np.asarray(labels)\n        num_classes = len(np.unique(labels))\n        class_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n        client_indices = [[] for _ in range(num_clients)]\n        for c, idxs in enumerate(class_indices):\n            np.random.shuffle(idxs)\n            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n            proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n            split = np.split(idxs, proportions)\n            for cid, part in enumerate(split):\n                client_indices[cid].extend(part.tolist())\n        return client_indices\n\n    # ---------------------------- main API -------------------------------- #\n\n    def get_data_loaders(self) -\u003e Tuple[Dict[int, DataLoader], DataLoader]:\n        batch_size = self.cfg[\"dataset\"][\"batch_size\"]\n        num_clients = self.cfg[\"dataset\"][\"num_clients\"]\n        alpha = self.cfg[\"dataset\"].get(\"alpha\", math.inf)\n\n        (trainset, testset), num_classes = DATASET_LOADERS[self.dataset_name](self.cfg)\n        self.cfg.setdefault(\"data\", {})\n        self.cfg[\"data\"][\"num_classes\"] = num_classes\n\n        # Partitioning\n        if alpha == math.inf:\n            indices = np.random.permutation(len(trainset))\n            splits = np.array_split(indices, num_clients)\n        else:\n            labels = [trainset[i][1] for i in range(len(trainset))]\n            splits = self._dirichlet_split(labels, num_clients, alpha)\n\n        client_loaders: Dict[int, DataLoader] = {}\n        for cid, idxs in enumerate(splits):\n            subset = Subset(trainset, idxs)\n            client_loaders[cid] = DataLoader(\n                subset, batch_size=batch_size, shuffle=True, drop_last=True\n            )\n\n        test_loader = DataLoader(testset, batch_size=256, shuffle=False)\n        return client_loaders, test_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"fedmpq_kd_framework\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"torchvision\u003e=0.15\",\n    \"matplotlib\u003e=3.8\",\n    \"seaborn\u003e=0.13\",\n    \"numpy\u003e=1.23\",\n    \"pandas\u003e=2.1\",\n    \"pyyaml\u003e=6.0\",\n    \"scikit-learn\u003e=1.3\",\n    \"tqdm\u003e=4.66\",\n]\n", "smoke_test_yaml": "experiments:\n  - run_id: fedavg_smoke\n    description: |-\n      Smoke-test full-precision FedAvg on synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedavg\n      architecture: tiny_cnn\n      bits: 32\n      lambda_b: 0.0\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 0\n\n  - run_id: fedmpq_smoke\n    description: |-\n      Smoke-test FedMPQ baseline with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 1\n\n  - run_id: fedpq_smoke\n    description: |-\n      Smoke-test FedPQ baseline with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.0\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 2\n\n  - run_id: fedmpq_kd_smoke\n    description: |-\n      Smoke-test FedMPQ-KD (ours) with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq_kd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 3\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n\n  - run_id: fedmpq_nokd_smoke\n    description: |-\n      Smoke-test ablation run (FedMPQ-noKD) identical to FedMPQ baseline.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 4\n", "train_py": "import argparse\nimport json\nimport os\nimport shutil\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\n\nfrom .model import (\n    FedAvgWrapper,\n    FedMPQKDWrapper,\n    FedMPQWrapper,\n    FedPQWrapper,\n    get_base_model,\n    kd_loss,\n)\nfrom .preprocess import Preprocessor\n\n\ndef set_seed(seed: int):\n    \"\"\"Make experiments deterministic as far as possible.\"\"\"\n\n    import random\n    import torch.backends.cudnn as cudnn\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\ndef average_state_dicts(state_dicts: List[Dict[str, torch.Tensor]]):\n    \"\"\"FedAvg aggregation (simple mean of model parameters).\"\"\"\n\n    avg_state: Dict[str, torch.Tensor] = {}\n    for k in state_dicts[0]:\n        avg_state[k] = torch.stack([sd[k].float() for sd in state_dicts], 0).mean(0)\n    return avg_state\n\n\ndef evaluate(model: torch.nn.Module, dataloader, device):\n    \"\"\"Return accuracy of MODEL on DATALOADER.\"\"\"\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\n###############################################################################\n# Wrapper resolver                                                             #\n###############################################################################\n\n\ndef _resolve_wrapper(name: str):\n    name = name.lower()\n    if name == \"fedmpq_kd\":\n        return FedMPQKDWrapper\n    if name == \"fedmpq\":\n        return FedMPQWrapper\n    if name == \"fedpq\":\n        return FedPQWrapper\n    if name == \"fedavg\":\n        return FedAvgWrapper\n    raise ValueError(f\"Unknown model.name=\u0027{name}\u0027. Must be one of fedavg / fedmpq / fedpq / fedmpq_kd.\")\n\n\n###############################################################################\n# Local-training routine                                                       #\n###############################################################################\n\n\ndef train_client(model_name: str, global_state, teacher_state, dataloader, device, cfg):\n    \"\"\"Single client\u0027s local optimisation step.\"\"\"\n\n    base_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    base_model.load_state_dict(global_state)\n    base_model = base_model.to(device)\n\n    wrapper_cls = _resolve_wrapper(model_name)\n\n    if wrapper_cls is FedMPQKDWrapper:\n        teacher_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n        teacher_model.load_state_dict(teacher_state)\n        teacher_model = teacher_model.to(device)\n        wrapper = wrapper_cls(base_model, teacher_model, cfg)\n    else:\n        wrapper = wrapper_cls(base_model, cfg)\n\n    wrapper.train()\n    optimizer = torch.optim.SGD(wrapper.parameters(), lr=cfg[\"training\"][\"lr\"], momentum=0.9)\n\n    for _ in range(cfg[\"training\"][\"local_epochs\"]):\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits_student, loss_bit = wrapper.forward_with_bit_loss(x)\n            loss_task = F.cross_entropy(logits_student, y)\n\n            if isinstance(wrapper, FedMPQKDWrapper):\n                logits_teacher = wrapper.teacher(x)\n                loss_kd = kd_loss(logits_student, logits_teacher, T=cfg[\"kd_params\"][\"T\"])\n                loss = (\n                    loss_task\n                    + cfg[\"model\"][\"lambda_b\"] * loss_bit\n                    + cfg[\"kd_params\"][\"alpha\"] * loss_kd\n                )\n            else:\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit\n            loss.backward()\n            optimizer.step()\n    return wrapper.base_model.state_dict()\n\n\n###############################################################################\n# Federated loop                                                               #\n###############################################################################\n\ndef run_experiment(cfg: Dict, results_dir: Path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Data\n    preproc = Preprocessor(cfg)\n    client_loaders, test_loader = preproc.get_data_loaders()\n    num_clients = len(client_loaders)\n\n    # 2. Global model\n    global_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    global_model = global_model.to(device)\n\n    history: List[Dict] = []\n\n    for round_idx in range(cfg[\"training\"][\"num_rounds\"]):\n        per_client_states = []\n        teacher_state = global_model.state_dict()  # snapshot for potential KD\n        for cid, cl_loader in client_loaders.items():\n            state = train_client(\n                cfg[\"model\"][\"name\"],\n                global_model.state_dict(),\n                teacher_state,\n                cl_loader,\n                device,\n                cfg,\n            )\n            per_client_states.append(state)\n        # Aggregate\n        global_state = average_state_dicts(per_client_states)\n        global_model.load_state_dict(global_state)\n\n        # Evaluate\n        acc = evaluate(global_model, test_loader, device)\n        history.append({\"round\": round_idx + 1, \"test_accuracy\": acc})\n        print(json.dumps({\"run_id\": cfg[\"run_id\"], \"round\": round_idx + 1, \"test_accuracy\": acc}))\n\n    # Persist\n    (results_dir / \"history.json\").write_text(json.dumps(history, indent=2))\n    torch.save(global_model.state_dict(), results_dir / \"final_model.pt\")\n\n\n###############################################################################\n# CLI                                                                         #\n###############################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation\")\n    parser.add_argument(\"--config\", required=True, help=\"Path to YAML config for this run\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store artefacts of this run\")\n    args = parser.parse_args()\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n\n    Path(args.results_dir).mkdir(parents=True, exist_ok=True)\n    shutil.copy(args.config, Path(args.results_dir) / \"config.yaml\")\n\n    set_seed(cfg[\"training\"].get(\"seed\", 0))\n\n    print(\"=\" * 80)\n    print(f\"Run-id: {cfg[\u0027run_id\u0027]}\")\n    print(cfg.get(\"description\", \"No description provided.\"))\n    print(\"=\" * 80)\n\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "Training script stores metrics under history.json but evaluation script looks for results.json → evaluation will find no files and fail. Rename to results.json or adjust evaluate.py. All other placeholders appear filled, but this inconsistency breaks the pipeline, therefore code not immediately executable."
}
