
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- To prevent labels, titles, and legends from overlapping, use `plt.tight_layout()` before saving the figure.
- All figures must be saved to `{results_dir}/images/` directory in .pdf format (e.g., using `plt.savefig(os.path.join(results_dir, "images", "filename.pdf"), bbox_inches="tight")`).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The `full_experiment.yaml` file defines a list of all experiments to be run (e.g., baseline, proposed, ablations). The `main.py` script reads this file and executes experiments sequentially.

The generated main.py must support:
```bash
# Smoke test (runs a lightweight version of ALL run variations defined in smoke_test.yaml)
uv run python -m src.main --smoke-test --results-dir <path>

# Full experiment (reads full_experiment.yaml, runs all variations sequentially)
uv run python -m src.main --full-experiment --results-dir <path>
```

The `--results-dir` argument is passed from the GitHub Actions workflow and specifies where all outputs (figures, logs, metrics) should be saved.

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:
- `src/train.py`: Logic to run a single experiment variation. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model architecture implementations. It will contain classes for baseline, proposed, and ablation models.
- `src/main.py`: The main orchestrator script. It reads a config file, launches train.py for each experiment sequentially, manages subprocesses, collects and consolidates logs, and finally triggers evaluate.py.
- `pyproject.toml`: Complete project dependencies
- `config/smoke_test.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.
- `config/full_experiment.yaml`: Configuration file template with placeholder structure for run variations. Actual variations will be populated in derive_specific step.

### Key Implementation Focus Areas
1. Algorithm Core: Full implementation of the proposed method with proper abstraction
2. Sequential Execution: main.py executes run variations one at a time in sequential order.
3. Configuration Driven: The entire workflow must be driven by the YAML configuration files.
4. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete.
5. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Core code Validation Feedback




# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "FedMPQ suffers a noticeable accuracy drop when the number of local epochs per communication round is small (e.g., 1).  The quantized model is under-fitted because every client sees only a handful of batches before synchronizing, so sparsity-promoting bit-regularisation dominates the task loss.\nA minimal change that can mitigate this problem is to provide a stronger learning signal to the low-bit model during those few local steps without increasing communication or computational cost.",
    "Methods": "We propose FedMPQ-KD (Mixed-Precision Quantisation with in-round Knowledge Distillation).\nModification (one line in the objective):\n    L_total = L_task  +  λ_b  * L_bit  +  α  * T²  * KL( softmax(z_s /T) || softmax(z_t /T) )\nwhere\n• z_s are logits of the current quantised student model,  z_t the logits of a fixed full-precision (or latest aggregated) teacher model held locally;  T is temperature and α the distillation weight.\n\nProcedure per client (changes in bold):\n1. Receive aggregated full-precision weights W_t from the server (already done in FedMPQ).\n2. Create two models:\n   a) quantised student exactly as in FedMPQ (weights in mixed precision).\n   b) ****freeze a copy of W_t in full precision as teacher****.\n3. For E local epochs, optimise the student with the extended loss above.  The teacher only produces logits; no back-prop.\n4. Send student weight updates as usual.\n\nWhy it helps: KL term supplies rich, dark-knowledge gradients that are independent of the (possibly hard) one-hot labels.  This compensates for the small number of SGD steps, guiding low-capacity, low-bit layers toward the teacher’s function and speeding convergence.  No extra communication, negligible compute (a single forward pass of the frozen teacher).",
    "Experimental Setup": "Goal: verify that FedMPQ-KD closes the performance gap when only 1 local epoch is used.\n• Dataset: CIFAR-10 (10 clients, α=0.5 Dirichlet split).\n• Network: ResNet-20.\n• Budgets: average 4-bit, mixed across layers as in the original paper.\n• Baselines:  (1) FedMPQ (original)  (2) FedMPQ-KD (ours).\n• Hyper-parameters:  λ_b =0.01 (unchanged),  α=0.5,  T=2.\n• Training: 50 communication rounds, 1 local epoch, batch-size 64, SGD lr 0.1.\n• Metric: global test accuracy after every round.\nExpected observation window: accuracy vs. rounds and final accuracy.",
    "Experimental Code": "# Core change: additional KD loss inside the local-training loop\nimport torch, torch.nn as nn, torch.nn.functional as F\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    \"\"\"KL divergence with temperature.\"\"\"\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\n\nclass LocalTrainer:\n    def __init__(self, student, teacher, dataloader, lr=0.1, lambda_b=0.01, alpha=0.5):\n        self.student = student  # quantised weights already applied\n        self.teacher = teacher.eval()  # full-precision copy, frozen\n        self.opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n        self.dl = dataloader\n        self.lambda_b = lambda_b\n        self.alpha = alpha\n\n    def train_one_epoch(self):\n        self.student.train()\n        for x, y in self.dl:\n            logits_s = self.student(x)\n            logits_t = self.teacher(x)\n            loss_task = F.cross_entropy(logits_s, y)\n            loss_bit  = self.student.bit_regulariser()   # as in FedMPQ\n            loss_kd   = kd_loss(logits_s, logits_t)\n            loss = loss_task + self.lambda_b*loss_bit + self.alpha*loss_kd\n            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "Expected Result": "With only 1 local epoch, FedMPQ reaches ≈82% CIFAR-10 accuracy after 50 rounds (reported drop of ~3-4% from full baseline).  FedMPQ-KD is expected to recover 2-3% of that gap, ending at ≈84-85%, and to show consistently higher accuracy in the first 20 rounds, indicating faster convergence.",
    "Expected Conclusion": "A single extra knowledge-distillation term supplies richer gradients to the quantised student during scarce local training, reducing under-fitting without additional communication or heavy computation.  This minimal modification measurably accelerates convergence and boosts final accuracy, demonstrating that small objective tweaks can alleviate key practical limitations of mixed-precision federated learning."
}

# Experimental Design
- Strategy: Objective: Build a single, coherent framework that will let every subsequent experiment answer one overarching question – does FedMPQ-KD provide a strictly better accuracy-vs-cost trade-off than competing quantised FL schemes across realistic operating conditions?

1. Core Hypotheses to Validate
   a. Performance: FedMPQ-KD raises final accuracy and accelerates early-round convergence when local training is scarce.
   b. Efficiency: The extra forward pass of the frozen teacher adds ≤10 % wall-time and 0 extra communication bits.
   c. Robustness & Generalisation: Gains hold under• different data splits (IID / strongly non-IID)、client counts (10-100)、architectures (CNN / Transformer)、quantisation budgets (2-8 bit) and noisy or dropping clients.
   d. Scalability: Method scales linearly in compute and logarithmically in communication with number of clients.

2. Comparison Palette (identical across all experiments)
   • Baselines: (1) FedMPQ (original) (2) Full-precision FedAvg (upper bound) (3) SOTA low-bit FL alternatives (FedPQ, FedKD, Q-FedAvg).
   • Ablations: i) –KD (λ_b+L_task only) ii) varying α, T iii) updating vs freezing the teacher iv) local vs global teacher refresh frequency.

3. Evaluation Angles
   3.1 Quantitative Performance
       – Top-1 accuracy vs communication rounds (primary)
       – Area-under-curve (AUC_acc) to capture convergence speed
       – Final accuracy gap to full-precision
   3.2 Cost Metrics
       – Client-side FLOPs & wall-clock per round (measured on A100)
       – Communication bits / round (should be unchanged)
       – Peak GPU VRAM & host RAM
   3.3 Robustness Metrics
       – Accuracy variance across 3 random seeds
       – Degradation (% drop) under extreme non-IID, stragglers, label noise
   3.4 Qualitative / Diagnostic
       – t-SNE of penultimate-layer features (student vs teacher)
       – Bit-width utilisation histograms
       – Gradient-norm and loss-landscape snapshots to illustrate stronger signals.

4. Experimental Axes (each future experiment picks one axis while keeping the rest fixed)
   A. Local compute budget: E∈{1,2,5}
   B. Quantisation budget: avg bits ∈{2,4,8}
   C. Data heterogeneity: Dirichlet α∈{0.1,0.5,∞}
   D. Model family: ResNet-20, MobileNet-V2, ViT-Small
   E. Scale: clients ∈{10,50,100}; participation rate 10-100 %
   F. Failure modes: 20 % clients randomly drop per round; 10 % label noise.

5. Success Criteria (must hold in ≥80 % of experimental settings)
   • +≥2 % absolute test accuracy OR ≥25 % smaller gap to full-precision compared with best quantised baseline, p<0.05 (t-test over seeds).
   • No statistically significant increase in communication volume.
   • Added wall-time ≤10 % per round on A100.
   • Under worst-case non-IID, KD variant still outperforms FedMPQ by ≥1 %.

6. Protocol & Reproducibility
   • Same optimiser, LR schedule, batch size across methods; only α and T tuned once on a small validation pool and kept fixed.
   • Three independent seeds; report mean±std.
   • Use PyTorch w/ deterministic flags; log hardware utilisation via NVIDIA-smi and PyTorch profiler.
   • Release code & logs; each run stores JSON metadata capturing hyper-params, seed, commit hash.

7. Resource Awareness
   • Single NVIDIA A100 (80 GB) easily fits teacher + student (≤4 GB each). Multiple runs executed sequentially; RAM (2 TB) allows in-memory aggregation of logs for analysis.

By following this common strategy—fixed baselines, uniform metrics, multifaceted validation axes, and rigid success criteria—each forthcoming experiment will contribute a comparable slice of evidence, together establishing the robustness, efficiency and overall superiority of FedMPQ-KD.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport shutil\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\n\nfrom .model import (FedMPQKDWrapper, FedMPQWrapper, get_base_model,\n                    kd_loss)\nfrom .preprocess import Preprocessor\n\n\ndef set_seed(seed: int):\n    \"\"\"Make experiments deterministic as far as possible.\"\"\"\n    import random\n    import torch.backends.cudnn as cudnn\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\ndef average_state_dicts(state_dicts: List[Dict[str, torch.Tensor]]):\n    \"\"\"FedAvg aggregation (simple mean of model parameters).\"\"\"\n    avg_state = {}\n    for k in state_dicts[0]:\n        avg_state[k] = torch.stack([sd[k].float() for sd in state_dicts], 0).mean(0)\n    return avg_state\n\n\ndef evaluate(model: torch.nn.Module, dataloader, device):\n    \"\"\"Return accuracy of MODEL on DATALOADER.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\ndef train_client(model_wrapper_cls, global_state, teacher_state, dataloader, device, cfg):\n    \"\"\"Single client's local training.\n\n    Args:\n        model_wrapper_cls: FedMPQWrapper or FedMPQKDWrapper\n        global_state: parameters broadcast from server (full-precision)\n        teacher_state: state dict for teacher (only KD variant uses it)\n        dataloader: client data loader\n        device: torch device\n        cfg: experiment config dict\n    Returns:\n        state_dict after local optimisation\n    \"\"\"\n    base_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    base_model.load_state_dict(global_state)\n    base_model = base_model.to(device)\n\n    if model_wrapper_cls is FedMPQKDWrapper:\n        teacher_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n        teacher_model.load_state_dict(teacher_state)\n        teacher_model = teacher_model.to(device)\n        wrapper = model_wrapper_cls(base_model, teacher_model, cfg)\n    else:\n        wrapper = model_wrapper_cls(base_model, cfg)\n\n    wrapper.train()\n    optimizer = torch.optim.SGD(wrapper.parameters(), lr=cfg[\"training\"][\"lr\"], momentum=0.9)\n\n    for _ in range(cfg[\"training\"][\"local_epochs\"]):\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits_student, loss_bit = wrapper.forward_with_bit_loss(x)\n            loss_task = F.cross_entropy(logits_student, y)\n            if model_wrapper_cls is FedMPQKDWrapper:\n                logits_teacher = wrapper.teacher(x)\n                loss_kd = kd_loss(logits_student, logits_teacher, T=cfg[\"kd_params\"][\"T\"])\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit + cfg[\"kd_params\"][\"alpha\"] * loss_kd\n            else:\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit\n            loss.backward()\n            optimizer.step()\n    return wrapper.base_model.state_dict()\n\n\ndef run_experiment(cfg: Dict, results_dir: Path):\n    \"\"\"Main federated loop for one run variation.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Data\n    preproc = Preprocessor(cfg)\n    client_loaders, test_loader = preproc.get_data_loaders()\n    num_clients = len(client_loaders)\n\n    # 2. Global model\n    global_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    global_model = global_model.to(device)\n\n    # 3. Training metadata containers\n    history = []\n\n    for round_idx in range(cfg[\"training\"][\"num_rounds\"]):\n        round_client_states = []\n        teacher_state = global_model.state_dict()\n        for client_id, cl_loader in client_loaders.items():\n            model_wrapper_cls = FedMPQKDWrapper if cfg[\"model\"][\"name\"].lower() == \"fedmpq_kd\" else FedMPQWrapper\n            client_state = train_client(model_wrapper_cls,\n                                        global_model.state_dict(),\n                                        teacher_state,\n                                        cl_loader,\n                                        device,\n                                        cfg)\n            round_client_states.append(client_state)\n        # Aggregate\n        new_global_state = average_state_dicts(round_client_states)\n        global_model.load_state_dict(new_global_state)\n\n        # Evaluate\n        acc = evaluate(global_model, test_loader, device)\n        history.append({\"round\": round_idx + 1, \"test_accuracy\": acc})\n        print(json.dumps({\"run_id\": cfg[\"run_id\"], \"round\": round_idx + 1, \"test_accuracy\": acc}))\n\n    # Persist\n    results_path = results_dir / \"results.json\"\n    with results_path.open(\"w\") as f:\n        json.dump(history, f, indent=2)\n    # save final model\n    torch.save(global_model.state_dict(), results_dir / \"final_model.pt\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation (client-server FL loop)\")\n    parser.add_argument(\"--config\", required=True, help=\"Path to run variation YAML config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store outputs for this run\")\n    args = parser.parse_args()\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    Path(args.results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Echo description first\n    exp_desc = cfg.get(\"description\", \"No description provided – please fill in.\")\n    print(\"=\" * 80)\n    print(\"Experiment description (run_id = {}):\\n{}\".format(cfg[\"run_id\"], exp_desc))\n    print(\"=\" * 80)\n\n    # Ensure config itself is stored for reproducibility\n    shutil.copy(args.config, Path(args.results_dir) / \"config.yaml\")\n\n    set_seed(cfg[\"training\"].get(\"seed\", 0))\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics as sk_metrics\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n\ndef load_history(path: Path) -> pd.DataFrame:\n    with path.open() as f:\n        data = json.load(f)\n    return pd.DataFrame(data)\n\n\ndef auc_accuracy(df: pd.DataFrame) -> float:\n    \"\"\"Area under the accuracy-vs-rounds curve (trapezoidal).\"\"\"\n    return sk_metrics.auc(df[\"round\"], df[\"test_accuracy\"])\n\n\ndef make_line_plot(histories: Dict[str, pd.DataFrame], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    for run_id, df in histories.items():\n        ax.plot(df[\"round\"], df[\"test_accuracy\"], label=run_id)\n        ax.annotate(f\"{df['test_accuracy'].iat[-1]*100:.2f}%\",\n                    xy=(df['round'].iat[-1], df['test_accuracy'].iat[-1]),\n                    textcoords=\"offset points\", xytext=(0, 5))\n    ax.set_xlabel(\"Communication round\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.legend()\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"accuracy_over_rounds.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef make_bar_plot(final_acc: Dict[str, float], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    run_ids = list(final_acc.keys())\n    values = [final_acc[r] for r in run_ids]\n    bars = ax.bar(run_ids, values)\n    ax.set_ylabel(\"Final Test Accuracy\")\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.001, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"final_accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef consolidate_metrics(histories: Dict[str, pd.DataFrame]):\n    metrics = {}\n    for run_id, df in histories.items():\n        metrics[run_id] = {\n            \"final_accuracy\": float(df[\"test_accuracy\"].iat[-1]),\n            \"best_accuracy\": float(df[\"test_accuracy\"].max()),\n            \"auc_accuracy\": float(auc_accuracy(df)),\n            \"num_rounds\": int(df[\"round\"].iat[-1]),\n        }\n    return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate & compare results of all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Root directory containing per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    # Collect histories\n    histories = {}\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            histories[run_dir.name] = load_history(res_file)\n\n    if not histories:\n        print(\"No results.json files found – nothing to evaluate.\")\n        return\n\n    # Produce figures\n    make_line_plot(histories, results_dir)\n    final_acc = {k: v[\"test_accuracy\"].iat[-1] for k, v in histories.items()}\n    make_bar_plot(final_acc, results_dir)\n\n    # Consolidated metrics\n    metrics = consolidate_metrics(histories)\n    print(json.dumps({\"comparison_metrics\": metrics}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Common data-handling utilities with dataset placeholders.\n\nThis module implements\n    • Dirichlet non-IID splitting\n    • Synthetic dataset for smoke tests\n    • Helper that returns (client_id ⇒ DataLoader)  + global test loader\n\nDataset-specific loading logic should be added in the derived step by\nregistering loaders in DATASET_LOADERS.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import datasets, transforms\n\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"Random images & labels for smoke testing.  32×32×3.\"\"\"\n\n    def __init__(self, n: int = 1024, num_classes: int = 10):\n        self.n = n\n        self.num_classes = num_classes\n        self.x = torch.randn(n, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n,))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# Registry for dataset-specific loaders.  New datasets should register here.\nDATASET_LOADERS = {}\n\n\ndef register_dataset(name):\n    def decorator(fn):\n        DATASET_LOADERS[name.lower()] = fn\n        return fn\n\n    return decorator\n\n\n@register_dataset(\"synthetic\")\ndef _load_synthetic(cfg):\n    total = cfg[\"data\"].get(\"total_samples\", 1024)\n    ds = SyntheticImageDataset(n=total, num_classes=cfg[\"data\"].get(\"num_classes\", 10))\n    return ds, cfg[\"data\"].get(\"num_classes\", 10)\n\n\n@register_dataset(\"cifar10\")\ndef _load_cifar10(cfg):\n    tfm = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    root = Path(cfg.get(\"data_dir\", \"./data\"))\n    trainset = datasets.CIFAR10(root=str(root), train=True, download=True, transform=tfm)\n    testset = datasets.CIFAR10(root=str(root), train=False, download=True, transform=tfm)\n    cfg[\"data\"][\"num_classes\"] = 10\n    return (trainset, testset), 10\n\n\nclass Preprocessor:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.dataset_name = cfg[\"dataset\"][\"name\"].lower()\n        if self.dataset_name not in DATASET_LOADERS:\n            raise NotImplementedError(\n                f\"Dataset '{self.dataset_name}' not registered. Add loader in preprocess.py register_dataset decorator.\")\n\n    def dirichlet_split(self, labels: List[int], num_clients: int, alpha: float) -> List[List[int]]:\n        \"\"\"Dirichlet partitioning over class labels as in FL research.\"\"\"\n        labels = np.array(labels)\n        num_classes = len(np.unique(labels))\n        class_indices = [np.where(labels == y)[0] for y in range(num_classes)]\n        client_indices = [[] for _ in range(num_clients)]\n        for c, idxs in enumerate(class_indices):\n            np.random.shuffle(idxs)\n            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n            proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n            split = np.split(idxs, proportions)\n            for cid, part in enumerate(split):\n                client_indices[cid].extend(part.tolist())\n        return client_indices\n\n    def get_data_loaders(self) -> Tuple[Dict[int, DataLoader], DataLoader]:\n        batch_size = self.cfg[\"dataset\"][\"batch_size\"]\n        num_clients = self.cfg[\"dataset\"][\"num_clients\"]\n        alpha = self.cfg[\"dataset\"].get(\"alpha\", math.inf)\n\n        dataset_obj, num_classes = DATASET_LOADERS[self.dataset_name](self.cfg)\n        self.cfg.setdefault(\"data\", {})\n        self.cfg[\"data\"][\"num_classes\"] = num_classes\n\n        # Handle datasets that return tuple(train,test) vs single dataset\n        if isinstance(dataset_obj, tuple):\n            trainset, testset = dataset_obj\n        else:\n            trainset = dataset_obj\n            testset = dataset_obj  # synthetic reuse for eval\n\n        # Partition trainset\n        if alpha == math.inf:\n            # iid split\n            indices = np.arange(len(trainset))\n            np.random.shuffle(indices)\n            splits = np.array_split(indices, num_clients)\n        else:\n            labels = [trainset[i][1] for i in range(len(trainset))]\n            splits = self.dirichlet_split(labels, num_clients, alpha)\n\n        client_loaders = {}\n        for cid, idxs in enumerate(splits):\n            subset = Subset(trainset, idxs)\n            client_loaders[cid] = DataLoader(subset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n        test_loader = DataLoader(testset, batch_size=256, shuffle=False)\n        return client_loaders, test_loader\n",
    "model_py": "\"\"\"Model definitions and wrappers implementing FedMPQ & FedMPQ-KD logic.\"\"\"\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n###############################################################################\n# Utility functions                                                            #\n###############################################################################\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction=\"batchmean\") * (T * T)\n\n\ndef quantize_tensor(t: torch.Tensor, num_bits: int = 8):\n    if num_bits >= 32:\n        return tensor  # No quantisation needed\n    qmin = -2 ** (num_bits - 1)\n    qmax = 2 ** (num_bits - 1) - 1\n    min_val, max_val = tensor.min(), tensor.max()\n    scale = (max_val - min_val) / float(qmax - qmin + 1e-8)\n    zp = qmin - torch.round(min_val / scale)\n    qt = torch.clamp(torch.round(tensor / scale + zp), qmin, qmax)\n    return (qt - zp) * scale\n\n\n###############################################################################\n# Tiny CNN (default when architecture placeholder not replaced)                #\n###############################################################################\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nBASE_MODEL_FACTORY = {\n    \"tiny_cnn\": TinyCNN,\n    \"MODEL_PLACEHOLDER\": None,  # PLACEHOLDER: Replace with specific model class (e.g., ResNet20)\n}\n\n\ndef get_base_model(name: str, num_classes: int):\n    name = name.lower()\n    if name not in BASE_MODEL_FACTORY or BASE_MODEL_FACTORY[name] is None:\n        raise NotImplementedError(f\"Model architecture '{name}' not implemented.  Replace placeholder in model.py.\")\n    return BASE_MODEL_FACTORY[name](num_classes=num_classes)\n\n\n###############################################################################\n# FedMPQ wrappers                                                              #\n###############################################################################\n\n\nclass FedMPQWrapper(nn.Module):\n    \"\"\"Wrap a base model with weight quantisation & bit-regularisation.\"\"\"\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n        self.bits = cfg[\"model\"].get(\"bits\", 8)\n        self.lambda_b = cfg[\"model\"].get(\"lambda_b\", 0.01)\n\n    def forward_with_bit_loss(self, x):\n        # Fake quantise weights on-the-fly\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        # Restore original weights to keep gradients correct\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n\n        bit_loss = self.bit_regulariser()\n        return logits, bit_loss\n\n    def forward(self, x):\n        logits, _ = self.forward_with_bit_loss(x)\n        return logits\n\n    def bit_regulariser(self):\n        reg = 0.0\n        for p in self.base_model.parameters():\n            reg = reg + torch.sum(torch.abs(p)) / p.numel()\n        return reg\n\n\nclass FedMPQKDWrapper(FedMPQWrapper):\n    def __init__(self, base_model: nn.Module, teacher_model: nn.Module, cfg: Dict):\n        super().__init__(base_model, cfg)\n        self.teacher = teacher_model.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n",
    "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nSCRIPT_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = SCRIPT_DIR.parent / \"config\"\n\n\nclass Tee:\n    \"\"\"Tee helper duplicating a stream into a log file while echoing to console.\"\"\"\n\n    def __init__(self, log_path: Path, stream):\n        self.file = log_path.open(\"w\")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.stream.write(data)\n        self.file.flush()\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n\ndef run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"w\") as out_f, stderr_path.open(\"w\") as err_f:\n        proc = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream output live\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess '{' '.join(cmd)}' exited with {proc.returncode}\")\n\n\ndef load_config(path: Path):\n    return yaml.safe_load(path.read_text())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator – runs all variations & evaluation.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error(\"Exactly one of --smoke-test or --full-experiment must be supplied.\")\n\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    cfg = load_config(cfg_file)\n    experiments = cfg.get(\"experiments\", [])\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Write individual run config\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.dump(exp, f)\n\n        print(\"Launching run_id =\", run_id)\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(run_dir)]\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        run_subprocess(cmd, cwd=SCRIPT_DIR.parent, stdout_path=stdout_log, stderr_path=stderr_log)\n\n    # After all runs – evaluation\n    print(\"All runs finished.  Initiating evaluation…\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    run_subprocess(eval_cmd, cwd=SCRIPT_DIR.parent, stdout_path=results_root / \"evaluate_stdout.log\", stderr_path=results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"fedmpq_kd_framework\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15\",\n    \"matplotlib>=3.8\",\n    \"seaborn>=0.13\",\n    \"numpy>=1.23\",\n    \"pandas>=2.1\",\n    \"pyyaml>=6.0\",\n    \"scikit-learn>=1.3\",\n    \"tqdm>=4.66\",\n]\n",
    "smoke_test_yaml": "experiments:\n  - run_id: baseline_smoke\n    description: |\n      Smoke-test run of FedMPQ baseline with synthetic data.\n      2 clients, 2 communication rounds, tiny CNN.\n    dataset:\n      name: synthetic          # Uses built-in synthetic dataset\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 1\n\n  - run_id: kd_smoke\n    description: |\n      Smoke-test run of FedMPQ-KD with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq_kd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 2\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n",
    "full_experiment_yaml": "experiments:\n  # PLACEHOLDER: Populate with real variations in derived-specific step\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: \"# PLACEHOLDER: Detailed description of the experimental condition\"\n    dataset:\n      name: DATASET_PLACEHOLDER          # e.g., cifar10\n      num_clients: SPECIFIC_CONFIG_PLACEHOLDER\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER             # fedmpq / fedmpq_kd / baseline etc.\n      architecture: ARCHITECTURE_PLACEHOLDER  # resnet20 etc.\n      bits: SPECIFIC_CONFIG_PLACEHOLDER\n      lambda_b: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      num_rounds: SPECIFIC_CONFIG_PLACEHOLDER\n      local_epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      seed: SPECIFIC_CONFIG_PLACEHOLDER\n    kd_params:\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      T: SPECIFIC_CONFIG_PLACEHOLDER\n  # Add more experiments as needed\n"
}
