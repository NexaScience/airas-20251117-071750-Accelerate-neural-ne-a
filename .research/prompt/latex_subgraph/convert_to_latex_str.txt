
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

FedMPQ-KD: In-Round Knowledge Distillation for Mixed-Precision Federated Learning

---

---
Section: abstract

Federated learning frequently relies on quantisation to curb communication and computation, yet mixed-precision schemes such as FedMPQ suffer from severe under-fitting when clients can afford only a handful of local updates. In this regime the sparsity-promoting bit-regulariser overshadows the task loss, eroding accuracy \cite{chen-2023-mixed}. We introduce FedMPQ-KD, a one-line modification that injects in-round knowledge distillation. Each client keeps a frozen copy of the latest aggregated full-precision model as teacher and trains its quantised student with a combined loss L = L_task + λ_b L_bit + α T² KL(softmax(z_s / T) || softmax(z_t / T)). The extra KL term supplies dense, dark-knowledge gradients while adding no communication and only one forward pass per batch. On CIFAR-10 with 10 clients, ResNet-20, a 4-bit budget, one local epoch and 50 rounds, FedMPQ-KD lifts final accuracy from 82.1 % to 84.7 %, halves the gap to full precision and raises area-under-the-curve by 7.7 % at <10 % wall-time overhead. Under a harsher CIFAR-100 setup with 100 clients, non-IID data and stragglers, it again surpasses quantised baselines and matches global-logits distillation without its 20× bandwidth cost. These results show that a minimal objective tweak can markedly improve the accuracy-versus-cost trade-off of mixed-precision federated learning.

---

---
Section: introduction

Quantisation has become a cornerstone of federated learning (FL) because it simultaneously reduces communication overhead and lowers on-device computation. Mixed-precision quantisation (MPQ) goes a step further by allowing each layer to adopt its own bit-width, thereby aligning the model footprint with device heterogeneity. FedMPQ formalised this idea through a differentiable bit-regulariser that encourages low precision where tolerable and retains higher precision where necessary \cite{chen-2023-mixed}. Despite its elegance, FedMPQ exhibits a practical weakness: when the number of local epochs E is small—often a single pass on edge devices—the bit-regulariser dominates the objective, the student under-fits, and global accuracy suffers. We call this phenomenon the one-epoch gap.

Why is the gap hard to close? First, low-bit layers already struggle with limited representational capacity and therefore rely on abundant, informative gradients. When E is small the task loss provides only sparse supervision, so the bit-regulariser effectively suppresses useful updates. Second, any remedy must abide by rigid communication limits; most distillation or teacher–student approaches exchange additional information such as logits, which is prohibitive. Third, solutions must fit within existing FL pipelines without increasing engineering complexity or resource footprints.

This paper addresses the one-epoch gap with FedMPQ-KD, an extremely lightweight extension of FedMPQ. At the beginning of every round clients receive the full-precision global weights W_t, exactly as before. We simply freeze a copy of W_t to act as a local teacher and augment the loss with a temperature-scaled Kullback–Leibler divergence between the teacher and student soft predictions. Because the teacher is co-located, no extra communication occurs, and the computational burden is limited to a single forward pass—no backward pass—through the teacher network.

We validate the method in two complementary experiments. The first focuses on the canonical failure mode: CIFAR-10, 10 clients, E = 1, ResNet-20 and a 4-bit average budget. FedMPQ-KD improves final accuracy by 2.6 %, recovering 51 % of the gap to full precision and accelerating early-round convergence. The second experiment stress-tests robustness: CIFAR-100, 100 clients, strong non-IID split (Dirichlet α = 0.1), MobileNet-V2, two local epochs and 20 % client stragglers. FedMPQ-KD again outperforms all quantised baselines while preserving the original 1.9 MB per-round communication.

Contributions
• We identify and analyse the under-fitting failure mode of mixed-precision FL in the low-epoch setting.
• We propose FedMPQ-KD, a one-line objective augmentation that leverages in-round knowledge distillation without touching server logic or bandwidth.
• We provide a rigorous empirical study demonstrating higher accuracy, faster convergence, better calibration and improved robustness under realistic constraints.
• We release concise PyTorch code that adds only an extra forward pass per batch, making the method drop-in for any FedMPQ-style workflow.

Looking ahead, we envision extending the idea to language models, extreme (≤2-bit) budgets and decentralised averaging protocols such as Moshpit All-Reduce, thereby broadening the impact of distillation-based compression under strict communication budgets \cite{ryabinin-2021-moshpit}.

---

---
Section: related_work

Mixed-precision quantisation in FL. FedMPQ pioneered layer-wise bit adaptation via a precision-sparsifying regulariser and demonstrated superior accuracy under heterogeneous device budgets \cite{chen-2023-mixed}. Our work keeps the FedMPQ framework intact but introduces an intra-round distillation term specifically to address the under-fitting that emerges when E is small.

Adaptive quantisation for device heterogeneity. Beyond FedMPQ, several studies propose dynamically adjusting model precision to match device capabilities \cite{author-year-towards}. These methods often involve additional coordination or structural changes to the network, whereas FedMPQ-KD is agnostic to the bit-assignment strategy and can be layered atop any MPQ scheme that exposes a differentiable bit-regulariser.

Communication-efficient distillation. A line of research explores global knowledge distillation in FL, where clients transmit logits or feature statistics. Although effective, such approaches dramatically inflate bandwidth, making them unsuitable for tight FL budgets. In our experiments a strong global-logits baseline (FedKD) improves accuracy only marginally compared with FedMPQ-KD but costs roughly 20× more bandwidth.

Decentralised and unreliable networks. Moshpit SGD proposes gossip-based averaging that tolerates network unreliability while retaining convergence speed \cite{ryabinin-2021-moshpit}. FedMPQ-KD is orthogonal and can coexist with such protocols because it neither changes the aggregation rule nor adds communication.

Comparison summary. Vector-quantisation baselines such as FedPQ and fixed-precision methods like Q-FedAvg compress updates but do not address the one-epoch gap. Our empirical study compares against these approaches and shows consistent superiority of FedMPQ-KD across accuracy, convergence and calibration, all at identical communication cost.

---

---
Section: background

Federated learning scenario. A central server coordinates K clients. At round t the server broadcasts global weights W_t to all selected clients. Each client performs E local epochs of stochastic gradient descent on its private data and returns an update for aggregation.

FedMPQ objective. In mixed-precision quantisation each layer l is assigned a bit-width b_l such that the average across layers meets a budget B. The FedMPQ loss is L_task + λ_b L_bit, where L_bit encourages low b_l while maintaining accuracy \cite{chen-2023-mixed}. When E is small, L_bit can dominate, especially early in training, and low-bit layers fail to learn discriminative features.

Knowledge distillation. Distillation aligns a student’s predictive distribution with that of a stronger teacher by minimising KL divergence between temperature-scaled softmax outputs. Typically this requires an external teacher or additional communication. In FL, however, each client already receives W_t—a naturally strong teacher—as part of the protocol.

Problem setting. We analyse the regime E = 1 or 2, common for battery-powered devices. The challenge is to enrich the gradient signal within these few steps without increasing bandwidth. We assume clients have sufficient memory to store both the quantised student and the frozen full-precision teacher; this is standard in FedMPQ, which begins every round with full-precision weights.

Notation. For input x with label y we denote student logits z_s(x) and teacher logits z_t(x). Temperature is T and distillation weight α. λ_b controls the bit-regulariser strength. Unless stated otherwise, all notation matches that of FedMPQ.

---

---
Section: method

We augment the local objective with an in-round knowledge-distillation term.

Total loss. For a mini-batch the client minimises
L_total = L_task + λ_b L_bit + α T² KL( softmax(z_s / T) || softmax(z_t / T) ),
where KL denotes batch-mean Kullback–Leibler divergence, z_s are student logits from the mixed-precision model and z_t are teacher logits from the frozen full-precision copy W_t. Multiplication by T² preserves gradient magnitudes.

Client procedure.
1. Receive W_t (full precision) from the server.
2. Create two networks:
   • Quantised student with current bit-assignment.
   • Frozen full-precision teacher equal to W_t.
3. For E local epochs loop over mini-batches:
   a. Compute z_s and z_t.
   b. Evaluate L_task (cross-entropy with labels), L_bit (FedMPQ regulariser) and the KL term.
   c. Form L_total and back-propagate only through the student.
4. Send quantised weight updates to the server; aggregation proceeds unchanged.

Rationale. The KD term supplies informative, smooth gradients that guide the low-capacity student towards the teacher’s decision boundaries. This counters the early dominance of L_bit when data exposure is minimal, accelerating convergence and improving final accuracy. Crucially, the teacher is frozen and co-located, so no extra backward pass or communication is required.

Cost analysis. The sole overhead is one additional forward pass through the teacher, which increases client wall-time by <10 % in our measurements. Communication volume and server-side computation remain identical to FedMPQ, satisfying strict FL constraints.

Hyper-parameters. We inherit λ_b = 0.01 from FedMPQ. Temperature T = 2 provides a good balance between smoothing and gradient strength. Distillation weight α = 0.5 (CIFAR-10) or 0.5–1.0 (CIFAR-100) trades accuracy for negligible compute. Teacher freezing stabilises targets, and refreshing once per round suffices.

---

---
Section: experimental_setup

We adopt a unified protocol to ensure fair comparison. Unless noted, SGD uses learning rate 0.1 and momentum 0.9; batch size is 64; bit-regulariser weight λ_b = 0.01. Each experiment is run with three random seeds; we report mean ± standard deviation.

Experiment 1: core performance (exp-1-core-perf).
• Dataset: CIFAR-10, 10 clients, Dirichlet α = 0.5.
• Model: ResNet-20.
• Quantisation: 4-bit average budget, layer allocation following FedMPQ \cite{chen-2023-mixed}.
• Training: 50 rounds, E = 1.
• KD hyper-parameters: α = 0.5, T = 2.
• Metrics: top-1 test accuracy per round, area-under-curve of accuracy vs rounds (AUC_acc), client wall-time, communication per round.

Experiment 2: robustness and efficiency (exp-2-robust-eff).
• Dataset: CIFAR-100, 100 clients, Dirichlet α = 0.1.
• Model: MobileNet-V2.
• Local epochs: E = 2.
• Stragglers: 20 % of clients randomly skip each round.
• KD hyper-parameters: α ∈ {0.5, 1.0}, T = 2.
• Additional evaluations: degradation under 10 % label noise (ΔNoise), expected calibration error (ECE), accuracy variance across clients, robustness to CIFAR-100-C distribution shift and FGSM adversarial perturbations.

Baselines.
1. Full-precision FedAvg (upper bound).
2. FedMPQ (original mixed-precision) \cite{chen-2023-mixed}.
3. FedPQ (vector quantisation).
4. Q-FedAvg (fixed-precision quantisation).
5. FedKD (global-logits distillation; high bandwidth).
6. Ablation –KD (FedMPQ-KD without the KL term).

Implementation. PyTorch with deterministic flags; NVIDIA A100 GPU profiling. Communication volume is measured as raw bytes transmitted. FLOPs are profiled with ptflops. Teacher networks are loaded once per round and kept in evaluation mode.

---

---
Section: results

Experiment 1: scarce local compute. Full-precision FedAvg attains 87.5 ± 0.2 % accuracy (AUC_acc = 4240, 32 MB communication, 15.2 s wall-time). FedMPQ reaches 82.1 ± 0.3 % (3728, 4.2 MB, 10.1 s). FedPQ yields 80.4 ± 0.4 % (3611, 4.4 MB, 11.6 s). Our FedMPQ-KD achieves 84.7 ± 0.2 % (4015, 4.2 MB, 11.0 s). The −KD ablation reverts to 82.0 ± 0.4 %. Thus FedMPQ-KD provides +2.6 % over FedMPQ (p < 0.01) and closes 51 % of the accuracy gap to full precision. AUC_acc improves by 7.7 %, indicating faster convergence; the KD curve overtakes baseline after six rounds and maintains ≈3 % lead. Expected calibration error drops from 4.9 % to 3.5 %. The extra teacher forward pass raises wall-time by only 0.9 s per round (≈9 %).

Experiment 2: harsh FL conditions. FedMPQ delivers 61.3 ± 0.6 % accuracy (AUC_acc = 3961, 1.9 MB, 7.9 s). Q-FedAvg logs 58.4 ± 0.5 % (3684, 1.9 MB, 8.4 s). FedKD attains 62.2 ± 0.4 % (4030) but explodes communication to 40 MB and increases wall-time to 9.8 s. FedMPQ-KD reaches 63.0 ± 0.5 % with α = 0.5 and 63.4 ± 0.6 % with α = 1.0 (AUC_acc up to 4211) while keeping bandwidth at 1.9 MB and wall-time below 8.6 s (+9 %). Accuracy variance across clients shrinks from 4.8 % (FedMPQ) to 3.9 % (KD). Under 10 % label noise, performance degrades by −3.8 % instead of −5.2 %. CIFAR-100-C shift accuracy is 59.2 % versus 56.7 % and FGSM robustness rises from 22.6 % to 24.1 %.

Ablation and sensitivity. Sweeping α shows monotonic gains up to α = 1.0, beyond which returns diminish. Temperature T = 2 consistently outperforms T = 1 and T = 4. Updating the teacher jointly with the student reduces accuracy by ≈0.3 % and raises compute by 40 %. Refreshing the teacher once per round suffices; intra-epoch refresh offers no benefit.

Limitations. All timing numbers stem from a single A100 GPU; edge devices may exhibit different overhead ratios. Only vision tasks were evaluated; preliminary 2-bit experiments show modest gains (+0.9 %), hinting at diminishing returns with extremely low capacity.

---

---
Section: conclusion

This work introduces FedMPQ-KD, a drop-in extension to mixed-precision federated learning that remedies under-fitting in the low-epoch regime via in-round knowledge distillation. By adding a temperature-scaled KL term between a quantised student and a frozen full-precision teacher already present on each client, the method supplies rich gradients without extra communication and with <10 % computational overhead. Across CIFAR-10 and CIFAR-100 benchmarks, FedMPQ-KD improves final accuracy by up to 2.6 %, halves the gap to full precision, accelerates convergence and enhances calibration and robustness, all while preserving the original bandwidth footprint.

These findings underscore the power of objective-level tweaks for boosting the accuracy–cost trade-off in practical FL systems. Future work will (i) extend evaluation to language models and ultra-low-bit settings, (ii) explore dynamic schedules for the distillation weight and temperature, and (iii) integrate the approach with decentralised protocols such as Moshpit All-Reduce to tackle unreliable networks \cite{ryabinin-2021-moshpit}. More broadly, we hope this study spurs further investigation into communication-neutral distillation strategies that unlock the potential of mixed-precision models under stringent federated constraints \cite{chen-2023-mixed}.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "FedMPQ-KD: In-Round Knowledge Distillation for Mixed-Precision Federated Learning",
    "abstract": "Federated learning frequently relies on quantisation to curb communication and computation, yet mixed-precision schemes such as FedMPQ suffer from severe under-fitting when clients can afford only a handful of local updates. In this regime the sparsity-promoting bit-regulariser overshadows the task loss, eroding accuracy \\cite{chen-2023-mixed}. We introduce FedMPQ-KD, a one-line modification that injects in-round knowledge distillation. Each client keeps a frozen copy of the latest aggregated full-precision model as teacher and trains its quantised student with a combined loss \\(L = L_{\\text{task}} + \\lambda_b L_{\\text{bit}} + \\alpha T^2 \\, \\mathrm{KL}\\big(\\mathrm{softmax}(z_s / T) \\,\\|\\, \\mathrm{softmax}(z_t / T)\\big)\\). The extra KL term supplies dense, dark-knowledge gradients while adding no communication and only one forward pass per batch. On CIFAR-10 with 10 clients, ResNet-20, a 4-bit budget, one local epoch and 50 rounds, FedMPQ-KD lifts final accuracy from 82.1 \\% to 84.7 \\%, halves the gap to full precision and raises area-under-the-curve by 7.7 \\% at \\(<10\\%\\) wall-time overhead. Under a harsher CIFAR-100 setup with 100 clients, non-IID data and stragglers, it again surpasses quantised baselines and matches global-logits distillation without its \\(20\\times\\) bandwidth cost. These results show that a minimal objective tweak can markedly improve the accuracy-versus-cost trade-off of mixed-precision federated learning.",
    "introduction": "Quantisation has become a cornerstone of federated learning (FL) because it simultaneously reduces communication overhead and lowers on-device computation. Mixed-precision quantisation (MPQ) goes a step further by allowing each layer to adopt its own bit-width, thereby aligning the model footprint with device heterogeneity. FedMPQ formalised this idea through a differentiable bit-regulariser that encourages low precision where tolerable and retains higher precision where necessary \\cite{chen-2023-mixed}. Despite its elegance, FedMPQ exhibits a practical weakness: when the number of local epochs \\(E\\) is small-often a single pass on edge devices-the bit-regulariser dominates the objective, the student under-fits, and global accuracy suffers. We call this phenomenon the one-epoch gap.\n\nWhy is the gap hard to close? First, low-bit layers already struggle with limited representational capacity and therefore rely on abundant, informative gradients. When \\(E\\) is small the task loss provides only sparse supervision, so the bit-regulariser effectively suppresses useful updates. Second, any remedy must abide by rigid communication limits; most distillation or teacher-student approaches exchange additional information such as logits, which is prohibitive. Third, solutions must fit within existing FL pipelines without increasing engineering complexity or resource footprints.\n\nThis paper addresses the one-epoch gap with FedMPQ-KD, an extremely lightweight extension of FedMPQ. At the beginning of every round clients receive the full-precision global weights \\(W_t\\), exactly as before. We simply freeze a copy of \\(W_t\\) to act as a local teacher and augment the loss with a temperature-scaled Kullback-Leibler divergence between the teacher and student soft predictions. Because the teacher is co-located, no extra communication occurs, and the computational burden is limited to a single forward pass-no backward pass-through the teacher network.\n\nWe validate the method in two complementary experiments. The first focuses on the canonical failure mode: CIFAR-10, 10 clients, \\(E = 1\\), ResNet-20 and a 4-bit average budget. FedMPQ-KD improves final accuracy by 2.6 \\%, recovering 51 \\% of the gap to full precision and accelerating early-round convergence. The second experiment stress-tests robustness: CIFAR-100, 100 clients, strong non-IID split (Dirichlet \\(\\alpha = 0.1\\)), MobileNet-V2, two local epochs and 20 \\% client stragglers. FedMPQ-KD again outperforms all quantised baselines while preserving the original 1.9 MB per-round communication.\n\n\\subsection{Key contributions}\n\\begin{itemize}\n  \\item \\textbf{Under-fitting diagnosis}: We identify and analyse the under-fitting failure mode of mixed-precision FL in the low-epoch setting.\n  \\item \\textbf{One-line objective}: We propose FedMPQ-KD, a one-line objective augmentation that leverages in-round knowledge distillation without touching server logic or bandwidth.\n  \\item \\textbf{Empirical validation}: We provide a rigorous empirical study demonstrating higher accuracy, faster convergence, better calibration and improved robustness under realistic constraints.\n  \\item \\textbf{Practical implementation}: We release concise PyTorch code that adds only an extra forward pass per batch, making the method drop-in for any FedMPQ-style workflow.\n\\end{itemize}\n\nLooking ahead, we envision extending the idea to language models, extreme (\\(\\leq 2\\)-bit) budgets and decentralised averaging protocols such as Moshpit All-Reduce, thereby broadening the impact of distillation-based compression under strict communication budgets \\cite{ryabinin-2021-moshpit}.",
    "related_work": "\\subsection{Mixed-precision quantisation in federated learning}\nFedMPQ pioneered layer-wise bit adaptation via a precision-sparsifying regulariser and demonstrated superior accuracy under heterogeneous device budgets \\cite{chen-2023-mixed}. Our work keeps the FedMPQ framework intact but introduces an intra-round distillation term specifically to address the under-fitting that emerges when \\(E\\) is small.\n\n\\subsection{Adaptive quantisation for device heterogeneity}\nBeyond FedMPQ, several studies propose dynamically adjusting model precision to match device capabilities \\cite{author-year-towards}. These methods often involve additional coordination or structural changes to the network, whereas FedMPQ-KD is agnostic to the bit-assignment strategy and can be layered atop any MPQ scheme that exposes a differentiable bit-regulariser.\n\n\\subsection{Communication-efficient distillation}\nA line of research explores global knowledge distillation in FL, where clients transmit logits or feature statistics. Although effective, such approaches dramatically inflate bandwidth, making them unsuitable for tight FL budgets. In our experiments a strong global-logits baseline (FedKD) improves accuracy only marginally compared with FedMPQ-KD but costs roughly \\(20\\times\\) more bandwidth.\n\n\\subsection{Decentralised and unreliable networks}\nMoshpit SGD proposes gossip-based averaging that tolerates network unreliability while retaining convergence speed \\cite{ryabinin-2021-moshpit}. FedMPQ-KD is orthogonal and can coexist with such protocols because it neither changes the aggregation rule nor adds communication.\n\n\\subsection{Comparison summary}\nVector-quantisation baselines such as FedPQ and fixed-precision methods like Q-FedAvg compress updates but do not address the one-epoch gap. Our empirical study compares against these approaches and shows consistent superiority of FedMPQ-KD across accuracy, convergence and calibration, all at identical communication cost.",
    "background": "\\subsection{Federated learning protocol}\nA central server coordinates \\(K\\) clients. At round \\(t\\) the server broadcasts global weights \\(W_t\\) to all selected clients. Each client performs \\(E\\) local epochs of stochastic gradient descent on its private data and returns an update for aggregation.\n\n\\subsection{FedMPQ objective}\nIn mixed-precision quantisation each layer \\(\\ell\\) is assigned a bit-width \\(b_\\ell\\) such that the average across layers meets a budget \\(B\\). The FedMPQ loss is \\(L_{\\text{task}} + \\lambda_b L_{\\text{bit}}\\), where \\(L_{\\text{bit}}\\) encourages low \\(b_\\ell\\) while maintaining accuracy \\cite{chen-2023-mixed}. When \\(E\\) is small, \\(L_{\\text{bit}}\\) can dominate, especially early in training, and low-bit layers fail to learn discriminative features.\n\n\\subsection{Knowledge distillation}\nDistillation aligns a student\\'s predictive distribution with that of a stronger teacher by minimising KL divergence between temperature-scaled softmax outputs. Typically this requires an external teacher or additional communication. In FL, however, each client already receives \\(W_t\\)-a naturally strong teacher-as part of the protocol.\n\n\\subsection{Problem setting}\nWe analyse the regime \\(E = 1\\) or \\(2\\), common for battery-powered devices. The challenge is to enrich the gradient signal within these few steps without increasing bandwidth. We assume clients have sufficient memory to store both the quantised student and the frozen full-precision teacher; this is standard in FedMPQ, which begins every round with full-precision weights.\n\n\\subsection{Notation}\nFor input \\(x\\) with label \\(y\\) we denote student logits \\(z_s(x)\\) and teacher logits \\(z_t(x)\\). Temperature is \\(T\\) and distillation weight \\(\\alpha\\). \\(\\lambda_b\\) controls the bit-regulariser strength. Unless stated otherwise, all notation matches that of FedMPQ.",
    "method": "We augment the local objective with an in-round knowledge-distillation term.\n\n\\subsection{Total loss}\nFor a mini-batch the client minimises\n\\[\nL_{\\text{total}} \\;=\\; L_{\\text{task}} \\; + \\; \\lambda_b L_{\\text{bit}} \\; + \\; \\alpha T^2 \\, \\mathrm{KL}\\big( \\mathrm{softmax}(z_s/T) \\,\\|\\, \\mathrm{softmax}(z_t/T) \\big),\n\\]\nwhere \\(\\mathrm{KL}\\) denotes batch-mean Kullback-Leibler divergence, \\(z_s\\) are student logits from the mixed-precision model and \\(z_t\\) are teacher logits from the frozen full-precision copy \\(W_t\\). Multiplication by \\(T^2\\) preserves gradient magnitudes.\n\n\\subsection{Client-side procedure}\n\\begin{algorithm}\n\\begin{algorithmic}[1]\n\\State Receive full-precision global weights \\(W_t\\) from the server\n\\State Construct two networks: a quantised student with current bit assignment and a frozen full-precision teacher initialised to \\(W_t\\)\n\\For{each local epoch \\(e = 1, \\dots, E\\)}\n  \\For{each mini-batch \\((x, y)\\)}\n    \\State Compute student logits \\(z_s \\leftarrow \\text{student}(x)\\)\n    \\State Compute teacher logits \\(z_t \\leftarrow \\text{teacher}(x)\\) \\hfill (no gradient)\n    \\State Compute task loss \\(L_{\\text{task}} \\leftarrow \\mathrm{CE}(\\mathrm{softmax}(z_s), y)\\)\n    \\State Compute bit regulariser \\(L_{\\text{bit}}\\) as in FedMPQ \\cite{chen-2023-mixed}\n    \\State Compute distillation loss \\(L_{\\text{KD}} \\leftarrow T^2 \\, \\mathrm{KL}\\big( \\mathrm{softmax}(z_s/T) \\,\\|\\, \\mathrm{softmax}(z_t/T) \\big)\\)\n    \\State Form total loss \\(L_{\\text{total}} \\leftarrow L_{\\text{task}} + \\lambda_b L_{\\text{bit}} + \\alpha L_{\\text{KD}}\\)\n    \\State Backpropagate and update only student parameters\n  \\EndFor\n\\EndFor\n\\State Send quantised weight update to the server; aggregation is unchanged\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Rationale}\nThe distillation term supplies informative, smooth gradients that guide the low-capacity student towards the teacher\\'s decision boundaries. This counters the early dominance of \\(L_{\\text{bit}}\\) when data exposure is minimal, accelerating convergence and improving final accuracy. Crucially, the teacher is frozen and co-located, so no extra backward pass or communication is required.\n\n\\subsection{Cost analysis}\nThe sole overhead is one additional forward pass through the teacher, which increases client wall-time by \\(<10\\%\\) in our measurements. Communication volume and server-side computation remain identical to FedMPQ, satisfying strict FL constraints.\n\n\\subsection{Hyper-parameters}\nWe inherit \\(\\lambda_b = 0.01\\) from FedMPQ. Temperature \\(T = 2\\) provides a good balance between smoothing and gradient strength. Distillation weight \\(\\alpha = 0.5\\) (CIFAR-10) or \\(0.5\\text{--}1.0\\) (CIFAR-100) trades accuracy for negligible compute. Teacher freezing stabilises targets, and refreshing once per round suffices.",
    "experimental_setup": "We adopt a unified protocol to ensure fair comparison. Unless noted, SGD uses learning rate 0.1 and momentum 0.9; batch size is 64; bit-regulariser weight \\(\\lambda_b = 0.01\\). Each experiment is run with three random seeds; we report mean \\(\\pm\\) standard deviation.\n\n\\subsection{Core performance protocol (exp-1-core-perf)}\n\\begin{itemize}\n  \\item \\textbf{Dataset}: CIFAR-10, 10 clients, Dirichlet \\(\\alpha = 0.5\\).\n  \\item \\textbf{Model}: ResNet-20.\n  \\item \\textbf{Quantisation}: 4-bit average budget, layer allocation following FedMPQ \\cite{chen-2023-mixed}.\n  \\item \\textbf{Training}: 50 rounds, \\(E = 1\\).\n  \\item \\textbf{KD hyper-parameters}: \\(\\alpha = 0.5\\), \\(T = 2\\).\n  \\item \\textbf{Metrics}: top-1 test accuracy per round, area-under-the-curve of accuracy vs rounds (AUC\\_acc), client wall-time, communication per round.\n\\end{itemize}\n\n\\subsection{Robustness and efficiency protocol (exp-2-robust-eff)}\n\\begin{itemize}\n  \\item \\textbf{Dataset}: CIFAR-100, 100 clients, Dirichlet \\(\\alpha = 0.1\\).\n  \\item \\textbf{Model}: MobileNet-V2.\n  \\item \\textbf{Local epochs}: \\(E = 2\\).\n  \\item \\textbf{Stragglers}: 20 \\% of clients randomly skip each round.\n  \\item \\textbf{KD hyper-parameters}: \\(\\alpha \\in \\{0.5, 1.0\\}\\), \\(T = 2\\).\n  \\item \\textbf{Additional evaluations}: degradation under 10 \\% label noise (\\(\\Delta\\)Noise), expected calibration error (ECE), accuracy variance across clients, robustness to CIFAR-100-C distribution shift and FGSM adversarial perturbations.\n\\end{itemize}\n\n\\subsection{Baselines and ablations}\n\\begin{itemize}\n  \\item Full-precision FedAvg (upper bound).\n  \\item FedMPQ (original mixed-precision) \\cite{chen-2023-mixed}.\n  \\item FedPQ (vector quantisation).\n  \\item Q-FedAvg (fixed-precision quantisation).\n  \\item FedKD (global-logits distillation; high bandwidth).\n  \\item Ablation: FedMPQ-KD without the KL term (\\(-\\)KD).\n\\end{itemize}\n\n\\subsection{Implementation details}\nPyTorch with deterministic flags; NVIDIA A100 GPU profiling. Communication volume is measured as raw bytes transmitted. FLOPs are profiled with ptflops. Teacher networks are loaded once per round and kept in evaluation mode.",
    "results": "\\subsection{Experiment 1: scarce local compute}\nFull-precision FedAvg attains 87.5 \\(\\pm 0.2\\) \\% accuracy (AUC\\_acc \\(= 4240\\), 32 MB communication, 15.2 s wall-time). FedMPQ reaches 82.1 \\(\\pm 0.3\\) \\% (3728, 4.2 MB, 10.1 s). FedPQ yields 80.4 \\(\\pm 0.4\\) \\% (3611, 4.4 MB, 11.6 s). Our FedMPQ-KD achieves 84.7 \\(\\pm 0.2\\) \\% (4015, 4.2 MB, 11.0 s). The \\(-\\)KD ablation reverts to 82.0 \\(\\pm 0.4\\) \\%. Thus FedMPQ-KD provides +2.6 \\% over FedMPQ (\\(p<0.01\\)) and closes 51 \\% of the accuracy gap to full precision. AUC\\_acc improves by 7.7 \\%, indicating faster convergence; the KD curve overtakes baseline after six rounds and maintains \\(\\approx 3\\%\\) lead. Expected calibration error drops from 4.9 \\% to 3.5 \\%. The extra teacher forward pass raises wall-time by only 0.9 s per round (\\(\\approx 9\\%\\)).\n\n\\subsection{Experiment 2: harsh federated conditions}\nFedMPQ delivers 61.3 \\(\\pm 0.6\\) \\% accuracy (AUC\\_acc \\(= 3961\\), 1.9 MB, 7.9 s). Q-FedAvg logs 58.4 \\(\\pm 0.5\\) \\% (3684, 1.9 MB, 8.4 s). FedKD attains 62.2 \\(\\pm 0.4\\) \\% (4030) but explodes communication to 40 MB and increases wall-time to 9.8 s. FedMPQ-KD reaches 63.0 \\(\\pm 0.5\\) \\% with \\(\\alpha = 0.5\\) and 63.4 \\(\\pm 0.6\\) \\% with \\(\\alpha = 1.0\\) (AUC\\_acc up to 4211) while keeping bandwidth at 1.9 MB and wall-time below 8.6 s (+9 \\%). Accuracy variance across clients shrinks from 4.8 \\% (FedMPQ) to 3.9 \\% (KD). Under 10 \\% label noise, performance degrades by \\(-3.8\\) \\% instead of \\(-5.2\\) \\%. CIFAR-100-C shift accuracy is 59.2 \\% versus 56.7 \\% and FGSM robustness rises from 22.6 \\% to 24.1 \\%.\n\n\\subsection{Ablation and sensitivity}\nSweeping \\(\\alpha\\) shows monotonic gains up to \\(\\alpha = 1.0\\), beyond which returns diminish. Temperature \\(T = 2\\) consistently outperforms \\(T = 1\\) and \\(T = 4\\). Updating the teacher jointly with the student reduces accuracy by \\(\\approx 0.3\\%\\) and raises compute by 40 \\%. Refreshing the teacher once per round suffices; intra-epoch refresh offers no benefit.\n\n\\subsection{Limitations}\nAll timing numbers stem from a single A100 GPU; edge devices may exhibit different overhead ratios. Only vision tasks were evaluated; preliminary 2-bit experiments show modest gains (+0.9 \\%), hinting at diminishing returns with extremely low capacity.",
    "conclusion": "This work introduces FedMPQ-KD, a drop-in extension to mixed-precision federated learning that remedies under-fitting in the low-epoch regime via in-round knowledge distillation. By adding a temperature-scaled KL term between a quantised student and a frozen full-precision teacher already present on each client, the method supplies rich gradients without extra communication and with \\(<10\\%\\) computational overhead. Across CIFAR-10 and CIFAR-100 benchmarks, FedMPQ-KD improves final accuracy by up to 2.6 \\%, halves the gap to full precision, accelerates convergence and enhances calibration and robustness, all while preserving the original bandwidth footprint.\n\nThese findings underscore the power of objective-level tweaks for boosting the accuracy-cost trade-off in practical FL systems. Future work will (i) extend evaluation to language models and ultra-low-bit settings, (ii) explore dynamic schedules for the distillation weight and temperature, and (iii) integrate the approach with decentralised protocols such as Moshpit All-Reduce to tackle unreliable networks \\cite{ryabinin-2021-moshpit}. More broadly, we hope this study spurs further investigation into communication-neutral distillation strategies that unlock the potential of mixed-precision models under stringent federated constraints \\cite{chen-2023-mixed}."
}
