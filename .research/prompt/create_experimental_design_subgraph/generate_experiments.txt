
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 2 distinct Experiment objects.

# Instructions
- Generate 2 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 2 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "FedMPQ suffers a noticeable accuracy drop when the number of local epochs per communication round is small (e.g., 1).  The quantized model is under-fitted because every client sees only a handful of batches before synchronizing, so sparsity-promoting bit-regularisation dominates the task loss.\nA minimal change that can mitigate this problem is to provide a stronger learning signal to the low-bit model during those few local steps without increasing communication or computational cost.",
    "Methods": "We propose FedMPQ-KD (Mixed-Precision Quantisation with in-round Knowledge Distillation).\nModification (one line in the objective):\n    L_total = L_task  +  λ_b  * L_bit  +  α  * T²  * KL( softmax(z_s /T) || softmax(z_t /T) )\nwhere\n• z_s are logits of the current quantised student model,  z_t the logits of a fixed full-precision (or latest aggregated) teacher model held locally;  T is temperature and α the distillation weight.\n\nProcedure per client (changes in bold):\n1. Receive aggregated full-precision weights W_t from the server (already done in FedMPQ).\n2. Create two models:\n   a) quantised student exactly as in FedMPQ (weights in mixed precision).\n   b) ****freeze a copy of W_t in full precision as teacher****.\n3. For E local epochs, optimise the student with the extended loss above.  The teacher only produces logits; no back-prop.\n4. Send student weight updates as usual.\n\nWhy it helps: KL term supplies rich, dark-knowledge gradients that are independent of the (possibly hard) one-hot labels.  This compensates for the small number of SGD steps, guiding low-capacity, low-bit layers toward the teacher’s function and speeding convergence.  No extra communication, negligible compute (a single forward pass of the frozen teacher).",
    "Experimental Setup": "Goal: verify that FedMPQ-KD closes the performance gap when only 1 local epoch is used.\n• Dataset: CIFAR-10 (10 clients, α=0.5 Dirichlet split).\n• Network: ResNet-20.\n• Budgets: average 4-bit, mixed across layers as in the original paper.\n• Baselines:  (1) FedMPQ (original)  (2) FedMPQ-KD (ours).\n• Hyper-parameters:  λ_b =0.01 (unchanged),  α=0.5,  T=2.\n• Training: 50 communication rounds, 1 local epoch, batch-size 64, SGD lr 0.1.\n• Metric: global test accuracy after every round.\nExpected observation window: accuracy vs. rounds and final accuracy.",
    "Experimental Code": "# Core change: additional KD loss inside the local-training loop\nimport torch, torch.nn as nn, torch.nn.functional as F\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    \"\"\"KL divergence with temperature.\"\"\"\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\n\nclass LocalTrainer:\n    def __init__(self, student, teacher, dataloader, lr=0.1, lambda_b=0.01, alpha=0.5):\n        self.student = student  # quantised weights already applied\n        self.teacher = teacher.eval()  # full-precision copy, frozen\n        self.opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n        self.dl = dataloader\n        self.lambda_b = lambda_b\n        self.alpha = alpha\n\n    def train_one_epoch(self):\n        self.student.train()\n        for x, y in self.dl:\n            logits_s = self.student(x)\n            logits_t = self.teacher(x)\n            loss_task = F.cross_entropy(logits_s, y)\n            loss_bit  = self.student.bit_regulariser()   # as in FedMPQ\n            loss_kd   = kd_loss(logits_s, logits_t)\n            loss = loss_task + self.lambda_b*loss_bit + self.alpha*loss_kd\n            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "Expected Result": "With only 1 local epoch, FedMPQ reaches ≈82% CIFAR-10 accuracy after 50 rounds (reported drop of ~3-4% from full baseline).  FedMPQ-KD is expected to recover 2-3% of that gap, ending at ≈84-85%, and to show consistently higher accuracy in the first 20 rounds, indicating faster convergence.",
    "Expected Conclusion": "A single extra knowledge-distillation term supplies richer gradients to the quantised student during scarce local training, reducing under-fitting without additional communication or heavy computation.  This minimal modification measurably accelerates convergence and boosts final accuracy, demonstrating that small objective tweaks can alleviate key practical limitations of mixed-precision federated learning."
}

# Experiment Strategy
Objective: Build a single, coherent framework that will let every subsequent experiment answer one overarching question – does FedMPQ-KD provide a strictly better accuracy-vs-cost trade-off than competing quantised FL schemes across realistic operating conditions?

1. Core Hypotheses to Validate
   a. Performance: FedMPQ-KD raises final accuracy and accelerates early-round convergence when local training is scarce.
   b. Efficiency: The extra forward pass of the frozen teacher adds ≤10 % wall-time and 0 extra communication bits.
   c. Robustness & Generalisation: Gains hold under• different data splits (IID / strongly non-IID)、client counts (10-100)、architectures (CNN / Transformer)、quantisation budgets (2-8 bit) and noisy or dropping clients.
   d. Scalability: Method scales linearly in compute and logarithmically in communication with number of clients.

2. Comparison Palette (identical across all experiments)
   • Baselines: (1) FedMPQ (original) (2) Full-precision FedAvg (upper bound) (3) SOTA low-bit FL alternatives (FedPQ, FedKD, Q-FedAvg).
   • Ablations: i) –KD (λ_b+L_task only) ii) varying α, T iii) updating vs freezing the teacher iv) local vs global teacher refresh frequency.

3. Evaluation Angles
   3.1 Quantitative Performance
       – Top-1 accuracy vs communication rounds (primary)
       – Area-under-curve (AUC_acc) to capture convergence speed
       – Final accuracy gap to full-precision
   3.2 Cost Metrics
       – Client-side FLOPs & wall-clock per round (measured on A100)
       – Communication bits / round (should be unchanged)
       – Peak GPU VRAM & host RAM
   3.3 Robustness Metrics
       – Accuracy variance across 3 random seeds
       – Degradation (% drop) under extreme non-IID, stragglers, label noise
   3.4 Qualitative / Diagnostic
       – t-SNE of penultimate-layer features (student vs teacher)
       – Bit-width utilisation histograms
       – Gradient-norm and loss-landscape snapshots to illustrate stronger signals.

4. Experimental Axes (each future experiment picks one axis while keeping the rest fixed)
   A. Local compute budget: E∈{1,2,5}
   B. Quantisation budget: avg bits ∈{2,4,8}
   C. Data heterogeneity: Dirichlet α∈{0.1,0.5,∞}
   D. Model family: ResNet-20, MobileNet-V2, ViT-Small
   E. Scale: clients ∈{10,50,100}; participation rate 10-100 %
   F. Failure modes: 20 % clients randomly drop per round; 10 % label noise.

5. Success Criteria (must hold in ≥80 % of experimental settings)
   • +≥2 % absolute test accuracy OR ≥25 % smaller gap to full-precision compared with best quantised baseline, p<0.05 (t-test over seeds).
   • No statistically significant increase in communication volume.
   • Added wall-time ≤10 % per round on A100.
   • Under worst-case non-IID, KD variant still outperforms FedMPQ by ≥1 %.

6. Protocol & Reproducibility
   • Same optimiser, LR schedule, batch size across methods; only α and T tuned once on a small validation pool and kept fixed.
   • Three independent seeds; report mean±std.
   • Use PyTorch w/ deterministic flags; log hardware utilisation via NVIDIA-smi and PyTorch profiler.
   • Release code & logs; each run stores JSON metadata capturing hyper-params, seed, commit hash.

7. Resource Awareness
   • Single NVIDIA A100 (80 GB) easily fits teacher + student (≤4 GB each). Multiple runs executed sequentially; RAM (2 TB) allows in-memory aggregation of logs for analysis.

By following this common strategy—fixed baselines, uniform metrics, multifaceted validation axes, and rigid success criteria—each forthcoming experiment will contribute a comparable slice of evidence, together establishing the robustness, efficiency and overall superiority of FedMPQ-KD.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-1-core-perf",
            "run_variations": [
                "full-precision-FedAvg",
                "FedMPQ-baseline",
                "FedPQ-SOTA",
                "FedMPQ-KD (ours)",
                "FedMPQ-noKD-ablation"
            ],
            "description": "Objective / Hypothesis: Quantitatively verify that FedMPQ-KD closes ≥25 % of the accuracy gap to full-precision while adding ≤10 % wall-time when local compute per round is scarce (E=1). Ablations (-KD) will show the necessity of the new distillation term.\n\nModels:\n • Student: ResNet-20 (mixed-precision: layer-wise 2–8 bit, avg 4 bit).\n • Teacher (for KD): full-precision ResNet-20 snapshot received from server at the start of the round. Frozen during local training.\n • Baselines/SOTA: original FedMPQ implementation, FedPQ (vector quantisation), full-precision FedAvg upper bound.\n\nDataset:\n • CIFAR-10, 32×32 RGB.\nPre-processing: per-channel mean/std normalisation, random 32→28 crop & horizontal flip on train only.\nSplit: 10 clients, Dirichlet α = 0.5 non-IID. Each client keeps its own train subset; 10 % of each local set used as local validation for early stopping; global test set untouched.\n\nTraining protocol:\n • Local epochs E = 1, batch 64, SGD lr 0.1, momentum 0.9, cosine decay.\n • 50 communication rounds, 3 independent seeds.\n • Early stopping not allowed; evaluate after each round.\nAveraging: report mean±std across seeds; primary score is final top-1 test accuracy, secondary AUC_acc (trapezoidal area under accuracy-vs-round curve).\nSelection criterion in plots: last checkpoint.\n\nCost measurement: client-side wall-time and FLOPs captured with PyTorch profiler, VRAM via nvprof; communication bits measured from update tensor sizes.\n\nHyper-param analysis: run FedMPQ-KD with α∈{0.25,0.5,1.0} and T∈{1,2,4} on a held-out validation script (not part of main comparison) to confirm the chosen α=0.5, T=2 is near-optimal.\n\nMetrics: Primary – top-1 accuracy; Secondary – AUC_acc, gap_to_full-precision, FLOPs/round, wall-time/round, comm_bits/round, ECE calibration error.\n\nRobustness hooks (recorded but not baseline-plotted): repeat whole experiment once with label-noise 10 % to verify ranking stability.\n\nExample code snippet (local training loop excerpt):\n```\nfor x,y in dl:\n    logits_s = student(x)\n    logits_t = teacher(x)          # frozen fp32\n    loss = ce(logits_s,y) + 0.01*student.bit_reg() + 0.5*kd_loss(logits_s, logits_t, T=2)\n    loss.backward(); opt.step()\n```\nResource estimate on A100:   GPU RAM 4 GB per process; wall-time per round 15 s full-precision, 10 s compressed, 11 s w/ KD (≈+9 %).\nSuccess criterion: FedMPQ-KD ≥84 % accuracy (≥+2 % over FedMPQ) with ≤11 s/round."
        },
        {
            "experiment_id": "exp-2-robust-eff",
            "run_variations": [
                "FedMPQ-baseline",
                "FedMPQ-KD α=0.5,T=2 (default)",
                "FedMPQ-KD α=1.0,T=2 (high-KD)",
                "FedKD-global-teacher",
                "Q-FedAvg"
            ],
            "description": "Objective / Hypothesis: Stress-test FedMPQ-KD under harsh FL conditions (100 clients, extreme non-IID, client dropouts) and quantify efficiency gains on lightweight models. We expect KD variants to maintain ≥1 % accuracy lead and stable convergence with negligible cost overhead.\n\nModels:\n • Student: MobileNet-V2 (avg 4-bit mixed precision).\n • Teacher: frozen copy of latest aggregated full-precision MobileNet-V2 (updated every round).\n • Alternative baseline: FedKD – classical KD that exchanges softened logits globally (communication overhead) – to highlight zero-cost advantage of in-round KD.\n\nDatasets:\n • CIFAR-100 (100 classes) to test generalisation difficulty.\nPre-processing: standard CIFAR-100 normalisation, random crop 32→28, horizontal flip.\nData heterogeneity: Dirichlet α = 0.1 (strongly non-IID); plus 20 % of clients randomly drop out each round (stragglers).\n\nData split & sampling:\n • Each client keeps its shard; 5 % of local data for val, rest train.\n • Global test set untouched.\n\nTraining schedule:\n • Local epochs E = 2, batch 64, AdamW lr 3e-4.\n • Total 100 communication rounds, participation rate 20 % (random).\nSeeds: 3; metrics averaged.\nCheckpoint selection: best-val per seed (val accuracy on each client averaged) then test.\n\nEvaluation metrics:\n • Primary – global top-1 test accuracy.\n • Secondary – convergence AUC_acc, robustness drop (%) when 20 % label noise injected at rnd 60, client-level accuracy variance (std across clients), FLOPs/round, GPU memory, wall-time, communication bits.\n • Calibration – Expected Calibration Error (ECE).\n\nRobustness procedures:\n 1. Noise injection: at round 60 flip 20 % of labels on 10 % random active clients; measure Δaccuracy.\n 2. Distribution shift: after training, evaluate on CIFAR-100-C (corrupted) severity 3.\n 3. Adversarial: FGSM ε=2/255 on test set to compare robustness scores.\n\nHyperparameter sensitivity: KD variants are re-run with α∈{0.25,0.5,1.0} on 1 seed; learning-rate sweep 1e-4–1e-3 to ensure fairness; results in appendix.\n\nEfficiency analysis: record\n • Training wall-clock per round with PyTorch profiler\n • Inference latency (teacher + student) on 1 k images\n • Peak VRAM\n • FLOPs via fvcore\n • Communication volume – model update size; highlight that FedKD baseline sends extra logits (≈38 MB/client/round).\n\nExample code excerpt for dropout handling:\n```\nactive = random.sample(clients, int(len(clients)*0.2))\nfor c in active:\n    if random.random()<0.2:  # straggler\n        continue\n    c.local_train()\n```\nResource projection: 100 MobileNet-V2 students (quantised) sequentially processed -> 8 h total on single A100; fits 80 GB VRAM if 2 processes run concurrently.\nSuccess criteria: Under all stressors FedMPQ-KD default ≥1 % absolute accuracy over FedMPQ and ≥25 % lower comm cost than FedKD; added wall-time ≤8 %."
        }
    ],
    "expected_models": [
        "ResNet-20",
        "MobileNet-V2",
        "ViT-Small"
    ],
    "expected_datasets": [
        "CIFAR-10",
        "CIFAR-100"
    ]
}
