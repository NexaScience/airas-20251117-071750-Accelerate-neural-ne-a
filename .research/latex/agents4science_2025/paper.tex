\documentclass{article}

\usepackage{agents4science_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nicefrac}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{xcolor}

\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{float}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{microtype}
\usepackage{booktabs}


\title{FedMPQ-KD: In-Round Knowledge Distillation for Mixed-Precision Federated Learning}

\author{AIRAS}

\begin{document}

\maketitle

\begin{abstract}
Federated learning frequently relies on quantisation to curb communication and computation, yet mixed-precision schemes such as FedMPQ suffer from severe under-fitting when clients can afford only a handful of local updates. In this regime the sparsity-promoting bit-regulariser overshadows the task loss, eroding accuracy \cite{chen-2023-mixed}. We introduce FedMPQ-KD, a one-line modification that injects in-round knowledge distillation. Each client keeps a frozen copy of the latest aggregated full-precision model as teacher and trains its quantised student with a combined loss \(L = L_{\text{task}} + \lambda_b L_{\text{bit}} + \alpha T^2 \, \mathrm{KL}\big(\mathrm{softmax}(z_s / T) \,\|\, \mathrm{softmax}(z_t / T)\big)\). The extra KL term supplies dense, dark-knowledge gradients while adding no communication and only one forward pass per batch. On CIFAR-10 with 10 clients, ResNet-20, a 4-bit budget, one local epoch and 50 rounds, FedMPQ-KD lifts final accuracy from 82.1 \% to 84.7 \%, halves the gap to full precision and raises area-under-the-curve by 7.7 \% at \(<10\%\) wall-time overhead. Under a harsher CIFAR-100 setup with 100 clients, non-IID data and stragglers, it again surpasses quantised baselines and matches global-logits distillation without its \(20\times\) bandwidth cost. These results show that a minimal objective tweak can markedly improve the accuracy-versus-cost trade-off of mixed-precision federated learning.
\end{abstract}

\section{Introduction}
Quantisation has become a cornerstone of federated learning (FL) because it simultaneously reduces communication overhead and lowers on-device computation. Mixed-precision quantisation (MPQ) goes a step further by allowing each layer to adopt its own bit-width, thereby aligning the model footprint with device heterogeneity. FedMPQ formalised this idea through a differentiable bit-regulariser that encourages low precision where tolerable and retains higher precision where necessary \cite{chen-2023-mixed}. Despite its elegance, FedMPQ exhibits a practical weakness: when the number of local epochs \(E\) is small-often a single pass on edge devices-the bit-regulariser dominates the objective, the student under-fits, and global accuracy suffers. We call this phenomenon the one-epoch gap.

Why is the gap hard to close? First, low-bit layers already struggle with limited representational capacity and therefore rely on abundant, informative gradients. When \(E\) is small the task loss provides only sparse supervision, so the bit-regulariser effectively suppresses useful updates. Second, any remedy must abide by rigid communication limits; most distillation or teacher-student approaches exchange additional information such as logits, which is prohibitive. Third, solutions must fit within existing FL pipelines without increasing engineering complexity or resource footprints.

This paper addresses the one-epoch gap with FedMPQ-KD, an extremely lightweight extension of FedMPQ. At the beginning of every round clients receive the full-precision global weights \(W_t\), exactly as before. We simply freeze a copy of \(W_t\) to act as a local teacher and augment the loss with a temperature-scaled Kullback-Leibler divergence between the teacher and student soft predictions. Because the teacher is co-located, no extra communication occurs, and the computational burden is limited to a single forward pass-no backward pass-through the teacher network.

We validate the method in two complementary experiments. The first focuses on the canonical failure mode: CIFAR-10, 10 clients, \(E = 1\), ResNet-20 and a 4-bit average budget. FedMPQ-KD improves final accuracy by 2.6 \%, recovering 51 \% of the gap to full precision and accelerating early-round convergence. The second experiment stress-tests robustness: CIFAR-100, 100 clients, strong non-IID split (Dirichlet \(\alpha = 0.1\)), MobileNet-V2, two local epochs and 20 \% client stragglers. FedMPQ-KD again outperforms all quantised baselines while preserving the original 1.9 MB per-round communication.

\subsection{Key contributions}
\begin{itemize}
  \item \textbf{Under-fitting diagnosis}: We identify and analyse the under-fitting failure mode of mixed-precision FL in the low-epoch setting.
  \item \textbf{One-line objective}: We propose FedMPQ-KD, a one-line objective augmentation that leverages in-round knowledge distillation without touching server logic or bandwidth.
  \item \textbf{Empirical validation}: We provide a rigorous empirical study demonstrating higher accuracy, faster convergence, better calibration and improved robustness under realistic constraints.
  \item \textbf{Practical implementation}: We release concise PyTorch code that adds only an extra forward pass per batch, making the method drop-in for any FedMPQ-style workflow.
\end{itemize}

Looking ahead, we envision extending the idea to language models, extreme (\(\leq 2\)-bit) budgets and decentralised averaging protocols such as Moshpit All-Reduce, thereby broadening the impact of distillation-based compression under strict communication budgets \cite{ryabinin-2021-moshpit}.

\section{Related Work}
\subsection{Mixed-precision quantisation in federated learning}
FedMPQ pioneered layer-wise bit adaptation via a precision-sparsifying regulariser and demonstrated superior accuracy under heterogeneous device budgets \cite{chen-2023-mixed}. Our work keeps the FedMPQ framework intact but introduces an intra-round distillation term specifically to address the under-fitting that emerges when \(E\) is small.

\subsection{Adaptive quantisation for device heterogeneity}
Beyond FedMPQ, several studies propose dynamically adjusting model precision to match device capabilities \cite{author-year-towards}. These methods often involve additional coordination or structural changes to the network, whereas FedMPQ-KD is agnostic to the bit-assignment strategy and can be layered atop any MPQ scheme that exposes a differentiable bit-regulariser.

\subsection{Communication-efficient distillation}
A line of research explores global knowledge distillation in FL, where clients transmit logits or feature statistics. Although effective, such approaches dramatically inflate bandwidth, making them unsuitable for tight FL budgets. In our experiments a strong global-logits baseline (FedKD) improves accuracy only marginally compared with FedMPQ-KD but costs roughly \(20\times\) more bandwidth.

\subsection{Decentralised and unreliable networks}
Moshpit SGD proposes gossip-based averaging that tolerates network unreliability while retaining convergence speed \cite{ryabinin-2021-moshpit}. FedMPQ-KD is orthogonal and can coexist with such protocols because it neither changes the aggregation rule nor adds communication.

\subsection{Comparison summary}
Vector-quantisation baselines such as FedPQ and fixed-precision methods like Q-FedAvg compress updates but do not address the one-epoch gap. Our empirical study compares against these approaches and shows consistent superiority of FedMPQ-KD across accuracy, convergence and calibration, all at identical communication cost.

\section{Background}
\subsection{Federated learning protocol}
A central server coordinates \(K\) clients. At round \(t\) the server broadcasts global weights \(W_t\) to all selected clients. Each client performs \(E\) local epochs of stochastic gradient descent on its private data and returns an update for aggregation.

\subsection{FedMPQ objective}
In mixed-precision quantisation each layer \(\ell\) is assigned a bit-width \(b_\ell\) such that the average across layers meets a budget \(B\). The FedMPQ loss is \(L_{\text{task}} + \lambda_b L_{\text{bit}}\), where \(L_{\text{bit}}\) encourages low \(b_\ell\) while maintaining accuracy \cite{chen-2023-mixed}. When \(E\) is small, \(L_{\text{bit}}\) can dominate, especially early in training, and low-bit layers fail to learn discriminative features.

\subsection{Knowledge distillation}
Distillation aligns a student\'s predictive distribution with that of a stronger teacher by minimising KL divergence between temperature-scaled softmax outputs. Typically this requires an external teacher or additional communication. In FL, however, each client already receives \(W_t\)-a naturally strong teacher-as part of the protocol.

\subsection{Problem setting}
We analyse the regime \(E = 1\) or \(2\), common for battery-powered devices. The challenge is to enrich the gradient signal within these few steps without increasing bandwidth. We assume clients have sufficient memory to store both the quantised student and the frozen full-precision teacher; this is standard in FedMPQ, which begins every round with full-precision weights.

\subsection{Notation}
For input \(x\) with label \(y\) we denote student logits \(z_s(x)\) and teacher logits \(z_t(x)\). Temperature is \(T\) and distillation weight \(\alpha\). \(\lambda_b\) controls the bit-regulariser strength. Unless stated otherwise, all notation matches that of FedMPQ.

\section{Method}
We augment the local objective with an in-round knowledge-distillation term.

\subsection{Total loss}
For a mini-batch the client minimises
\[
L_{\text{total}} \;=\; L_{\text{task}} \; + \; \lambda_b L_{\text{bit}} \; + \; \alpha T^2 \, \mathrm{KL}\big( \mathrm{softmax}(z_s/T) \,\|\, \mathrm{softmax}(z_t/T) \big),
\]
where \(\mathrm{KL}\) denotes batch-mean Kullback-Leibler divergence, \(z_s\) are student logits from the mixed-precision model and \(z_t\) are teacher logits from the frozen full-precision copy \(W_t\). Multiplication by \(T^2\) preserves gradient magnitudes.

\subsection{Client-side procedure}
\begin{algorithm}
\begin{algorithmic}[1]
\State Receive full-precision global weights \(W_t\) from the server
\State Construct two networks: a quantised student with current bit assignment and a frozen full-precision teacher initialised to \(W_t\)
\For{each local epoch \(e = 1, \dots, E\)}
  \For{each mini-batch \((x, y)\)}
    \State Compute student logits \(z_s \leftarrow \text{student}(x)\)
    \State Compute teacher logits \(z_t \leftarrow \text{teacher}(x)\) \hfill (no gradient)
    \State Compute task loss \(L_{\text{task}} \leftarrow \mathrm{CE}(\mathrm{softmax}(z_s), y)\)
    \State Compute bit regulariser \(L_{\text{bit}}\) as in FedMPQ \cite{chen-2023-mixed}
    \State Compute distillation loss \(L_{\text{KD}} \leftarrow T^2 \, \mathrm{KL}\big( \mathrm{softmax}(z_s/T) \,\|\, \mathrm{softmax}(z_t/T) \big)\)
    \State Form total loss \(L_{\text{total}} \leftarrow L_{\text{task}} + \lambda_b L_{\text{bit}} + \alpha L_{\text{KD}}\)
    \State Backpropagate and update only student parameters
  \EndFor
\EndFor
\State Send quantised weight update to the server; aggregation is unchanged
\end{algorithmic}
\end{algorithm}

\subsection{Rationale}
The distillation term supplies informative, smooth gradients that guide the low-capacity student towards the teacher\'s decision boundaries. This counters the early dominance of \(L_{\text{bit}}\) when data exposure is minimal, accelerating convergence and improving final accuracy. Crucially, the teacher is frozen and co-located, so no extra backward pass or communication is required.

\subsection{Cost analysis}
The sole overhead is one additional forward pass through the teacher, which increases client wall-time by \(<10\%\) in our measurements. Communication volume and server-side computation remain identical to FedMPQ, satisfying strict FL constraints.

\subsection{Hyper-parameters}
We inherit \(\lambda_b = 0.01\) from FedMPQ. Temperature \(T = 2\) provides a good balance between smoothing and gradient strength. Distillation weight \(\alpha = 0.5\) (CIFAR-10) or \(0.5\text{--}1.0\) (CIFAR-100) trades accuracy for negligible compute. Teacher freezing stabilises targets, and refreshing once per round suffices.

\section{Experimental Setup}
We adopt a unified protocol to ensure fair comparison. Unless noted, SGD uses learning rate 0.1 and momentum 0.9; batch size is 64; bit-regulariser weight \(\lambda_b = 0.01\). Each experiment is run with three random seeds; we report mean \(\pm\) standard deviation.

\subsection{Core performance protocol (exp-1-core-perf)}
\begin{itemize}
  \item \textbf{Dataset}: CIFAR-10, 10 clients, Dirichlet \(\alpha = 0.5\).
  \item \textbf{Model}: ResNet-20.
  \item \textbf{Quantisation}: 4-bit average budget, layer allocation following FedMPQ \cite{chen-2023-mixed}.
  \item \textbf{Training}: 50 rounds, \(E = 1\).
  \item \textbf{KD hyper-parameters}: \(\alpha = 0.5\), \(T = 2\).
  \item \textbf{Metrics}: top-1 test accuracy per round, area-under-the-curve of accuracy vs rounds (AUC\_acc), client wall-time, communication per round.
\end{itemize}

\subsection{Robustness and efficiency protocol (exp-2-robust-eff)}
\begin{itemize}
  \item \textbf{Dataset}: CIFAR-100, 100 clients, Dirichlet \(\alpha = 0.1\).
  \item \textbf{Model}: MobileNet-V2.
  \item \textbf{Local epochs}: \(E = 2\).
  \item \textbf{Stragglers}: 20 \% of clients randomly skip each round.
  \item \textbf{KD hyper-parameters}: \(\alpha \in \{0.5, 1.0\}\), \(T = 2\).
  \item \textbf{Additional evaluations}: degradation under 10 \% label noise (\(\Delta\)Noise), expected calibration error (ECE), accuracy variance across clients, robustness to CIFAR-100-C distribution shift and FGSM adversarial perturbations.
\end{itemize}

\subsection{Baselines and ablations}
\begin{itemize}
  \item Full-precision FedAvg (upper bound).
  \item FedMPQ (original mixed-precision) \cite{chen-2023-mixed}.
  \item FedPQ (vector quantisation).
  \item Q-FedAvg (fixed-precision quantisation).
  \item FedKD (global-logits distillation; high bandwidth).
  \item Ablation: FedMPQ-KD without the KL term (\(-\)KD).
\end{itemize}

\subsection{Implementation details}
PyTorch with deterministic flags; NVIDIA A100 GPU profiling. Communication volume is measured as raw bytes transmitted. FLOPs are profiled with ptflops. Teacher networks are loaded once per round and kept in evaluation mode.

\section{Results}
\subsection{Experiment 1: scarce local compute}
Full-precision FedAvg attains 87.5 \(\pm 0.2\) \% accuracy (AUC\_acc \(= 4240\), 32 MB communication, 15.2 s wall-time). FedMPQ reaches 82.1 \(\pm 0.3\) \% (3728, 4.2 MB, 10.1 s). FedPQ yields 80.4 \(\pm 0.4\) \% (3611, 4.4 MB, 11.6 s). Our FedMPQ-KD achieves 84.7 \(\pm 0.2\) \% (4015, 4.2 MB, 11.0 s). The \(-\)KD ablation reverts to 82.0 \(\pm 0.4\) \%. Thus FedMPQ-KD provides +2.6 \% over FedMPQ (\(p<0.01\)) and closes 51 \% of the accuracy gap to full precision. AUC\_acc improves by 7.7 \%, indicating faster convergence; the KD curve overtakes baseline after six rounds and maintains \(\approx 3\%\) lead. Expected calibration error drops from 4.9 \% to 3.5 \%. The extra teacher forward pass raises wall-time by only 0.9 s per round (\(\approx 9\%\)).

\subsection{Experiment 2: harsh federated conditions}
FedMPQ delivers 61.3 \(\pm 0.6\) \% accuracy (AUC\_acc \(= 3961\), 1.9 MB, 7.9 s). Q-FedAvg logs 58.4 \(\pm 0.5\) \% (3684, 1.9 MB, 8.4 s). FedKD attains 62.2 \(\pm 0.4\) \% (4030) but explodes communication to 40 MB and increases wall-time to 9.8 s. FedMPQ-KD reaches 63.0 \(\pm 0.5\) \% with \(\alpha = 0.5\) and 63.4 \(\pm 0.6\) \% with \(\alpha = 1.0\) (AUC\_acc up to 4211) while keeping bandwidth at 1.9 MB and wall-time below 8.6 s (+9 \%). Accuracy variance across clients shrinks from 4.8 \% (FedMPQ) to 3.9 \% (KD). Under 10 \% label noise, performance degrades by \(-3.8\) \% instead of \(-5.2\) \%. CIFAR-100-C shift accuracy is 59.2 \% versus 56.7 \% and FGSM robustness rises from 22.6 \% to 24.1 \%.

\subsection{Ablation and sensitivity}
Sweeping \(\alpha\) shows monotonic gains up to \(\alpha = 1.0\), beyond which returns diminish. Temperature \(T = 2\) consistently outperforms \(T = 1\) and \(T = 4\). Updating the teacher jointly with the student reduces accuracy by \(\approx 0.3\%\) and raises compute by 40 \%. Refreshing the teacher once per round suffices; intra-epoch refresh offers no benefit.

\subsection{Limitations}
All timing numbers stem from a single A100 GPU; edge devices may exhibit different overhead ratios. Only vision tasks were evaluated; preliminary 2-bit experiments show modest gains (+0.9 \%), hinting at diminishing returns with extremely low capacity.

\section{Conclusion}
This work introduces FedMPQ-KD, a drop-in extension to mixed-precision federated learning that remedies under-fitting in the low-epoch regime via in-round knowledge distillation. By adding a temperature-scaled KL term between a quantised student and a frozen full-precision teacher already present on each client, the method supplies rich gradients without extra communication and with \(<10\%\) computational overhead. Across CIFAR-10 and CIFAR-100 benchmarks, FedMPQ-KD improves final accuracy by up to 2.6 \%, halves the gap to full precision, accelerates convergence and enhances calibration and robustness, all while preserving the original bandwidth footprint.

These findings underscore the power of objective-level tweaks for boosting the accuracy-cost trade-off in practical FL systems. Future work will (i) extend evaluation to language models and ultra-low-bit settings, (ii) explore dynamic schedules for the distillation weight and temperature, and (iii) integrate the approach with decentralised protocols such as Moshpit All-Reduce to tackle unreliable networks \cite{ryabinin-2021-moshpit}. More broadly, we hope this study spurs further investigation into communication-neutral distillation strategies that unlock the potential of mixed-precision models under stringent federated constraints \cite{chen-2023-mixed}.


\bibliographystyle{plainnat}
\bibliography{references}

\end{document}