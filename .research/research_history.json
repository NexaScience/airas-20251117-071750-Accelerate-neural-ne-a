{
  "research_topic": "Accelerate neural network training",
  "queries": [
    "distributed mixed-precision training"
  ],
  "research_study_list": [
    {
      "title": "Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices",
      "abstract": "While federated learning (FL) systems often utilize quantization to battle communication and computational bottlenecks, they have heretofore been limited to deploying fixed-precision quantization schemes. Meanwhile, the concept of mixed-precision quantization (MPQ), where different layers of a deep learning model are assigned varying bit-width, remains unexplored in the FL settings. We present a novel FL algorithm, FedMPQ, which introduces mixed-precision quantization to resource-heterogeneous FL systems. Specifically, local models, quantized so as to satisfy bit-width constraint, are trained by optimizing an objective function that includes a regularization term which promotes reduction of precision in some of the layers without significant performance degradation. The server collects local model updates, de-quantizes them into full-precision models, and then aggregates them into a global model. To initialize the next round of local training, the server relies on the information learned in the previous training round to customize bit-width assignments of the models delivered to different clients. In extensive benchmarking experiments on several model architectures and different datasets in both iid and non-iid settings, FedMPQ outperformed the baseline FL schemes that utilize fixed-precision quantization while incurring only a minor computational overhead on the participating devices.",
      "full_text": "Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices Huancheng Chen University of Texas at Austin huanchengch@utexas.edu Haris Vikalo University of Texas at Austin hvikalo@ece.utexas.edu Abstract While federated learning (FL) systems often utilize quan- tization to battle communication and computational bottle- necks, they have heretofore been limited to deploying fixed- precision quantization schemes. Meanwhile, the concept of mixed-precision quantization (MPQ), where different layers of a deep learning model are assigned varying bit-width, remains unexplored in the FL settings. We present a novel FL algorithm, FedMPQ, which introduces mixed-precision quantization to resource-heterogeneous FL systems. Specif- ically, local models, quantized so as to satisfy bit-width constraint, are trained by optimizing an objective function that includes a regularization term which promotes reduc- tion of precision in some of the layers without significant performance degradation. The server collects local model updates, de-quantizes them into full-precision models, and then aggregates them into a global model. To initialize the next round of local training, the server relies on the infor- mation learned in the previous training round to customize bit-width assignments of the models delivered to different clients. In extensive benchmarking experiments on several model architectures and different datasets in both iid and non-iid settings, FedMPQ outperformed the baseline FL schemes that utilize fixed-precision quantization while in- curring only a minor computational overhead on the par- ticipating devices. 1. Introduction Federated Learning (FL) [27, 32, 37, 52] paradigm enables collaboration among numerous distributed devices (clients) while protecting their privacy by avoiding collection of data stored at those devices. The vanilla FL algorithm, FedAvg [37], deploys a client-server framework in which the server periodically collects locally trained/updated models from the clients and aggregates them into the global model. A number of of follow-up studies [44, 54, 55, 62] explored convergence guarantees for FedAvg with convex and non- convex loss functions in the settings where clients’ data is independent and identically distributed (i.i.d.). How- ever, in real-world scenarios, the participating clients’ data is likely to be non-i.i.d., which has detrimental effects on the convergence properties of FedAvg [33]. This motivated a considerable amount of research aiming to address the challenges of statistical heterogeneity in federated learning [2, 5–7, 27, 30–32, 47, 48, 52, 65]. On a related note, clients in large-scale FL systems are likely to have varying com- munication and computational resources at their disposal, requiring development of more sophisticated, constraints- aware schemes [26]. Recently, there has been a growing in- terest in compressing models so that they can be deployed on devices having limited resources [10]. A particularly ef- fective model compression technique is that of model quan- tization [20, 38, 39, 42, 43, 61]. The use of model quantization in FL has primarily been motivated by the high communications costs that arise when the server collects updates from a potentially very large number of clients. Reducing the precision of local up- dates by using lower bit-width to represent model param- eters allows the clients with limited bandwidth to efficiently communicate with the server [17, 20, 24, 39, 40, 42, 43]. However, existing quantization-based FL algorithms ex- pect each client to learn a full-precision local model re- gardless of the potential constraints on the client’s compu- tational resources. In real-world settings, clients such as smart phones or wearable devices may not have sufficient amount of memory to allow full-precision model training. Nevertheless, there are only few prior studies considering quantization-aware training in FL [1, 13, 25, 38, 61], all of them implementing fixed-precision quantization (FPQ) of local models. The recent advancements in mixed-precision quantization (MPQ) [1, 11, 51, 53, 56, 58, 59] remain unex- plored in the federated learning settings. Adapting the exist- ing MPQ schemes to federated learning is far from straight- forward. The existing MPQ schemes typically train a full- precision model to enable computing the quantization er- rors which are then used to optimize layer-wise bit-width allocation. Both search-based [53, 56] and optimization- 1 arXiv:2311.18129v1  [cs.LG]  29 Nov 2023based [51, 59, 64] MPQ methods consume significantly more computation than the FL training itself. The aim of this paper is to introduce mixed-precision quantization to federated learning, developing an efficient framework that addresses the challenge of resource hetero- geneity in FL. In particular, we study federated learning sys- tem where the clients deploy local models at average bit- width that may vary from one device to another. In such scenarios, clients with low average bit-width budgets can- not run computationally intensive MPQ methods. We pro- pose FedMPQ, a novel Federated learning algorithm with Mixed-Precision Quantization, which enables training of quantized local models within the allocated bit-width bud- get. FedMPQ first initializes local models as fixed-precision quantized networks that satisfy clients’ average bit-width budget, and then converts these quantized networks into a representation that allows bit-level sparsity-promoting training. In particular, while learning local models whose parameters admit binary representation, the clients deploy a group Lasso regularization term which imposes a trade- off between the task loss and bit-sparsity. The precision of layers that end up having parameters which exhibit higher degree of sparsity is reduced to allow increasing precision of other layers. During the aggregation step, FedMPQ em- ploys the pruning-growing strategy where the server aggre- gates clients’ models (locally trained at potentially differ- ent bit-widths), resulting in the global model. Before trans- mitting the global model to a client, the bit-width of the model is adjusted to match the client’s bit-width budget. To evaluate the effectiveness of FedMPQ, we conducted exper- iments on CIFAR10, CIFAR100 and Tiny-Imagenet [29]. FedMPQ outperforms the baseline in non-i.i.d. data settings and achieves performance similar to the FPQ8 baseline even though a subset of the clients train their models at a very low precision. The contributions of the paper are summarized as follows: • We propose a method for mixed-precision quantization in FL which does not require training full-precision models on devices. • We introduce a pruning-growing strategy for allocating layer-wise bit-width without a need for computation- ally expensive procedures that may violate resource con- straints. • We conduct extensive experiments in non-i.i.d. FL set- tings where the clients have heterogeneous computational resources, demonstrating the performance of the pro- posed FedMPQ. 2. Related Work 2.1. Quantization for Federated Learning Much of the existing research on federated learning under resource constraints focuses on quantizing local updates for the purpose of reducing communication bandwidth [20, 61]. The milestone work, FedPAQ [39], presents a federated learning framework where each client communicates quan- tized local updates to the server, and provides analytical convergence guarantees for both strongly-convex and non- convex settings. FedCOMGATE [16] extends the ideas of FedPAQ to introduce a local gradient tracking scheme mit- igating detrimental effect introduced when learning from non-i.i.d. data. UVeQFed [43] takes a step further, utiliz- ing vector quantization to move from lossy to lossless com- pression. The follow-up studies [20, 24, 36, 42] propose adaptive quantization for local updates in communication- constrained settings while still requiring clients to locally train full-precision models. AQFL [1] took a step towards mitigating the detrimen- tal effects of computational heterogeneity by training quan- tized models with bit-widths proportional to the clients’ computational resources. The follow-up works [8, 13, 61] developed a series of methods improving the server’s aggre- gation of local updates quantized at varying levels. How- ever, these methods are limited to fixed-precision quantiza- tion, assigning the same bit-width to the entire model. This motivates us to explore mixed-precision quantization in FL schemes as an alternative approach to learning in computa- tionally heterogeneous scenarios. 2.2. Mixed-Precision Quantization Aiming to enable stronger expressiveness of learned mod- els, mixed-precision quantization (MPQ) assigns different bit-widths to different layers/modules of the models. The MPQ strategies, which generally attempt to assign bit- widths in proportion to the importance of different layers, can be organized in three categories: (1) search-based, (2) optimization-based, and (3) metric-based. Search-based methods such as HAQ [53] and AutoQ [34] utilizes reinforcement learning (RL) [46] to pursue op- timal bit-widths where the model performance is set as the reward. DNAS [56] and SPOS[14] apply neural architec- ture search (NAS) to explore the quantization space which is growing exponentially with the number of layers. Since both RL-based and NAS-based methods require an enor- mous amount of computation, it is unrealistic to implement them in FL settings. Optimization-based methods approach MPQ from an op- timization perspective by formulating the bit-width allo- cation problem using differentiable variables and apply- ing the straight-through [3, 51, 59, 64] or gumbel-softmax [4, 15, 21, 23] estimator. However, this leads to mixed- integer programming problems which are NP-hard, render- ing the use of optimization-based methods in FL scenarios practically infeasible. In contrast to the search-based and optimization-based methods, metric-based methods leverage a variety of met- 2rics to evaluate the importance of layers and subsequently decide on the bit-width allocation. Such metrics include the eigenvalues of the Hessian matrix [9, 11, 12, 60], orthogo- nality conditions [35], entropy measures [45], synaptic flow [49] and learnable layer-wise importance [50]. The compu- tation of the aforementioned metrics is relatively expensive as it requires the information from full-precision models. Recently, BSQ [58] presented a bit-pruning MPQ strategy that achieves high compression rate while preserving model performance; however, BSQ simulates binary representa- tion with floating-point values, making it impractical for FL settings. 3. Methodology 3.1. Federated Learning with Quantization In a cross-device scenario with N clients, where client n owns a private dataset Dn, the standard federated learning (FL) considers training a single global model W by mini- mizing the loss (empirical risk) min W L(W) = NX n=1 pnLn(W), (1) where Ln(·) is the local loss function onDn and pn ∈ [0, 1] denotes the weight assigned to client n. At round t of FedAvg [37], a widely used FL algorithm, each partici- pating client locally trains a model Wt n on local data and communicates it to the server; the server aggregates the collected local models to form the global model Wt =PN k=1 pnWt n. Since the clients with restricted resources may not implement full-precision model training, one may instead opt for quantization-aware training. In that case, the aggregation at the server can be described as Wt = NX k=1 pnQbn(Wt n) s.t. bn · m/∥m∥1 ≤ vn, ∀n ∈ [N], (2) where Qbn denotes the mixed-precision quantizer, bn ∈ ZL denotes the bit-width assigned to each layer; m = {M(1), . . . , M(L)} ∈ZL is the number of parameters in each layer of model, L is the number of layers, and vn is the budget of the average bit-width for client n. Since in resource-heterogeneous FL vn varies across clients, naive aggregation according to Eq. 2 may discard beneficial knowledge of high-precision models. 3.2. Binary Representation of Model Parameters In the conventional deep neural networks (DNNs), the full- precision model parameters are typically stored in 32-bit floating-point form. Compared to the floating-point for- mat, integer-arithmetic operations such as mult, add and shift, on model parameters in fixed-point representation are more efficient and hardware-friendly. Following studies [41, 57, 58, 61], we assume clients perform training us- ing low-precision fixed-point quantization. A B-bit matrix W(l) ∈ RC×K of the parameters in the l-th layer of the model can be represented in binary format with a ternary matrix B(l) ∈ {0, 1}B×C×K, a layer-wise scaling factor s(l) with floating-point value, and a layer-wise zero-point z(l) ∈ Z+ as W(l) j,k = s(l) 2B − 1  BX i=1 2i−1B(l) i,j,k − z(l) ! , (3) where z(l) is typically set to2B−1 (signed integer); the scal- ing factor s(l) is updated at each training round accord- ing to the maximum absolute value of the parameters at the l-th layer. Therefore, the product between activation A ∈ RK×U and W(l) can be simplified by using shift and add according to A⊤ ·,u · W(l) j,· = s(l) 2B − 1 KX k=1 Ak,u  BX i=1 2i−1B(l) i,j,k − z(l) ! = s(l) 2B − 1  BX i=1 2i−1A⊤ ·,u · B(l) i,j,· − z(l) KX k=1 Ak,u ! . (4) Note that B(l) consists of discrete values 0 and 1, which cannot be searched for via gradient descent. To optimize over these binary parameters, we adopt the straight-through estimator (STE) [3] as in [58]. STE enables a quantized network to forward pass intermediate signals using model parameters represented in fixed-point format (as shown in Eq. 3) while computing the gradients with continuous floating-point parameters as Forward: W(l) j,k = s(l) 2B − 1  BX i=1 2i−1B(l) i,j,k − z(l) ! Backward: ∂L ∂B(l) i,j,k = s(l)2i−1 2B − 1 ∂L ∂W(l) j,k (5) where Backward pass follows from the chain rule. BSQ [58] relaxes the binary constraint and allows using floating- point values to update B(l) i,j,k, which means that BSQ trains the networks with simulated quantization [22]. Different from BSQ, we adapt the W AGE [22] strategy and update binary parameters using integer operations via the power- of-two function S(·) defined as S(x) = 2⌈log x⌋, (6) where S(x) returns the nearest power-of-two ofx. Then we compute an update of W(l) j,k via gradient descent with step 3size η according to ∆W(l) j,k = −ν · s(l) 2B − 1 BX i=1 2i−1S   η \f\f\f\f\f ∂L ∂B(l) i,j,k \f\f\f\f\f ! = −ν · s(l) 2B − 1 BX i=1 2qi−1, (7) where ν ∈ {−1, 1} denotes the sign of ∂L ∂W(l) j,k , and qi is the power of 2i · S \u0012 η \f\f\f\f ∂L ∂B(l) i,j,k \f\f\f\f \u0013 . According to Eq. 5, qi is strictly ascending, i.e., qi < qj for i < j. Let Q = {q1, . . . , qB} denote the set of qi, and let max(Q) be the maximum element in Q. On one hand, if max(Q) > B the absolute value of∆W(l) j,k exceeds the scale s(l) and thus ∆W(l) j,k needs to be clipped as C(∆W(l) j,k) = −ν · s(l) 2B − 1 BX i=1 2i−1. (8) Note that ∆W(l) j,k cannot be represented by a fixed-point in- teger value ifqi ≤ 0 ∈ Q. Adapting the strategy of updating parameters with small-magnitude gradients in W AGE [22], ∆W(l) j,k is converted to C(∆W(l) j,k) = − ν · s(l) 2B − 1 BX i=1 2i−1 · I{i ∈ Q} − ν · s(l) 2B − 1Bernoulli( X qi≤0 2qi−1), (9) where I(·) is an indicator, and Bernoulli(·) randomly sam- ples decimal parts to either 0 or 1. When the magnitude of the gradient is small, the integer part of ∆W(l) j,k is al- ways 0, which impedes the update of the parameters. Due to the second term on the right hand side in Eq. 9, W(l) j,k is updated with the minimum step size even if the gradient is very small. After converting ∆W(l) j,k to the fixed-point format, one can update the parameters according to W(l) j,k ← −Clipping (W(l) j,k + C(∆W(l) j,k), min(l), max(l)), (10) where min(l) = − s(l) 2B−1 z(l) denotes the minimum value of the parameters, and max (l) = s(l) 2B−1 \u0000 2B − 1 − z(l)\u0001 is the maximum value of the parameters in the l-th layer. Since the updated W(l) j,k is in fixed-point format, updating binary representation B(l) i,j,k is straightforward. 3.3. Sparsity-Promoting Training Sparsity-promoting regularizers including L1 (Lasso) and L2 (ridge) have been widely used to induce sparsity e.g. during feature selection. Following BSQ [58], we use group Lasso regularization [18] to promote obtaining highly sparse parameters and ensure stable convergence. The group Lasso regularizer for the parameters in the l-th layer is defined as RGL(B(l)) = b(l) X i=1 \r\r\rB(l) i,·,· \r\r\r 2 , (11) where b(l) denotes the bit-width of the l-th layer, and B(l) i,·,· ∈ {0, 1}C×K is the i-th binary representation posi- tion of the parameters in the l-th layer. Since the number of parameters typically differ from one layer to another, we vary the weights assigned to the group Lasso regularizers according to the memory constraints. Specifically, the ob- jective function used in local training is formulated as Llocal = Ltask(B(1:L)) + λ LX l=1 M(l) M RGL(B(l)), (12) where M(l) denotes the number of parameters in the l- th layer, M = PL l=1 M(l), and λ is a non-negative pre- determined hyper-parameter. Note that Ltask(B(1:L)) can be computed through the STE forward pass while the gra- dients can be computed through the STE backward pass as indicated in Eq. 5. Let δ(l) i denote the sparsity of the i-th binary representation position in the l-th layer, δ(l) i = \r\r\rB(l) i,·,· \r\r\r 0 /M(l). (13) Essentially, δ(l) i is the proportion of the parameters having 1 at the i-th binary representation position. The smaller δ(l) i is, the higher the sparsity at the i-th position of the param- eters’ binary representation. We set up a threshold ϵ and prune the most significant bits (MSBs) ifδ(l) b(l) ≤ ϵ (see Fig- ure 1). 3.4. The End-to-End Training Procedure The local sparsity-promoting training and MSBs pruning results in reduced bit-widths of the model parameters in certain layers, leading to a higher compression rate while maintaining the model’s performance. However, our aim is to leverage mixed-precision quantization to improve the performance of the global model given all of the available resources, rather than pursue high compression rate. To this end, we propose a pruning-growing procedure at the server that restores the average bit-width allocated to each local model. In particular, the server aggregates the collected lo- cal models characterized by different bit-width allocations into the global model Wt+1 with the global bit-width al- location bt+1. For a given client, bt+1 is adjusted to sat- isfy the client’s bit-width constraint via 2 operations: (1) 4Figure 1. An example of pruning the MSBs in the model parame- ters B(l). Since the 4-th and 3-rd bits in B(l) have a fraction of1’s below the thresholdϵ, the bit-width b(l) of the l-th layer is reduced from 4 bits to 2 bits. For clarity, ϵ is set to 0.4 in the example but the value of ϵ is smaller in the experiments. pruning: reducing the bit-width of a subset of layers for the clients with low bit-width budget until the constraint in Eq. 2 is satisfied; (2) growing: increasing the bit-width of a subset of layers for the clients with high bit-width budget until violating the constraint in Eq. 2. Alg. 2 specifies the greedy policy used to select the layers for pruning or grow- ing. Next, we provide an outline of the end-to-end training procedure, as formalized in Alg.1. 3.4.1 Initialization and Local Training At the start of training, the local bit-width assignment of client n is initialized as ˆb0 n = {vn, . . . , vn}, while the global model W0 (full precision) is randomly initialized at the server. Then, W0 is converted to a vn-bit fixed-point representation G0 n, with scale s0 n ∈ RL set to the maximum absolute value of the model parameters in each layer, i.e., s(l),0 n = max j,k \f\f\fW(l),0 j,k \f\f\f. (14) The customized global model G0 n is then sent to client n to be used for initialization of local training. With such an initialization, the local model B0 n ← −G0 n satisfies the con- straint of average bit-width vn. At global round t, client n receives the global model Gt n and uses it to initializeBt n. The client n updates Bt n, bt n and potentially st n, and possibly prunes MSBs of the model pa- rameters across different layers as discussed in Sections 3.2 and 3.3. After updating, the client n sends Bt+1 n and bt+1 n to the server for aggregation. 3.4.2 Models Aggregation The server collects local models Bt+1 n and the correspond- ing local bit-width assignments bt+1 n from the participating clients, n ∈ [N], and then converts the collected fixed-point models to the floating-point models according to Eq. 3. The Algorithm 1:FedMPQ Input: global round T, local epochs τ, budget vn, local data Dn, number of parameters m, λ and ϵ. Output: the global model WT 1 initialize W0, G0 n, ˆb0 n ← − {vn, . . . , vn}, ∀n ∈ [N] ; 2 for t = 0, . . . , Tdo /* local training in the clients */ 3 for n = 1, . . . , Ndo 4 Bt n ← −Gt n, bt n ← −ˆbt n; 5 Bt+1 n , bt+1 n ← −LocalUpdate(Dn, τ, λ, ϵ); 6 send Bt+1 n , bt+1 n to the server; 7 end /* aggregation in the server */ 8 for n = 1, . . . , Ndo 9 Wt+1 n ← −ConvertToFP(Bt+1 n , bt+1 n ); 10 end 11 Wt+1 ← −PN n pnWt+1 n , bt+1 ← −PN n pnbt+1 n ; /* post-aggregation adjustment */ 12 for n = 1, . . . , Ndo 13 ∆bt n ← −ˆbt n − bt+1 n ; 14 ˆbt+1 n ← −Pruning-Growing(bt+1, ∆bt n, m, vn); 15 Gt+1 n ← −Binary-Representation(Wt+1 n , ˆbt+1 n ); 16 end 17 end global model and the average bit-width are computed ac- cording to Eq. 2, Wt+1 = NX n=1 pnWt+1 n , bt+1 = NX n=1 pnbt+1 n , (15) where pn = vn · |Dn|/P is proportional to the number of samples in local dataset Dn and client n’s budget vn, and P = PN i=1 vi · |Di|. Note that larger weights are assigned to the clients having higher bit-width budgets and training on larger local datasets. The local bit-width assignment bt+1 n is learned during the local sparsity-promoting training, where the bit-widths for less sensitive layers are reduced while the bit-widths for more sensitive layers are preserved. Note that the sensitivity of layers can vary across clients with non-i.i.d data, as it is affected by the data distribution. bt+1 aggregates local bit-width assignments and is reflective of the importance of different layers. 3.4.3 Post-Aggregation Adjustment Due to the constraints on the local bit-width, the aggregated full-precision global model Wt+1 cannot be directly broad- casted to the clients. Similar to the initialization described in Section 3.4.1, the server needs to customize different fixed-point global models Gt+1 n with bit-width assignments ˆbt+1 n based on bt+1 and budget vn. There are three options 5Algorithm 2:Pruning-Growing Input: bt+1, ∆bt n, m, vn Output: bit-width assignment for client n, ˆbt+1 n 1 initial: ˆbt+1 n ← −bt+1, v ← −ˆbt+1 n · m/ ∥m∥1, d ← −argDescending(m ⊙ (∆bt n + 1)), cur ← −0; /* Pruning */ 2 while v > vn and cur <|m| do 3 l ← −d[cur] ; 4 if ˆbt+1 n [l] > 1 then 5 ˆbt+1 n [l] ← −ˆbt+1 n [l] − 1, v ← −v − m[l]/ ∥m∥1; 6 else 7 cur ← −cur + 1; 8 end 9 end 10 cur ← − |m| −1; /* Growing */ 11 while v < vn and cur >0 do 12 l ← −d[cur] ; 13 if ˆbt+1 n [l] < 8 then 14 ˆbt+1 n [l] ← −ˆbt+1 n [l] + 1, v ← −v + m[l]/ ∥m∥1; 15 else 16 cur ← −cur − 1; 17 end 18 end for updating bt+1 n : ˆbt+1 =    bt+1, if v = vn, pruning(bt+1), if v > vn, growing(bt+1), if v < vn, (16) where v = bt+1 · m/ ∥m∥1. The first assignment is han- dled in a straightforward manner. For the later two we apply a greedy policy for executing pruning or growing. Specifi- cally, when v > vn, we attempt to reduce the bit-width of the layer having the most parameters, attempting to satisfy the budget constraint vn while maintaining the precision of other layers. When v < vn, we first increase the bit-width of the layer with the fewest parameters. If two layers have the same number of parameters, the pruning preference is given to the layer whose bit-width in the last round of lo- cal sparsity-promoting training had been reduced more, as that suggests this layer is less important. It is beneficial to record the bit-width change during local training by defin- ing ∆bt n = ˆbt n − bt+1 n . Further details of pruning and growing are specified in Alg. 2. 4. Experiments 4.1. Setup We evaluate the performance of the proposed FedMPQ in various settings on three datasets: CIFAR10, CIFAR100 and Tiny-ImageNet [28]. We perform the experiments using ResNet20 [19] model for CIFAR10/100, and ResNet44 [19] model for Tiny-ImageNet. We use the mini-batch stochas- tic gradient descent (SGD) with a learning rate initialized to 0.1 in all experiments. The SGD momentum is set to 0.9 and the weight decay is set to 0.0005. The batch size is set to 64 and the number of global rounds is set to 50, with 5 local epochs within each global round. The value of the bit-pruning threshold ϵ is 0.03, while the regularizing hyper-parameter λ is equal to 0.01. The number of clients is 10; unless stated otherwise, the fraction of participating clients is 0.5. Following the strategy in [63], we use Dirich- let distribution with varying concentration parameter α to generate data partitions at different levels of heterogeneity (smaller α leads to generating less balanced data). Since there exist no prior methods for mixed-precision quantization-aware training (QAT) in FL, we primarily fo- cus on comparing FedMPQ with the AQFL [1], the first method to deploy fixed-precision quantization-aware train- ing in FL. As FedMPQ only modifies the precision of the model weights, we fix the precision of the activa- tion to 4 bits in both FedMPQ and AQFL throughout the entire training process. Furthermore, we implement FedAvg with full-precision (FP32) training, 8-bits fixed- precision quantization-aware training (FPQ8) [57], and two communication-efficient FL methods, FedPAQ [39] and UVeQFed [43], which train full-precision local models but then utilize scalar and vector quantization to compress the local updates before communicating them to the server. 4.2. Effects of Data Heterogeneity To evaluate our method in the scenarios characterized by varied levels of data heterogeneity, we conduct3 sets of ex- periments where α takes on values from {0.1, 0.5, 1}; these correspond to severely imbalanced, moderately imbalanced and mildly imbalanced data, respectively. Results of the experiments are reported in Table 1. As can be seen there, performance of the global model of all the considered meth- ods deteriorates as the data heterogeneity increases. Fed- PAQ and UVeQFed, the two communication-efficient FL approaches, achieve performance comparable to the FP32 baseline when α = 0.5 and 1 but experience performance degradation when α = 0.1. A significant performance de- cline is experienced in the experiments with FPQ8 due to the low capacity of the low-precision models. However, the performance gap between FP32 and FPQ8 narrows as the level of data heterogeneity increases. For instance, when α = 1 , the test accuracy of FP32 is 11.3% higher than that of FPQ8; when α = 0 .1, the test accuracy differ- ence is 5.8% (experiments on CIFAR10). When the data is extremely imbalanced, FedAvg suffers from the so-called “client-drift” problem [27] caused by overfitting on the lo- cal data. As a result, the model capacity advantage of FP32 (due to having higher precision) might not make as much of 6Table 1. Test accuracy (%) of the considered schemes as the concentration parameter α takes values from {0.1, 0.5, 1}. The number of clients in these experiments is 10, while their average bit-width budgets arev = {2, 2, 4, 4, 4, 6, 6, 6, 8, 8} (vn denotes the budget of client n). The numbers in the column “Update”, “Weight” and “Activation” indicate the bits used to store the local update values, model weights and the activation signals, respectively. The last column indicates whether the scheme needs to train a full-precision model or not. CIFAR10 CIFAR100 Tiny-ImageNet Update Weight Activation Full-Precision?α 0.1 0 .5 1 0 .1 0 .5 1 0 .1 0 .5 1 FP32 60.3 77.5 82.1 40.3 47.0 49.6 24.6 35.1 38.1 32 32 32 \" FedPAQ 56.3 77.0 81.2 39.7 46.8 48.4 22.87 34.6 37.5 vn 32 32 \" UVeQFed 56.8 76.7 81.5 38.6 46.5 48.8 21.3 34.3 37.4 vn 32 32 \" FPQ8 54.5 68.4 70.8 35.4 41.3 42.3 23.8 33.4 35.6 8 8 8 % AQFL 44.3 58.0 62.1 23.8 32.9 36.1 17.1 23.5 25.3 vn vn 4 % FedMPQ 49.1 67.1 69.3 31.7 41.1 43.6 20.3 27.0 28.2 vn vn 4 % Table 2. Test accuracy (%) as the number of clients N varies over 10, 20 and 40. Here, 20%, 30%, 30% and 20% clients have aver- age bit-width budget of2 bits, 4 bits, 6 bits and 8 bits, respectively. The concentration parameter α is set to 0.5. CIFAR10 CIFAR100 N 10 20 40 10 20 40 FP32 77.5 68.1 64.5 47.0 40.1 35.0 FedPAQ 77.0 61.5 59.9 46.8 38.9 33.1 UVeQFed 76.7 66.4 63.2 46.5 39.5 34.1 FPQ8 68.4 56.3 48.4 41.3 36.2 31.9 AQFL 58.0 49.7 37.3 32.9 20.4 13.9 FedMPQ 67.1 56.8 45.4 41.1 26.1 19.9 a positive impact on the accuracy, explaining the narrowing gap between FP32 and FPQ8. Since local models with ultra-low precision are aggre- gated into the global model, performance of AQFL shows further deterioration. The proposed method, FedMPQ out- performs AQFL (implementing fixed-precision quantiza- tion) in all scenarios even though training models under the same resource constraints. As shown in Table.1, FedMPQ outperforms AFQL at most9.1%, 8.2% and 2.9% test accu- racy on CIFAR10, CIFAR100 and Tiny-ImageNet respec- tively. On CIFAR10/100 datasets, FedMPQ nearly pre- serves performance of FPQ8 baseline in the settings α = 0.5 and 1 by efficiently allocating precision to different layers, even though training the global model on resource- constrained heterogeneous devices. 4.3. Scalability To evaluate the effect of the system size on the perfor- mance of FedMPQ, we conducted 3 sets of experiments on CIFAR10/100 data in the FL system with 10, 20 and 40 clients. To simulate a resource-constrained heterogeneous system, we allocate to 20%, 20%, 20% and 30% clients the bit-width budget of 2, 4, 6 and 8 bits, respectively. The concentration parameter α is set to 0.5 and kept constant throughout the experiments. Not surprisingly, as the results in Table 2 show, the per- formance of all schemes deteriorates as the FL training in- volves an increasingly larger number of clients with ultra- low budget. FedMPQ consistently outperforms AQFL, with up to 9.1% and 8.2% higher accuracy on CIFAR10 and CI- FAR100, respectively. Note that FedMPQ manages to ap- proach the performance of FPQ8 in the settings involving 10 clients, though the gap (as expected) widens as the num- ber of clients participating in the system grows. 4.4. The Number of Local Epochs We study the impact of the number of local epochs on the system’s performance by conducting 5 sets of exper- iments where the number of local epochs is varied over {1, 5, 10, 15, 20}. The results are shown in Figure 2. As can be seen there, when the number of local epochs is set to 1, the three quantization-aware training methods show sig- nificant performance degradation while the impact on FP32 baseline is only minor. This is likely due to underfitting of the quantized models. For larger numbers of local epochs, FedMPQ provides a global model whose performance is comparable to FPQ8, demonstrating remarkable improve- ment over AQFL. 4.5. Fine-Tuning Hyper-Parameters The experiments discussed in the previous section are con- ducted with hyperparameters ϵ = 0 .03 and λ = 0 .01. Note that ϵ and λ jointly affect the training: λ controls the weight of the group Lasso regularization used in lo- cal training while ϵ controls the threshold for pruning the MSBs after local training. To explore the space of hyper- parameters, we consider a number of configurations with varied values of ϵ ∈ {0.01, 0.02, 0.03, 0.04, 0.05} and λ ∈ {0.1, 0.01, 0.001}. As the results shown in Table 3 indicate, when the threshold ϵ is too large, the accuracy considerably deteriorates since a larger fraction of model parameters gets compressed. Selecting large regularization weights, e.g. 7(a) CIFAR10  (b) CIFAR100 Figure 2. Top-1 accuracy vs. the number of local epochs. In all experiments, the number of clients is N = 10, the concentration parameter α = 0.5, and the number of global rounds is set to 50. All results are the final test accuracy after 50 global rounds. Table 3. Test accuracy ( %) of FedMPQ running with different combinations of threshold value ϵ and regularization weights λ. All experiments are on CIFAR10 with 10 clients and α = 0.5. threshold value ϵ λ 0.01 0.02 0.03 0.04 0.05 0.1 64.1 63.4 65.9 63.9 64.8 0.01 66.4 68.0 67.1 64.7 64.3 0.001 67.1 70.0 65.5 64.7 64.4 λ = 0.1, leads to the performance drop since FedMPQ fo- cuses on pursuing higher bit-level sparsity. Our experiments suggest that to achieve satisfactory performance, one should select ϵ ≤ 0.03 and λ ≤ 0.01. 4.6. An Ablation Study In this section, we empirically analyze the effect of each procedure in FedMPQ by a comparison to the AQFL base- line. The three procedures that distinguish FedMPQ from AQFL include: (1) the group Lasso regularization in the objective function as described in Section 3.3; (2) bit-level pruning in the most significant bits (MSBs); and (3) Alg. 2 which restores the precision of local models. We refer to different combinations of these procedures, shown in Ta- ble 4, as settings (1)-(5). According to the results for setting (1) in Table 4, the group Lasso regularization achieves high performance while promoting bit-level sparsity, with small 1.2% and 0.3% drops in accuracy on CIFAR10 and CIFAR100, re- spectively. As shown in the results for setting (2), MSBs pruning without sparsity-promoting training causes more severe performance degradation, with 2.9% and 4.2% ac- curacy drop on these two datasets. Finally, by combining the group Lasso regularization with MSBs pruning (setting (3)) enables the global model to achieve performance close to the baseline, even though the precision is reduced af- ter MSBs pruning. Interestingly, the group Lasso regular- ization forces the clients to learn local models with highly Table 4. Test accuracy (%) of the global model trained using dif- ferent combinations of the FedMPQ subroutines. “Lasso” refers to the group Lasso regularization; “MSBs” denotes bit-level pruning. All experiments involve 10 clients. α, ϵand λ are set to 0.5, 0.03 and 0.01, respectively (the same setting as in Section 4.1). CIFAR10 CIFAR100 AQFL (baseline) 58.0 32.9 (1) Lasso 56.8 32.6 (2) MSBs 55.1 28.7 (3) MSBs + Lasso 56.2 31.8 (4) MSBs + Alg.2 65.4 40.1 (5) MSBs + Lasso + Alg.2 67.1 41.1 sparse binary weight representations, implying no degrada- tion due to bit-pruning. Algorithm 2 enables clients to recover layer-wise pre- cision budget allocated to their local models, resulting in significant performance improvements. For instance, test accuracy in setting (4) is 10.3% and 11.4% higher than in setting (2); setting (5) achieves 10.9% and 9.3% higher ac- curacy than setting (3) on CIFAR10 and CIFAR100, respec- tively. This ablation study provides an insight in how FedMPQ operates: the group Lasso regularization forces clients to learn a local model with bit-level sparsity while preserving performance; MSBs pruning allows reduction of the pre- cision of local models without major performance degra- dation; Algorithm 2, implemented at the server, conducts pruning-growing to restore the precision of local models so they fully exploit allocated bit-width budgets while seeking effective bit-width allocations to layers. The results in Ta- ble 4 suggest that Algorithm 2 plays a major role in helping FedMPQ achieve high accuracy. 5. Conclusion In this paper we presented FedMPQ, a novel framework for heterogeneous, resource-constrained federated learning systems, which aims to judiciously utilize bit-width budget of clients by conducting mixed-precision quantization. A group Lasso regularizer applied in local training promotes sparsity of the binary representation of the model param- eters, and then reduces the precision of a subset of the local model’s layers. The server deploys a greedy pruning- growing procedure that restores precision of the pruned local models to fully exploit the assigned bit-width budget. We conducted extensive experiments on several benchmark datasets for a number of problem configurations that simulate resource-heterogeneity across clients. The exper- imental results demonstrate that FedMPQ outperforms the fixed-precision quantization baseline, and provides perfor- mance remarkably close to the 8-bit quantization baseline. 8References [1] Ahmed M Abdelmoniem and Marco Canini. Towards mit- igating device heterogeneity in federated learning via adap- tive model quantization. In Proceedings of the 1st Workshop on Machine Learning and Systems , pages 96–103, 2021. 1, 2, 6 [2] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regular- ization. arXiv preprint arXiv:2111.04263, 2021. 1 [3] Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. 2, 3 [4] Zhaowei Cai and Nuno Vasconcelos. Rethinking differen- tiable search for mixed-precision neural networks. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2349–2358, 2020. 2 [5] Huancheng Chen and Haris Vikalo. Federated learning in non-iid settings aided by differentially private synthetic data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , pages 5026–5035, 2023. 1 [6] Huancheng Chen and Haris Vikalo. Accelerating non-iid federated learning via heterogeneity-guided client sampling. arXiv preprint arXiv:2310.00198, 2023. [7] Huancheng Chen, Chaining Wang, and Haris Vikalo. The best of both worlds: Accurate global and personalized models through federated learning with data-free hyper- knowledge distillation. In The Eleventh International Con- ference on Learning Representations, 2023. 1 [8] Shengbo Chen, Cong Shen, Lanxue Zhang, and Yuanmin Tang. Dynamic aggregation for heterogeneous quantization in federated learning. IEEE Transactions on Wireless Com- munications, 20(10):6804–6819, 2021. 2 [9] Weihan Chen, Peisong Wang, and Jian Cheng. Towards mixed-precision quantization of neural networks via con- strained optimization. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision , pages 5350– 5359, 2021. 3 [10] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A sur- vey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. 1 [11] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Ma- honey, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 293–302, 2019. 1, 3 [12] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Ad- vances in neural information processing systems, 33:18518– 18529, 2020. 3 [13] Ahmed Roushdy Elkordy and A Salman Avestimehr. Het- erosag: Secure aggregation with heterogeneous quantization in federated learning. IEEE Transactions on Communica- tions, 70(4):2372–2386, 2022. 1, 2 [14] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one- shot neural architecture search with uniform sampling. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVI 16, pages 544–560. Springer, 2020. 2 [15] Hai Victor Habi, Roy H Jennings, and Arnon Netzer. Hmq: Hardware friendly mixed precision quantization block for cnns. In Computer Vision–ECCV 2020: 16th European Con- ference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16, pages 448–463. Springer, 2020. 2 [16] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated learning with compression: Unified analysis and sharp guarantees. In In- ternational Conference on Artificial Intelligence and Statis- tics, pages 2350–2358. PMLR, 2021. 2 [17] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated learning with compression: Unified analysis and sharp guarantees. In In- ternational Conference on Artificial Intelligence and Statis- tics, pages 2350–2358. PMLR, 2021. 1 [18] Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the lasso and generaliza- tions. CRC press, 2015. 4 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 6 [20] Robert H ¨onig, Yiren Zhao, and Robert Mullins. Dadaquant: Doubly-adaptive quantization for communication-efficient federated learning. In International Conference on Machine Learning, pages 8852–8866. PMLR, 2022. 1, 2 [21] Xijie Huang, Zhiqiang Shen, Shichao Li, Zechun Liu, Hu Xianghong, Jeffry Wicaksana, Eric Xing, and Kwang-Ting Cheng. Sdq: Stochastic differentiable quantization with mixed precision. In International Conference on Machine Learning, pages 9295–9309. PMLR, 2022. 2 [22] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018. 3, 4 [23] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. 2 [24] Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, and Yonina C Eldar. Adaptive quantization of model updates for communication-efficient federated learning. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3110–3114. IEEE, 2021. 1, 2 [25] Yu Ji and Lan Chen. Fedqnn: A computation– communication-efficient federated learning framework for iot with low-bitwidth neural network quantization. IEEE In- ternet of Things Journal, 10(3):2494–2507, 2022. 1 [26] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista 9Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum- mings, et al. Advances and open problems in federated learn- ing. Foundations and Trends® in Machine Learning, 14(1– 2):1–210, 2021. 1 [27] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for feder- ated learning. In International conference on machine learn- ing, pages 5132–5143. PMLR, 2020. 1, 6 [28] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 6 [29] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 2 [30] Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv:1910.03581, 2019. 1 [31] Qinbin Li, Bingsheng He, and Dawn Song. Model- contrastive federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10713–10722, 2021. [32] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimiza- tion in heterogeneous networks. Proceedings of Machine learning and systems, 2:429–450, 2020. 1 [33] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019. 1 [34] Qian Lou, Feng Guo, Lantao Liu, Minje Kim, and Lei Jiang. Autoq: Automated kernel-wise neural network quantization. arXiv preprint arXiv:1902.05690, 2019. 2 [35] Yuexiao Ma, Taisong Jin, Xiawu Zheng, Yan Wang, Huixia Li, Yongjian Wu, Guannan Jiang, Wei Zhang, and Rongrong Ji. Ompq: Orthogonal mixed precision quantization. In Pro- ceedings of the AAAI Conference on Artificial Intelligence , pages 9029–9037, 2023. 3 [36] Yuzhu Mao, Zihao Zhao, Guangfeng Yan, Yang Liu, Tian Lan, Linqi Song, and Wenbo Ding. Communication-efficient federated learning with adaptive quantization. ACM Trans- actions on Intelligent Systems and Technology (TIST), 13(4): 1–26, 2022. 2 [37] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication- efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics , pages 1273–1282. PMLR, 2017. 1, 3 [38] Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Dig- gavi. Quped: Quantized personalization via distillation with applications to federated learning. Advances in Neural Infor- mation Processing Systems, 34:3622–3634, 2021. 1 [39] Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Has- sani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with pe- riodic averaging and quantization. In International Confer- ence on Artificial Intelligence and Statistics , pages 2021– 2031. PMLR, 2020. 1, 2, 6 [40] Felix Sattler, Simon Wiedemann, Klaus-Robert M ¨uller, and Wojciech Samek. Robust and communication-efficient fed- erated learning from non-iid data. IEEE transactions on neu- ral networks and learning systems, 31(9):3400–3413, 2019. 1 [41] Yusuke Sekikawa and Shingo Yashima. Bit-pruning: A sparse multiplication-less dot-product. In The Eleventh In- ternational Conference on Learning Representations , 2022. 3 [42] Nir Shlezinger, Mingzhe Chen, Yonina C Eldar, H Vincent Poor, and Shuguang Cui. Federated learning with quan- tization constraints. In ICASSP 2020-2020 IEEE Interna- tional Conference on Acoustics, Speech and Signal Process- ing (ICASSP), pages 8851–8855. IEEE, 2020. 1, 2 [43] Nir Shlezinger, Mingzhe Chen, Yonina C Eldar, H Vincent Poor, and Shuguang Cui. Uveqfed: Universal vector quanti- zation for federated learning. IEEE Transactions on Signal Processing, 69:500–514, 2020. 1, 2, 6 [44] Sebastian U Stich. Local sgd converges fast and communi- cates little. arXiv preprint arXiv:1805.09767, 2018. 1 [45] Zhenhong Sun, Ce Ge, Junyan Wang, Ming Lin, Hesen Chen, Hao Li, and Xiuyu Sun. Entropy-driven mixed- precision quantization for deep network design. Advances in Neural Information Processing Systems, 35:21508–21520, 2022. 3 [46] Richard S Sutton and Andrew G Barto. Reinforcement learn- ing: An introduction. MIT press, 2018. 2 [47] Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes.Advances in Neu- ral Information Processing Systems, 33:21394–21405, 2020. 1 [48] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning.IEEE Transactions on Neural Networks and Learning Systems, 2022. 1 [49] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iter- atively conserving synaptic flow. Advances in neural infor- mation processing systems, 33:6377–6389, 2020. 3 [50] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Wen Ji, Yaowei Wang, and Wenwu Zhu. Mixed-precision neural network quantization via learned layer-wise importance. In European Conference on Computer Vision, pages 259–275. Springer, 2022. 3 [51] Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision dnns: All you need is a good parametrization. arXiv preprint arXiv:1905.11452, 2019. 1, 2 [52] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency prob- lem in heterogeneous federated optimization. Advances in neural information processing systems , 33:7611–7623, 2020. 1 [53] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8612–8620, 2019. 1, 2 10[54] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and Kevin Chan. Adap- tive federated learning in resource constrained edge comput- ing systems. IEEE journal on selected areas in communica- tions, 37(6):1205–1221, 2019. 1 [55] Blake E Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. Ad- vances in neural information processing systems , 31, 2018. 1 [56] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantiza- tion of convnets via differentiable neural architecture search. arXiv preprint arXiv:1812.00090, 2018. 1, 2 [57] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018. 3, 6 [58] Huanrui Yang, Lin Duan, Yiran Chen, and Hai Li. Bsq: Ex- ploring bit-level sparsity for mixed-precision neural network quantization. arXiv preprint arXiv:2102.10462, 2021. 1, 3, 4 [59] Linjie Yang and Qing Jin. Fracbits: Mixed precision quanti- zation via fractional bit-widths. In Proceedings of the AAAI Conference on Artificial Intelligence , pages 10612–10620, 2021. 1, 2 [60] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gho- lami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural net- work quantization. In International Conference on Machine Learning, pages 11875–11886. PMLR, 2021. 3 [61] Jaehong Yoon, Geon Park, Wonyong Jeong, and Sung Ju Hwang. Bitwidth heterogeneous federated learning with pro- gressive weight dequantization. In International Conference on Machine Learning, pages 25552–25565. PMLR, 2022. 1, 2, 3 [62] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demysti- fying why model averaging works for deep learning. In Pro- ceedings of the AAAI Conference on Artificial Intelligence , pages 5693–5700, 2019. 1 [63] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khaza- eni. Bayesian nonparametric federated learning of neural networks. In International conference on machine learning, pages 7252–7261. PMLR, 2019. 6 [64] Zhaoyang Zhang, Wenqi Shao, Jinwei Gu, Xiaogang Wang, and Ping Luo. Differentiable dynamic quantization with mixed precision and adaptive resolution. In International Conference on Machine Learning , pages 12546–12556. PMLR, 2021. 2 [65] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learn- ing. In International conference on machine learning, pages 12878–12889. PMLR, 2021. 1 11",
      "meta_data": {
        "arxiv_id": "2311.18129v1",
        "authors": [
          "Huancheng Chen",
          "Haris Vikalo"
        ],
        "published_date": "2023-11-29T22:43:40Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18129v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces FedMPQ, a novel federated learning (FL) algorithm that brings mixed-precision quantization (MPQ) to resource-heterogeneous FL systems. It addresses the challenge of deploying varied bit-width models across clients without requiring computationally intensive full-precision model training on local devices. Key contributions include a method for MPQ in FL that does not require training full-precision models on devices, a pruning-growing strategy for efficient layer-wise bit-width allocation that avoids expensive procedures, and extensive experimental validation demonstrating superior performance over fixed-precision quantization baselines in non-i.i.d. FL settings with heterogeneous computational resources.",
        "methodology": "FedMPQ operates by initializing local models as fixed-precision quantized networks to satisfy client-specific average bit-width budgets. These networks are then converted to a binary representation that allows for sparsity-promoting training. Clients train local models using a group Lasso regularization term, which optimizes an objective function by balancing task loss and bit-sparsity. This regularization encourages reducing precision in less sensitive layers, freeing up bits for more critical layers. The straight-through estimator (STE) and WAGE strategy are adapted for updating binary parameters. During aggregation, the server collects local model updates, de-quantizes them into full-precision models, and aggregates them. For the next round, the server customizes bit-width assignments for each client using a greedy pruning-growing procedure (Algorithm 2). This procedure adjusts the global bit-width allocation to match each client's budget by either reducing bit-width for layers with many parameters (pruning) or increasing it for layers with fewer parameters (growing), prioritizing layers whose precision was reduced more in previous local training.",
        "experimental_setup": "The proposed FedMPQ was evaluated on three datasets: CIFAR10, CIFAR100, and Tiny-ImageNet. ResNet20 was used for CIFAR10/100, and ResNet44 for Tiny-ImageNet. Training employed mini-batch Stochastic Gradient Descent (SGD) with a learning rate of 0.1, momentum of 0.9, and weight decay of 0.0005, using a batch size of 64 for 50 global rounds with 5 local epochs per round. The bit-pruning threshold (ϵ) was set to 0.03, and the regularization hyper-parameter (λ) to 0.01. Experiments involved 10 clients (default, varied up to 40) with a 0.5 participation fraction. Data heterogeneity was simulated using a Dirichlet distribution with concentration parameters α ∈ {0.1, 0.5, 1}. Resource heterogeneity was modeled by assigning varying average bit-width budgets (e.g., 2, 4, 6, 8 bits) to different clients. FedMPQ's performance was compared against AQFL (fixed-precision QAT in FL), FedAvg with full-precision (FP32) training, 8-bit fixed-precision quantization-aware training (FPQ8), and communication-efficient methods like FedPAQ and UVeQFed. Activation precision was fixed at 4 bits for all methods. Validation was based on test accuracy (%). An ablation study analyzed the impact of FedMPQ's subroutines.",
        "limitations": "FedMPQ exhibits performance degradation when the number of local epochs is too low (e.g., 1), likely due to underfitting of the quantized models. The performance gap compared to the 8-bit fixed-precision baseline (FPQ8) tends to widen as the number of participating clients in the system increases, suggesting potential scalability challenges for maintaining peak performance in very large FL systems. Additionally, the method's effectiveness is sensitive to the selection of regularization hyperparameters (λ) and the bit-pruning threshold (ϵ), requiring careful tuning to achieve satisfactory results. The current approach fixes activation precision at 4 bits, limiting the mixed-precision application solely to model weights.",
        "future_research_directions": "Future research could explore methods to enhance FedMPQ's scalability and performance in scenarios involving a significantly larger number of clients. Investigating mixed-precision quantization for activations, in addition to weights, could lead to further model compression and resource optimization. Developing adaptive strategies for dynamically tuning hyperparameters such as the regularization weight (λ) and the bit-pruning threshold (ϵ) could reduce sensitivity to manual configuration and improve the method's robustness across diverse FL settings."
      }
    },
    {
      "title": "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices",
      "abstract": "Training deep neural networks on large datasets can often be accelerated by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-Reduce. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real-world applications, such as federated learning and cloud-based distributed training, operate on unreliable devices with unstable network bandwidth. As a result, these applications are restricted to using parameter servers or gossip-based averaging protocols. In this work, we lift that restriction by proposing Moshpit All-Reduce - an iterative averaging protocol that exponentially converges to the global average. We demonstrate the efficiency of our protocol for distributed optimization with strong theoretical guarantees. The experiments show 1.3x speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5x speedup when training ALBERT-large from scratch using preemptible compute nodes.",
      "full_text": "Moshpit SGD: Communication-Efﬁcient Decentralized Training on Heterogeneous Unreliable Devices Max Ryabinin∗ Yandex, Russia HSE University, Russia Eduard Gorbunov∗ MIPT, Russia HSE University, Russia Yandex, Russia Vsevolod Plokhotnyuk Yandex, Russia HSE University, Russia Gennady Pekhimenko University of Toronto, Canada Vector Institute, Canada Abstract Training deep neural networks on large datasets can often be accelerated by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-Reduce. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real- world applications, such as federated learning and cloud-based distributed training, operate on unreliable devices with unstable network bandwidth. As a result, these applications are restricted to using parameter servers or gossip-based averaging protocols. In this work, we lift that restriction by proposing Moshpit All-Reduce — an iterative averaging protocol that exponentially converges to the global average. We demonstrate the efﬁciency of our protocol for distributed optimization with strong theoretical guarantees. The experiments show 1.3x speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5x speedup when training ALBERT-large on preemptible compute nodes. 1 Introduction Many recent inﬂuential discoveries in deep learning were enabled by the trend of scaling model and dataset size. Over the last decade, computer vision has grown from training models with 60 million parameters [1] on 1.3 million images [2] to 15 times more parameters [3] and 200 times more training data [4]. In natural language processing, the state-of-the-art language models [ 5] with 175 billion parameters are trained on over 570GB of texts, and even this does not saturate the model quality [6]. Training these large models can take years even with a top-of-the-line GPU server [7]. As a result, researchers and practitioners often have to run distributed training with multiple machines [8]. The dominant approach to distributed deep learning is data-parallel training [9], where each worker processes a fraction of the training batch and then exchanges its gradients with peers. If done naïvely, the gradient exchange step can overload the network as the number of workers increases. To combat this issue, modern distributed training algorithms take advantage of communication-efﬁcient protocols, such as all-reduce [10]. These protocols allow workers to collectively compute the global average gradient with a constant communication overhead, regardless of the total number of peers. ∗Equal contribution. Correspondence to mryabinin0@gmail.com. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2103.03239v4  [cs.LG]  11 Jan 2022However, this efﬁciency makes the protocols more fragile: if any single participant fails or takes too long to process its batch, all other nodes are stalled. Therefore, scaling all-reduce protocols beyond a couple of servers requires specialized infrastructure with dedicated ultra-high bandwidth networking [8]. This kind of infrastructure is notoriously expensive compared to regular GPU servers or preemptible cloud VMs (see Appendix A for details). Hence, it is tempting to consider distributed training on cheap unreliable instances as a cost-efﬁcient alternative. A similar scenario arises in federated learning [11], where a single model is trained on heterogeneous devices due to privacy concerns. In both scenarios, workers use a shared network, where both latency and bandwidth can vary drastically due to interference from other users [ 12]. Furthermore, compute nodes are also subject to failure (or preemption) caused by factors beyond the protocol’s control. Running large-scale distributed training in these circumstances requires fault- and latency-tolerant algorithms [ 14, 15]. Most of these algorithms replace all-reduce averaging with gossip: each participant periodically downloads the latest parameters from their neighbors in a sparsely connected communication graph and averages the results. The updates gradually propagate through the graph over multiple rounds of averaging. However, the communication required to perform gossip grows linearly with the number of neighbors. Hence, when scaling to hundreds of peers, decentralized SGD has to keep the communication graph sparse, slowing down the convergence. In this work, we propose an alternative approach. Instead of relying on a predeﬁned communica- tion graph, participants dynamically organize themselves into groups using a fully decentralized matchmaking algorithm called Moshpit All-Reduce. This strategy allows us to use communication- efﬁcient all-reduce protocols that signiﬁcantly reduce the network load compared to gossip-based averaging, while still being able to operate in unreliable hardware and network conditions. Our contributions can be summarized as follows: • We proposeMoshpit All-Reduce — a novel decentralized averaging protocol for large-scale training with unreliable communication-constrained devices. According to our analysis, this method has exponential convergence rate independent of network topology and size. • Armed with this averaging protocol, we develop Moshpit SGD for distributed optimization. We derive convergence rates for this algorithm and establish its equivalence to Centralized (Local) SGD in terms of iteration complexity under realistic assumptions. • Our experiments demonstrate that Moshpit All-Reduce is signiﬁcantly more efﬁcient under network latency in realistic conditions. In particular, we train ResNet-50 on ImageNet to 75% accuracy 1.3 times faster than existing decentralized training algorithms and pretrain ALBERT-large 1.5 times faster on preemptible cloud VMs.2 2 Related Work 2.1 Data parallel training The most popular way to accelerate neural network training with multiple devices is data-parallel training [9, 16, 17]. On each optimization step, this strategy splits the training batch among partici- pants. Each participant then runs forward and backward passes to obtain gradients of the objective function on their part of the training batch. After that, we can aggregate the gradients from workers and perform an optimization step. There are two main strategies for this aggregation. Historically, the ﬁrst solution to gradient aggregation was to use Parameter Server (PS) [18]: a separate process or a dedicated server that keeps track of model parameters and optimizer statistics. After each round, the PS accumulates the gradients from each worker and updates the model parameters using SGD or any other optimizer, such as Adam [ 19]. Finally, the server distributes the updated model parameters to workers. This strategy is robust and easy to implement, but it requires the server to regularly download full model gradients from every single worker. As a result, the parameter server can quickly become a bottleneck for large-scale training [20]. Since the original PS, researchers have proposed several modiﬁcations that reduce the communication load: accumulating multiple batches [ 22], compression [23, 24], server sharding [25, 26]. A more detailed overview is given in Appendix B. 2Implementation and code of experiments are at github.com/yandex-research/moshpit-sgd. 2In turn, many practical distributed training systems have instead switched to averaging with All- Reduce [ 16, 27, 28, 17]. This name refers to a collection of protocols originally developed for HPC applications. Workers can follow these protocols to collectively compute the average3 gradient more efﬁciently than with a central server. 2.2 Communication-efﬁcient All-Reduce There are several all-reduce protocols optimized for different network topologies. The simplest one is known as Butterﬂy All-Reduce [10]. Each of N participants splits its local vector into N chunks. Then, i-th worker aggregates i-th chunk of data from all peers and sends back the averaged chunk. Worker 1 Split Scatter Reduce All-Gather Worker 2 Worker 3 x1 x2 x3 aavg bavg cavg a1 b1 c1 a2 b2 c2 a3 b3 c3 aavg bavg cavg aavg bavg cavg Σ Σ Σ aavg bavg cavg Figure 1: A schematic illustration of Butterﬂy All-Reduce. As long as the vector size sis greater than N, this protocol uses O ( s×N−1 N ) total bandwidth on each worker. However, it requires all-to-all communication, which is not always practical for the HPC infrastructure due to network contention [10]. As a result, real-world systems typically use Ring or Tree All-Reduce, where each worker only communicates with a small subset of its peers. These protocols enable highly efﬁcient and scalable averaging with O(1) or O(log N) total commu- nication per worker, but they also share a common drawback: they cannot tolerate node failures or network instability. If any single participant fails to execute its part or takes long to respond, this paralyzes all other workers. 2.3 Distributed training in unstable conditions Some distributed training applications must deal with unstable network bandwidth and/or unreliable workers. This issue is most prevalent in federated learning [11, 29, 30]. When dealing with privacy- sensitive data distributed across multiple actors, such as hospital servers [31, 32] or mobile phones [33, 34], one must train the model using whichever hardware and network available to those actors. Another important motivational factor is cost: HPC-grade infrastructure can be prohibitively expen- sive, pushing researchers and practitioners towards commodity servers or preemptible cloud VMs that are signiﬁcantly cheaper (see Appendix A). Another solution is to use volunteer computing [35, 36] with abundant, but even less reliable, compute resources. Training under these conditions requires specialized strategies. At a small scale, one can deploy one or a few reliable parameter servers to aggregate the updates from workers. This strategy can tolerate individual node failures [37], but scales poorly due to the reasons discussed in Section 2.1. 2.4 Decentralized training If there are too many participants for PS, it can be advantageous to use decentralized SGD via gossip-based averaging [38, 39, 14]. In this scenario, participants form a sparse graph: each worker periodically downloads parameters from its neighbors and mixes them with local parameters. In essence, gossip-based averaging removes the communication bottlenecks of PS at the cost of using different local parameters on each peer. That said, gossip-based optimization algorithms can match, and sometimes even outperform, their centralized counterparts in terms of training speed [40, 41, 42, 14, 43]. However, the convergence properties of gossip averaging and gossip- based optimization methods signiﬁcantly depend on the communication graph through the spectral properties of the mixing matrix [44, 42] or the Laplacian matrix of the network [45, 46]. 3All-Reduce works with any commutative associative operation, such as min, max, or product. 3Consequently, as the number of peers increases, gossip-based averaging has to either increase the number of neighbors (hence more communication) or accept slower convergence speed. Because of this, gossip is less communication-efﬁcient than all-reduce algorithms reviewed in Section 2.2. However, gossip-based algorithms are more robust to changes, which makes them applicable to time-varying networks [47, 48, 49, 50] and federated learning [51, 52, 53]. 3 Moshpit SGD Large-scale training with unreliable participants requires a protocol that is both communication- efﬁcient and fault-tolerant. Unfortunately, existing methods have only provide one of these properties. To better address our conditions, we propose Moshpit All-Reduce — a fully decentralized averaging protocol that combines the efﬁciency of all-reduce and the fault tolerance of gossip-based averaging. The rest of this section is organized as follows: • Section 3.1 describes the protocol and proves its correctness and communication efﬁciency; • Section 3.2 provides the analysis of the protocol and proves exponential convergence rate for averaging and the rate matching the one of centralized Local-SGD for optimization; • Section 3.3 contains implementation details for training with heterogeneous compute nodes. 3.1 Moshpit All-Reduce The core idea of Moshpit All-Reduce is that workers perform averaging in small independent groups. That way, a single failed participant would only affect his current group. In turn, the composition of each group should be chosen dynamically to converge in the least number of steps. Ideally, if there are 9 peers with local parameters θ, we can average them in 2 rounds, as demonstrated in Figure 2: First round Second round A  θA θA θA θB θB θB θC θC θC Group C Group B Group Average θ in groups θ3θ1 θ2 θ6θ4 θ5 θ9θ8θ7 Figure 2: Example averaging order for 9 peers in 2 rounds. On each round, peers are split into 3 groups that run All-Reduce in parallel. Algorithm 1 Moshpit All-Reduce (for i-th peer) Input: parameters {θj}N j=1, number of peers N, d, M, number of iterations T, peer index i θ0 i := θi C0 i := get_initial_index(i) for t∈1 ...T do DHT[Ct−1 i ,t].add(addressi) Matchmaking() // wait for peers to assemble peerst := DHT.get([Ct−1 i ,t]) θt i,ct i := AllReduce(θt−1 i ,peerst) Ct i := (Ct−1 i [1:],ct i) // same as eq. (1) end for Return θT i To achieve this in a decentralized system, we use Distributed Hash Tables (DHT) — a decentralized key-value storage; Appendix B contains its more detailed description. On each averaging round: • Each worker computes its group key Ci; • Workers add their network addresses to the DHT key corresponding to Ci; • Each worker can now fetch a full list of peers that have the same Ci and run All-Reduce with those peers. Unfortunately, the averaging structure from Figure 2 is impossible to maintain when participants are constantly joining, leaving, and failing. However, we can achieve equivalent results without global structure using a simple rule: if two peers were in the same group in round t, they must choose different groups in roundt+1. A natural way to enforce this rule is to take advantage of the chunk indices from Butterﬂy All-Reduce (see Figure 1). Recall that each worker accumulates a unique chunk of parameters deﬁned by an index ci. By setting Ci := ci, we can guarantee that any workers that were in the same group at a round twill have different group indices in round t+1. 4This averaging scheme can be generalized to more than two dimensions in order to ﬁt a larger number of peers or reduce the group size. For a d-dimensional hypercube, nodes should ﬁnd groups of peers that they have not communicated with during d−1 previous rounds. To that end, we deﬁne Ci as tuples containing chunk indices from d−1 previous rounds (tdenotes the communication round): Ct i := (ct−d+1 i ,ct−d+2 i ,...,c t i). (1) The above intuition can be formalized with Algorithm 1. Here, N peers form a virtual d-dimensional grid with M peers per row and average their parameters θi over T rounds. DHT[·] is a shortcut for using the DHT to add or retrieve values for a given key. TheMatchmaking step corresponds to the decentralized matchmaking procedure that organizes active workers with the same index into groups, described in detail in Appendix E. In turn, AllReduce denotes running all-reduce to compute the average θin a given group. The get_initial_index function takes the peer index iand returns d−1 integers in range [0,M) such that the size of initial groups does not exceed M. This way, the groups formed on subsequent rounds will also have at most M participants. One possible strategy is: get_initial_index(i) = ( ⌊i/Md−1⌋mod M ) j∈{1,...,d } (2) If N=Md and there are no node/network failures, Algorithm 1 is equivalent to Torus All-Reduce [54], achieving the exact average after drounds of communication (see Appendix C.1). However, our typical use case is far from this perfect scenario; for example, some groups can have less than M members. Furthermore, a peer might fail during all-reduce, causing its groupmates to skip a round of averaging. Still, Moshpit All-Reduce is applicable even in these conditions: Theorem 3.1 (Correctness). If all workers have a non-zero probability of successfully running a communication round and the order of peerst is random, then all local vectors θt i converge to the global average with probability 1: ∀i, ⏐⏐⏐ ⏐⏐⏐θt i − 1 N ∑ i θ0 i ⏐⏐⏐ ⏐⏐⏐ 2 2 −−−→ t→∞ 0. (3) Proof (sketch, complete in Appendix C.2). Running all-reduce with a subset of peers preserves the invariant 1 N ∑ iθt i = 1 N ∑ iθt−1 i and reduces the deviation of θt i from the overall average. Complexity. The matchmaking protocol is implemented over Kademlia DHT [55], meaning that each read and write operation needs at most O(log N) requests and O(M) bandwidth to load peerst. After the matchmaking is over, each group runs a single all-reduce round to compute the average. In principle, Moshpit Averaging can use any general-purpose all-reduce protocol. We opted for a butterﬂy-like version (Figure 1), as it is simpler than Ring All-Reduce while still being communication- efﬁcient. The communication complexity of this algorithm is O ( max(s,M) ×M−1 M ) , where sis the size of vector θ. Thus, the total time complexity of Algorithm 1 becomes: O ( T × [ log2 N + M + max(s,M) ×M −1 M ]) . (4) This compares favorably to gossip, where network load grows linearly with the number of neighbors. 3.2 Convergence analysis 3.2.1 Mixing properties of Moshpit Averaging As stated in the previous section, Moshpit All-Reduce computes the exact average when N = Md, which cannot be guaranteed in practice. Therefore, additional analysis is needed to establish how quickly Moshpit Averaging approximates the actual average of N vectors stored on peers. In the following theorem, we provide such analysis for a simpliﬁed version of Moshpit Averaging. One can ﬁnd the full proof in Appendix C.3. Theorem 3.2. Consider a modiﬁcation of Moshpit All-Reduce that works as follows: at each iteration k ≥1, 1) peers are randomly split in r disjoint groups of sizes Mk 1 ,...,M k r in such a way that∑r i=1 Mk i = N and Mk i ≥1 for all i= 1,...,r and 2) peers from each group compute their group average via All-Reduce. Let θ1,...,θ N be the input vectors of this procedure and θT 1 ,...,θ T N be the outputs after T iterations. Also, let θ= 1 N ∑N i=1 θi Then, E [ 1 N N∑ i=1 ∥θT i −θ∥2 ] = (r−1 N + r N2 )T 1 N N∑ i=1 ∥θi −θ∥2. (5) 5Algorithm 2 Moshpit SGD 1: Input: starting point θ0, learning rate γ >0, communication period τ ≥1 2: for k= 0,1,... do 3: for each peer i∈Pk+1 in parallel do 4: Compute the stochastic gradient gk i at the current point θk i 5: if k+ 1 mod τ = 0 then 6: θk+1 i = Moshpit All-Reducej∈Pk+1 (θk j −γgk j) for i-th peer (Algorithm 1) 7: else 8: θk+1 i = θk i −γgk i 9: end if 10: end for 11: end for In particular, this result implies that even if workers are randomly split into pairs at each iteration, the simpliﬁed version of Moshpit Averaging makes the average distortion (the left-hand side of Equation 5) less than ε in expectation after O(log(1/ε)) iterations. That is, this algorithm ﬁnds ε-accurate average on each node with the rate that does not depend on the spectral properties of the communication graph like gossip and its variants (see Section 2.4 and Appendix B.1). Since Moshpit Averaging prevents two peers from participating in the same groups during successive iterations, the actual algorithm should ﬁnd ε-accurate averages on participating peers even faster than Equation 5 predicts. Moreover, in Appendix C.3 we explain how this result can be generalized to the case when {Mk i }N i=1 and rdepends on kor even is random. In Appendix C.4, we also provide the guarantees measuring how fast Algorithm 1 reduces the variance when averaging random vectors. 3.2.2 Moshpit SGD We consider a classical distributed optimization problem min θ∈Rn { f(θ) = 1 N N∑ i=1 fi(θ) } , (6) where N is the number of workers and worker ihas access only to the function fi. We propose a new algorithm called Moshpit SGD to solve this problem (see Algorithm 2). In this algorithm, workers perform independent local SGD steps and periodically synchronize their parameters θk i with other peers using Moshpit All-Reduce. Moreover, we deﬁne the indices of participating nodes at iteration kas Pk+1 (P0 = {1,...,N }) allowing peers to vanish. First of all, we list the key assumptions that we use in the convergence analysis of Moshpit SGD. Assumption 3.1 (Bounded variance). We assume that for all k ≥0 and i = 1,...,N stochastic gradients gk i satisfy E [ gk i |θk i ] = ∇fi(θk i) and E [ ∥gk i −∇fi(θk i)∥2 |θk i ] ≤ σ2. (7) This assumption is classical in the stochastic optimization literature [ 56, 57]. We notice that our analysis can be generalized to the settings when the stochastic gradients satisfy less restrictive assumptions such as expected smoothness [58] or have more sophisticated structure similar to [59] using the theoretical framework from [60]. The following assumption controls the averaging properties and the effect of the peers’ vanishing. Assumption 3.2 (Averaging quality & peers’ vanishing). We assume that the vanishing of peers does not change the global average of the iterates of Moshpit SGD too much, i.e.,Pk+1 ⊆Pk and |Pk|≥ Nmin for all k≥0, |Paτ|≤ 2|Pa(τ+1)|for all non-negative integers a≥0, and there exist such ˜θ∈Rn and a sequence of non-negative numbers {∆k pv}k≥0 that ∀k≥0 E [ ⟨θk+1 −ˆθk+1,θk+1 + ˆθk+1 −2˜θ⟩ ] ≤∆k pv,f convex; (8) E [ ⟨∇f(θk),θk+1 −ˆθk+1⟩+ L∥ˆθk+1 −θk+1∥2 ] ≤∆k pv,f non-convex, L-smooth, (Def. D.1) (9) where Nk = |Pk|, θk+1 = 1 Nk+1 ∑ i∈Pk+1 θk+1 i , and ˆθk+1 = 1 Nk ∑ i∈Pk (θk i −γgk i) for k≥0. 6Moreover, we assume that for someδaq ≥0 and for all non-negative integers a≥0, E [ 1 Naτ ∑ i∈Paτ ∥θaτ i −θaτ∥2 ] ≤ γ2δ2 aq. (10) If Pk = Pk+1 = {1,...,N }for all k≥0, i.e., peers do not vanish, thenθk = ˆθk and properties (8, 9) hold with ∆k pv ≡0 for all k≥0. Moreover, according to the mixing properties of Moshpit Averaging established in Theorem 3.2, inequality 10 holds after O(log (1/γ2δ2 aq)) iterations of Algorithm 1. Therefore, the assumption above is natural and well-motivated. Under these assumptions, we derive the convergence rates both for convex and non-convex problems. The full statements and complete proofs are deferred to Appendix D. Theorem 3.3 (Convex case). Let f1 = ... = fN = f, function f be µ-strongly convex (Def. D.2) and L-smooth (see Def. D.1), and Assumptions 3.1 and 3.2 hold with∆k pv = δpv,1γµE[∥θk−θ∗∥2] + γ2δ2 pv,2 and ˜θ= θ∗, where θ∗∈argminθ∈Rn f(θ) and δpv,1 ∈[0,1), δpv,2 ≥0. Then there exists a choice of γsuch that E [ f(θ K ) −f(θ∗) ] ≤εafter Kiterations of Moshpit SGD, where Kequals ˜O ( L (1−δpv,1)µ+ δ2 pv,2+σ2 /Nmin (1 −δpv,1)µε + √ L((τ−1)σ2+δ2aq) (1−δpv,1)2µ2ε ) , µ>0; (11) O  LR2 0 ε + R2 0(δ2 pv,2+σ2 /Nmin) ε2 + R2 0 √ L( (τ−1)σ2+δ2aq) ε 3/2  , µ= 0, (12) where θ K = 1 WK K∑ k=0 1 Nk ∑ i∈Pk wkθk i, wk = (1 −γµ)−(k+1), WK = ∑K k=0 wk, R0 = ∥θ0 −θ∗∥and ˜O(·) hides constant and log(1/ε) factors. That is, if δpv,1 ≤1/2, Nmin = Ω(N), δ2 pv,2 = O(σ2 /Nmin), and δ2 aq = O((τ −1)σ2), then Moshpit SGD has the same iteration complexity as Local-SGD in the homogeneous case [61, 62]. However, the averaging steps of Moshpit SGD are much faster than those of the parameter-server architecture when the number of peers is large. Also, unlike the state-of-the-art convergence guarantees for Decentralized Local-SGD [63], our bounds do not depend on the spectral properties of the communication graph (see Appendix B.1 for the details). Theorem 3.4 (Non-convex case). Let f1 = ... = fN = f, function f be L-smooth and bounded from below by f∗, and Assumptions 3.1 and 3.2 hold with ∆k pv = δpv,1γE[∥∇f(θk)∥2] + Lγ2δ2 pv,2, δpv,1 ∈[0,1/2), δpv,2 ≥0. Then there exists such choice of γthat E [ ∥∇f(θK rand)∥2] ≤ε2 after K iterations of Moshpit SGD, where Kequals O ( L∆0 ( 1−2δpv,1)2ε2 [ 1+τ √ 1−2δpv,1+ δ2 pv,2+σ2/Nmin ε2 + √ (1−2δpv,1)(δ2aq+(τ−1)σ2) ε ]) , ∆0 = f(θ0) −f(θ∗) and θK rand is chosen uniformly from {θ0,θ1,...,θ K−1}deﬁned in As. 3.2. Again, if δpv,1 ≤1/3, Nmin = Ω(N), δ2 pv,2 = O(σ2 /Nmin), and δ2 aq = O((τ −1)σ2), then the above theorem recovers the state-of-the-art results in the non-convex case for Local-SGD [64, 63]. 3.3 Implementation details Training on heterogeneous unreliable hardware also poses a number of engineering challenges. The most obvious one is that the system must be able to recover from node failures. To address this challenge, we use a fully decentralized infrastructure where all information is replicated in a Dis- tributed Hash Table; see Appendix B.5 for details. When a new worker joins midway through training, it can download the latest model parameters and metadata from any other peer (see Appendix F). Another challenge arises when devices in a group have uneven network bandwidth. In that case, we dynamically adjust the communication load of each peer to avoid being bottlenecked. More information on this procedure can be found in Appendix G. 74 Experiments In this section, we conduct empirical evaluation of the proposed averaging protocol and its corre- sponding optimization algorithm. First, we check the theoretical properties of Moshpit All-Reduce in a controlled setup (Section 4.1). Then, we compare Moshpit SGD with other distributed methods on practical tasks of image classiﬁcation and masked language model pretraining (Sections 4.2 and 4.3). 4.1 Decentralized averaging In this series of experiments, we aim to empirically verify the convergence and fault tolerance properties proven in Section 3.2. To measure this in a controlled setting, we create peers with parameters that are scalar values drawn from the standard Gaussian distribution. We study the convergence of different distributed methods with respect to the number of workers N and their individual failure rate for a single iteration of averaging p(failed peers return in the next round). We compare Moshpit Averaging with the following algorithms from prior work: All-Reduce (with restarts in case of node failures), Gossip, PushSum (equivalent to the method described in [ 15]). Also, we provide the results of averaging in random groups as a simpler version of our approach. However, the implementation of group averaging maintains approximately the same group size across all iterations: this property might be hard to achieve in a decentralized setting, and as a result, the estimate of this method’s performance should be considered highly optimistic. We report the average squared difference between the worker parameters and the actual average of all values; the results are averaged across 100 restarts from different random initializations. We compare the convergence for 512–1024 peers and consider failure probabilities ranging from 0 to 0.01. For Moshpit Averaging and random group averaging, we use groups of size 32, which corresponds to M = 32 and d= 2 for Algorithm 1. 0 2 4 6 8 10 Iterations 10 13 10 9 10 5 10 1 Mean squared error N=1024, p=0 All-Reduce Gossip PushSum Random groups Moshpit Averaging 0 2 4 6 8 10 Iterations N=1024, p=0.005 All-Reduce Gossip PushSum Random groups Moshpit Averaging 0 2 4 6 8 10 Iterations N=768, p=0.005 All-Reduce Gossip PushSum Random groups Moshpit Averaging Figure 3: Convergence of averaging algorithms in different conﬁgurations. Figure 3 displays the results of experiments for several combinations ofN and p; the complete results with additional grid conﬁgurations are available in Appendix I. We make several key observations: 1. When the failure rate of each peer is zero, standard All-Reduce predictably computes the average faster than all other methods. However, as soon as preaches a value of at least 0.005, the number of retries needed for the success becomes prohibitively high. 2. Previous decentralized averaging methods, such as Gossip or PushSum, require signiﬁcantly more iterations for convergence to the global average than Moshpit All-Reduce, likely due to the structure of their communication graphs. 3. As discussed in Section 3.1, when the total number of peers is equal to the grid capacity and there are no failures, Moshpit All-Reduce matches the result of regular All-Reduce with the number of steps equal to the number of grid dimensions (2 in this case). 4. Averaging in random groups can perform comparably to Moshpit Averaging when the number of peers is less than half of the grid capacity. The reason for this behavior is that when the workers do not fully occupy the grid, the group sizes are no longer guaranteed to be equal across groups and across iterations. In the worst case, there can be groups of only one peer for certain grid coordinates, which may signiﬁcantly affect the convergence. However, as the grid utilization grows, Moshpit Averaging starts to outperform random group averaging. Moreover, even if we use 512 peers, arranging them in a proper 8x8x8 grid leads to faster convergence. 80h 4h 8h 12h 16h 20h 24h 28h 32h Time (hours) 0% 25% 50% 75%Top-1 validation accuracy 26.818.517.112.89.8 AR-SGD, homog. AD-PSGD, homog. SGP, homog. Moshpit SGD, homog. AD-PSGD, heterog. SGP, heterog. Moshpit SGD, heterog. 0 15 30 45 60 75 90 105 120 135 150 Epochs 0% 25% 50% 75%Top-1 validation accuracy AR-SGD, homog. AD-PSGD, homog. SGP, homog. Moshpit SGD, homog. AD-PSGD, heterog. SGP, heterog. Moshpit SGD, heterog. 0h 30h 60h 90h 120h 150h 180h Time (hours) 2 4 6 8 10Training loss AR-SGD, homog. Moshpit SGD, heterog. Figure 4: (Left, Middle) ResNet-50 top-1 validation accuracy for ImageNet as a function of training time (left) and epochs (middle). (Right) Full training objective (MLM + SOP) of ALBERT-large on BookCorpus as a function of training time. 4.2 ImageNet training Here, we evaluate the performance of Moshpit SGD in distributed training. More speciﬁcally, we train ResNet-50 [65] on the ILSVRC [2] dataset, following the training protocol of [16]. Trainers use SGD with Nesterov momentum with a batch size of 256 and 32-bit precision regardless of the GPU type4. We evaluate the following training strategies: • All-Reduce SGD (AR-SGD) — traditional distributed training with all-reduce gradient averaging; • Asynchronous Decentralized Parallel SGD (AD-PSGD) — parallel SGD that runs gossip com- munication in a cycle: each worker averages parameters with 2 neighbors [66]. Communication rounds are overlapped with computation; • Stochastic Gradient Push (SGP) — a more advanced algorithm with an exponential communica- tion graph and push-based communication [15]; • Moshpit SGD — similar to SGP, but with 1 round of Moshpit Averaging instead of PushSum. We report top-1 validation accuracy as a function of training time in two experimental setups: • Homogeneous: 16 servers with a single Tesla V100-PCIe GPU, 6 CPU cores, and 64GB RAM. • Heterogeneous: a total of 81 GPUs (V100, 1080Ti, and P40) across 64 servers and workstations.5 All servers and workstations communicate over the network with 1Gb/s Ethernet (non-dedicated symmetric bandwidth). The machines are located in two data centers and one ofﬁce within 300 km of one another. The communication latency is 1–6ms depending on the location. To simulate shared usage, at the beginning of each communication round we inject additional latency sampled from the exponential distribution [67] with the mean of 100ms. For Moshpit SGD, we use a two-dimensional “grid” with 4 and 8 groups for homogeneous and heterogeneous setups respectively. For AD-PSGD, we attempt to compensate for slow convergence by training for 60 more epochs without changing the learning rate schedule. Finally, we only report AR-SGD in the ﬁrst setup, as it is unsuitable for heterogeneous hardware. The results in Figure 4 (Left) demonstrate that the two most efﬁcient strategies for our setting are Moshpit SGD and SGP. In the homogeneous setup, Moshpit is only slightly more efﬁcient than SGP, likely due to higher efﬁciency of all-reduce. This advantage increases to over 30% for the heterogeneous setup with 64 servers. In turn, AR-SGD demonstrates the best performance per iteration, but its training time is by far the longest due to network latency (1.5×of Moshpit SGD). Finally, AD-PSGD predictably shows the best throughput (time per epoch), but achieves lower accuracy even after training for 150 epochs. We report results for smaller setups in Appendix J. 4.3 Masked Language Model training Finally, we evaluate Moshpit All-Reduce training performance in the wild with preemptible cloud instances. For this experiment, we perform one of the most resource-demanding tasks in modern deep learning — unsupervised pretraining of Transformers [68, 69, 70, 5]. We opt for the ALBERT model [71] to make better use of communication-constrained devices. This model has fewer trainable parameters due to layer-wise weight sharing. 4For GPUs that cannot ﬁt this into memory, we accumulate gradients over 2 batches of 128 examples. 5We provide a detailed conﬁguration in Appendix H. 9Speciﬁcally, we train ALBERT-large (18M parameters) on the BookCorpus [72] dataset, following the training setup from the original paper. We minimize the masked language modeling loss (MLM) along with the sentence order prediction loss (SOP) using the LAMB optimizer [17] with a global batch size of 4096 and sequence length 512. We measure convergence in terms of full training loss [73, 74]. Similarly to Section 4.2, we use two training setups: • Homogeneous: a single cloud instance with 8 Tesla V100-PCIe GPUs and 56 vCPUs; • Heterogeneous: a total of 66 preemptible GPUs, 32 of which are cloud T4, and the remaining 34 are various devices rented on a public marketplace. Despite the fact that the latter setup has almost 3×more raw compute6, its hourly rent costs less than the homogeneous setup due to relying on preemptible instances7. This instance type is much cheaper than regular cloud instances, but it can be interrupted at any time. As a side-effect, the participants in heterogeneous setup are also spread across 3 continents with uneven network bandwidth, ranging from 100Mb/s to 1500Mb/s per worker. These limitations make it impractical to deploy conventional all-reduce protocols. By contrast, the fully decentralized nature of Moshpit SGD allows it to operate on unreliable nodes. In this setup, the participants accumulate gradients over multiple local batches and use DHT to track the global batch size. Once the swarm collectively accumulates gradients over 4096 training samples, it runs 2 rounds of Moshpit All-Reduce with M=8 and d=2. Unfortunately, training with simple parameter averaging does not converge, likely due to diverging LAMB statistics. To mitigate this issue, workers recover “pseudo-gradients” [76, 77] after averaging to update the optimizer statistics. Figure 4 (right) demonstrates that Moshpit SGD with a fully preemptible ﬂeet of machines trains 1.5 times faster than the traditional data-parallel setup. The ﬁnal loss achieved by two training strategies is the same within the margin of error. A closer investigation reveals that this speedup is entirely explained by the reduced iteration time. An interesting observation is that the iteration time of Moshpit SGD varies between 10–22 seconds, while AR-SGD consistently spends 25s per step. This can be explained by natural variation in the preemptible ﬂeet size: there were 30–66 active participants depending on the resource availability. 5 Conclusion and future work In this work, we propose Moshpit All-Reduce, a decentralized averaging protocol intended for distributed optimization in unstable and network-constrained environments. It has favorable theoreti- cal properties when compared to gossip-based approaches and achieves considerable speedups in distributed training for image classiﬁcation and masked language modeling. Our approach was primarily designed for cloud-based training and federated learning, as well as for distributed training on unreliable instances; future work might explore additional settings, such as collaborative training of neural networks. Another potential research direction is to study the interactions of Moshpit All-Reduce with other methods that improve communication efﬁciency of distributed optimization, such as gradient compression. Finally, the idea of arranging All-Reduce nodes into groups can be improved to address speciﬁc issues that may arise in practice, such as the varying number of workers and their geographical distribution. Acknowledgements We would like to thank Anastasia Koloskova, Liudmila Prokhorenkova and Anton Osokin for helpful feedback and discussions. We are also grateful to the anonymous reviewers for their suggestions on improving the paper. Finally, we would like to thank Dmitry Afanasiev, Vladimir Aliev, Anand Jayarajan and Michael Solotky for their suggestions on the technical aspects of our study. This project was supported in part by the Canada Foundation for Innovation JELF grant, NSERC Discovery grant, AWS Machine Learning Research Award, and Facebook Faculty Research Award. The paper was also partially supported by by a grant for research centers in the ﬁeld of artiﬁcial intelligence, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identiﬁer 000000D730321P5Q0002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138. The computational resources for the experiments were provided by the Amazon Research Awards program and Yandex. 6Based on ofﬁcial performance benchmarks [75]. 7Please refer to Appendix H for full experimental setups. 10References [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012. [2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. [3] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020. [4] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017. [5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [6] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. [7] Chuan Li. Demystifying gpt-3 language model: A technical overview, 2020. \" https: //lambdalabs.com/blog/demystifying-gpt-3\". [8] Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim Hazelwood, Andrew Hock, Xinyuan Huang, Bill Jia, Daniel Kang, David Kanter, Naveen Kumar, Jeffery Liao, Guokai Ma, Deepak Narayanan, Tayo Ogun- tebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom St. John, Carole-Jean Wu, Lingjie Xu, Cliff Young, and Matei Zaharia. MLPerf Training Benchmark. In Proceedings of the 3rd Conference on Machine Learning and Systems (MLSys’20), 2020. [9] Leslie G Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1990. [10] Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J. Parallel Distrib. Comput., 69(2):117–124, February 2009. [11] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics, pages 1273–1282, 2017. [12] V . Persico, P. Marchetta, A. Botta, and A. Pescape. On network throughput variability in microsoft azure cloud. In 2015 IEEE Global Communications Conference (GLOBECOM), pages 1–6, 2015. [13] Valerio Persico, Pietro Marchetta, Alessio Botta, and Antonio Pescapè. Measuring network throughput in the cloud: The case of amazon ec2. Computer Networks, 93:408 – 422, 2015. Cloud Networking and Communications II. [14] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decen- tralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5330–5340, 2017. [15] Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for distributed deep learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, edi- tors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 344–353. PMLR, 09–15 Jun 2019. [16] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017. [17] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. 11[18] Mu Li. Scaling distributed machine learning with the parameter server. In Proceedings of the 2014 International Conference on Big Data Science and Computing, BigDataScience ’14, New York, NY , USA, 2014. Association for Computing Machinery. [19] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, 2015. [20] Salem Alqahtani and Murat Demirbas. Performance analysis and comparison of distributed machine learning systems, 2019. [21] Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim Verbelen, and Jan S. Rellermeyer. A survey on distributed machine learning. ACM Comput. Surv., 53(2), March 2020. [22] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 23, pages 2595–2603. Curran Associates, Inc., 2010. [23] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on Learning Representations, 2018. [24] Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and gossip algorithms with compressed communication. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3478–3487. PMLR, 09–15 Jun 2019. [25] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc Le, and Andrew Ng. Large scale distributed deep networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25, pages 1223–1231. Curran Associates, Inc., 2012. [26] Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, and Chuanxiong Guo. A uniﬁed architecture for accelerating distributed DNN training in heterogeneous gpu/cpu clusters. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 463–479. USENIX Association, November 2020. [27] Hiroaki Mikami, Hisahiro Suganuma, Pongsakorn U-chupala, Yoshiki Tanaka, and Yuichi Kageyama. Massively distributed sgd: Imagenet/resnet-50 training in a ﬂash, 2019. [28] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019. [29] Aaron Segal, Antonio Marcedone, Benjamin Kreuter, Daniel Ramage, H. Brendan McMahan, Karn Seth, K. A. Bonawitz, Sarvar Patel, and Vladimir Ivanov. Practical secure aggregation for privacy-preserving machine learning. In CCS, 2017. [30] K. A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé M Kiddon, Jakub Koneˇcný, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In SysML 2019, 2019. To appear. [31] Micah J. Sheller, Brandon Edwards, G. Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrotsou, Mikhail Milchenko, Weilin Xu, Daniel Marcus, Rivka R. Colen, and Spyridon Bakas. Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. Scientiﬁc Reports, 10(1):12598, Jul 2020. [32] Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maxim- ilian Baust, Yan Cheng, Sébastien Ourselin, M. Jorge Cardoso, and Andrew Feng. Privacy- Preserving Federated Brain Tumour Segmentation, pages 133–141. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioin- formatics). SPRINGER, January 2019. 10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019 held in conjunction with the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019 ; Conference date: 13-10-2019 Through 13-10-2019. 12[33] Andrew Hard, Chloé M Kiddon, Daniel Ramage, Francoise Beaufays, Hubert Eichner, Kan- ishka Rao, Rajiv Mathews, and Sean Augenstein. Federated learning for mobile keyboard prediction, 2018. [34] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Françoise Beaufays. Applied federated learning: Improving google keyboard query suggestions, 2018. [35] Ekasit Kijsipongse, Apivadee Piyatumrong, and Suriya U-ruekolan. A hybrid gpu cluster and volunteer computing platform for scalable deep learning. The Journal of Supercomputing, 04 2018. [36] Max Ryabinin and Anton Gusev. Towards crowdsourced training of large neural networks using decentralized mixture-of-experts. In Advances in Neural Information Processing Systems, 2020. [37] Aaron Harlap, Alexey Tumanov, Andrew Chung, Gregory R. Ganger, and Phillip B. Gibbons. Proteus: Agile ml elasticity through tiered reliability in dynamic resource markets. In Proceed- ings of the Twelfth European Conference on Computer Systems, EuroSys ’17, page 589–604, New York, NY , USA, 2017. Association for Computing Machinery. [38] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE transactions on information theory, 52(6):2508–2530, 2006. [39] John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984. [40] Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Tat Lee, and Laurent Massoulié. Op- timal algorithms for smooth and strongly convex distributed optimization in networks. In International Conference on Machine Learning, pages 3027–3036, 2017. [41] Kevin Scaman, Francis Bach, Sébastien Bubeck, Laurent Massoulié, and Yin Tat Lee. Optimal algorithms for non-smooth distributed optimization in networks. In Advances in Neural Information Processing Systems, pages 2740–2749, 2018. [42] Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Lee, and Laurent Massoulié. Optimal convergence rates for convex distributed optimization in networks. Journal of Machine Learning Research, 20:1–31, 2019. [43] Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for distributed deep learning. In International Conference on Machine Learning, pages 344–353. PMLR, 2019. [44] Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control Letters, 53(1):65–78, 2004. [45] Russell Merris. Laplacian matrices of graphs: a survey. Linear algebra and its applications, 197:143–176, 1994. [46] César A Uribe, Soomin Lee, Alexander Gasnikov, and Angelia Nedi´c. A dual approach for optimal algorithms in distributed optimization over networks. Optimization Methods and Software, pages 1–40, 2020. [47] Angelia Nedi ´c and Alex Olshevsky. Distributed optimization over time-varying directed graphs. IEEE Transactions on Automatic Control, 60(3):601–615, 2014. [48] Angelia Nedi´c and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on time-varying directed graphs. IEEE Transactions on Automatic Control, 61(12):3936–3947, 2016. [49] Angelia Nedi´c, Alex Olshevsky, and Michael G Rabbat. Network topology and communication- computation tradeoffs in decentralized optimization.Proceedings of the IEEE, 106(5):953–976, 2018. [50] Alexander Rogozin and Alexander Gasnikov. Projected gradient method for decentralized optimization over time-varying networks. arXiv preprint arXiv:1911.08527, 2019. [51] S Sundhar Ram, A Nedi´c, and Venugopal V Veeravalli. Asynchronous gossip algorithms for stochastic optimization. In Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference , pages 3581–3586. IEEE, 2009. 13[52] Feng Yan, Shreyas Sundaram, SVN Vishwanathan, and Yuan Qi. Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties. IEEE Transactions on Knowledge and Data Engineering, 25(11):2483–2493, 2012. [53] Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835–1854, 2016. [54] Paul Sack and William Gropp. Collective algorithms for multiported torus networks. ACM Trans. Parallel Comput., 1(2), February 2015. [55] Petar Maymounkov and David Mazieres. Kademlia: A peer-to-peer information system based on the xor metric. In International Workshop on Peer-to-Peer Systems, pages 53–65. Springer, 2002. [56] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochas- tic approximation approach to stochastic programming. SIAM Journal on optimization , 19(4):1574–1609, 2009. [57] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. [58] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. Sgd: General analysis and improved rates. In International Conference on Machine Learning, pages 5200–5209. PMLR, 2019. [59] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132–5143. PMLR, 2020. [60] Eduard Gorbunov, Filip Hanzely, and Peter Richtarik. Local sgd: Uniﬁed theory and new efﬁcient methods. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3556–3564. PMLR, 13–15 Apr 2021. [61] Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artiﬁcial Intelligence and Statistics, pages 4519–4529. PMLR, 2020. [62] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International Conference on Machine Learning, pages 10334–10343. PMLR, 2020. [63] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A uniﬁed theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pages 5381–5393. PMLR, 2020. [64] Xiang Li, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Communication efﬁcient decen- tralized training with multiple local updates. arXiv preprint arXiv:1910.09126, 5, 2019. [65] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2015. [66] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3043–3052. PMLR, 10–15 Jul 2018. [67] Andrei M Sukhov, MA Astrakhantseva, AK Pervitsky, SS Boldyrev, and AA Bukatov. Gener- ating a function for network delay. Journal of High Speed Networks, 22(4):321–333, 2016. [68] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. [69] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019. [70] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan- guage models are unsupervised multitask learners. 2019. 14[71] Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. [72] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015. [73] Jiahuang Lin, Xin Li, and Gennady Pekhimenko. Multi-node bert-pretraining: Cost-efﬁcient approach, 2020. [74] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efﬁcient sparsity, 2021. [75] NVIDIA. Nvidia data center deep learning product performance. \" https:// developer.nvidia.com/deep-learning-performance-training-inference \", ac- cessed at 2021.02.03. [76] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2021. [77] Xiangyi Chen, Xiaoyun Li, and Ping Li. Toward communication efﬁcient adaptive gradient method. In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference, FODS ’20, page 119–128, New York, NY , USA, 2020. Association for Computing Machinery. [78] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016. [79] David Aldous and James Allen Fill. Reversible markov chains and random walks on graphs, 2002. unﬁnished monograph, recompiled 2014, 2002. [80] Jinming Xu, Ye Tian, Ying Sun, and Gesualdo Scutari. Distributed algorithms for composite optimization: Uniﬁed and tight convergence analysis. arXiv preprint arXiv:2002.11534, 2020. [81] Alireza Fallah, Mert Gurbuzbalaban, Asu Ozdaglar, Umut Simsekli, and Lingjiong Zhu. Robust distributed accelerated stochastic gradient methods for multi-agent networks. arXiv preprint arXiv:1910.08701, 2019. [82] Dmitry Kovalev, Adil Salim, and Peter Richtárik. Optimal and practical algorithms for smooth and strongly convex decentralized optimization. Advances in Neural Information Processing Systems, 33, 2020. [83] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. Advances in neural information processing systems, 28:1756–1764, 2015. [84] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014. [85] Dan Alistarh, Demjan Grubic, Jerry Z Li, Ryota Tomioka, and Milan V ojnovic. Qsgd: communication-efﬁcient sgd via gradient quantization and encoding. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1707–1718, 2017. [86] Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International Conference on Machine Learning, pages 3329–3337. PMLR, 2017. [87] Ali Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan Alistarh, and Daniel M Roy. Nuqsgd: Provably communication-efﬁcient data-parallel sgd via nonuni- form quantization. Journal of Machine Learning Research, 22(114):1–43, 2021. [88] Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M Roy, and Ali Ramezani- Kebrya. Adaptive gradient quantization for data-parallel sgd. Advances in Neural Information Processing Systems, 33:3174–3185, 2020. [89] Samuel Horvath, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter Richtarik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988, 2019. 15[90] Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compression for distributed learning. arXiv preprint arXiv:2002.12410, 2020. [91] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: ternary gradients to reduce communication in distributed deep learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1508–1518, 2017. [92] Konstantin Mishchenko, Eduard Gorbunov, Martin Takáˇc, and Peter Richtárik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019. [93] Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik. Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019. [94] Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtarik. Acceleration for compressed gradient descent in distributed and federated optimization. In International Conference on Machine Learning, pages 5895–5904. PMLR, 2020. [95] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtarik. Linearly converg- ing error compensated sgd. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20889–20900. Curran Associates, Inc., 2020. [96] Constantin Philippenko and Aymeric Dieuleveut. Artemis: tight convergence guarantees for bidirectional compression in federated learning. arXiv preprint arXiv:2006.14591, 2020. [97] Zhize Li and Peter Richtárik. A uniﬁed analysis of stochastic gradient methods for nonconvex federated optimization. arXiv preprint arXiv:2006.07013, 2020. [98] Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated learning with compression: Uniﬁed analysis and sharp guarantees. arXiv preprint arXiv:2007.01154, 2020. [99] Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit S Dhillon. Improved convergence rates for non-convex federated learning with compression. arXiv preprint arXiv:2012.04061, 2020. [100] Eduard Gorbunov, Konstantin P. Burlachenko, Zhize Li, and Peter Richtarik. Marina: Faster non-convex distributed learning with compression. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, pages 3788–3798. PMLR, 18–24 Jul 2021. [101] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed sgd with memory. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 4452–4463, 2018. [102] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback ﬁxes signsgd and other gradient compression schemes. InInternational Conference on Machine Learning, pages 3252–3261. PMLR, 2019. [103] Xun Qian, Peter Richtárik, and Tong Zhang. Error compensated distributed sgd can be accelerated. arXiv preprint arXiv:2010.00091, 2020. [104] Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, and Ramtin Pedarsani. An exact quantized decentralized gradient descent algorithm. IEEE Transactions on Signal Processing, 67(19):4934–4947, 2019. [105] Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, and Sebastian Stich. A linearly convergent algorithm for decentralized optimization: Sending less bits for free! In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 4087–4095. PMLR, 13–15 Apr 2021. [106] Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning with arbitrary communication compression. In International Conference on Learning Representations, 2020. [107] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. arXiv preprint arXiv:1610.05492, 2016. 16[108] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Ar- jun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019. [109] Sebastian Urban Stich. Local SGD converges fast and communicates little. International Conference on Learning Representations (ICLR), page arXiv:1805.09767, 2019. [110] Tao Lin, Sebastian Urban Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches, use local SGD. ICLR, page arXiv:1808.07217, 2020. [111] Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous distributed learning. arXiv preprint arXiv:2006.04735, 2020. [112] Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural Information Processing Systems, 33, 2020. [113] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Dis- tributed SGD with quantization, sparsiﬁcation and local computations. In Advances in Neural Information Processing Systems, pages 14668–14679, 2019. [114] Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. arXiv preprint arXiv:2011.08474, 2020. [115] Mahmoud Assran, Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson, and Michael G Rabbat. Advances in asynchronous parallel and distributed optimization. Proceed- ings of the IEEE, 108(11):2013–2031, 2020. [116] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693–701, 2011. [117] Shen-Yi Zhao and Wu-Jun Li. Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016. [118] Rémi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Asaga: asynchronous parallel saga. In Artiﬁcial Intelligence and Statistics, pages 46–54. PMLR, 2017. [119] Zhimin Peng, Yangyang Xu, Ming Yan, and Wotao Yin. Arock: an algorithmic frame- work for asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 38(5):A2851–A2879, 2016. [120] Konstantin Mishchenko, Franck Iutzeler, Jérôme Malick, and Massih-Reza Amini. A delay- tolerant proximal-gradient algorithm for distributed learning. In International Conference on Machine Learning, pages 3587–3595. PMLR, 2018. [121] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. InProceedings of the 24th International Conference on Neural Information Processing Systems, pages 873– 881, 2011. [122] Hamid Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson. An asynchronous mini- batch algorithm for regularized stochastic optimization. IEEE Transactions on Automatic Control, 61(12):3740–3754, 2016. [123] Yossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. In Algorithmic Learning Theory , pages 111–132. PMLR, 2020. [124] Hari Balakrishnan, M Frans Kaashoek, David Karger, Robert Morris, and Ion Stoica. Looking up data in p2p systems. Communications of the ACM, 46(2):43–48, 2003. [125] Seymour Kaplan. Application of programs with maximin objective functions to problems of optimal resource allocation. Operations Research, 22(4):802–807, 1974. [126] Erling D. Andersen and Knud D. Andersen. The mosek interior point optimizer for linear programming: An implementation of the homogeneous algorithm. In Applied Optimization, pages 197–232. Springer US, 2000. [127] Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, and Gennady Pekhimenko. Priority-based parameter propagation for distributed dnn training. In A. Talwalkar, V . Smith, and M. Zaharia, editors, Proceedings of Machine Learning and Systems , volume 1, pages 132–145, 2019. 17Supplementary Material A GPU instance costs This section provides a brief cost analysis of typical deep learning compute resources both in the cloud and on-premises. For brevity, we limit this analysis to the popular GPUs available at the time of submission. Note that the exact costs will depend on a variety of factors such as the cloud provider, the region, electricity costs, and market ﬂuctuations. Therefore, we warn the reader to consider this analysis only as a rough estimate. Speciﬁcally, we estimate the compute costs for the occasional usage scenario: running a single set of experiments over several weeks or conducting infrequent experiments. This scenario covers most research scientists and small organizations. The most straightforward way to provision a GPU server in such a scenario is to rent it from a cloud provider (e.g., GCP or AWS) or a public marketplace (e.g., Vast.ai or Golem). While the exact server speciﬁcations vary from one provider to another, there are two broad categories of GPU machines: regular and preemptible. Regular instance types typically offer 1–8 GPUs per node with tight uptime guarantees (typically 99.99%) and a high-bandwidth network (tens of Gb/s). In turn, preemptible instances provide the same resource type at a signiﬁcant discount with the condition that the machine can be terminated at any time after short notice. To account for individual variations, we report the average rent price over three popular cloud providers. We consider three popular instance types: two high-end instances with 8 Tesla V100 or A100 GPUs and a low-end instance with a single Tesla T4 GPU. We also describe several low-end servers and workstations available on a public marketplace. Unlike cloud VMs, these instances are hosted on non-curated hardware with less uptime guarantees (typically 95% – 99.9%), slower network and signiﬁcant variation in performance. However, marketplace instances are the cheapest in terms of cost per TFLOPS. To quantify this, we report the average over three most affordable instances that ﬁt the chosen minimum requirements. As a point of comparison, we also measure each system’s training performance for BERT-Large [68] ﬁne-tuning on SQuAD v1.1 [78] in PyTorch with mixed precision. We follow the ofﬁcial benchmark- ing protocol by [75] and reuse the ofﬁcial performance results for V100, A100, and T4 instances. The only exception is GTX 1080Ti, where we use full 32-bit precision because that device does not support efﬁcient half-precision operations. Table 1: Cloud and marketplace GPU instance pricing for short-term usage. Minimum system speciﬁcations Average cost, $/hour BERT-Large training samples/sGPU CPU cores CPU type RAM, GB Regular Preemptible Cloud instances 8× V100 64 Intel Xeon Broadwell 480 23.47 7.13 354 8× A100 96 AMD Epyc ROME 960 30.65 10.18 755 1× T4 4 Intel Xeon Cascade Lake 16 0.46 0.18 18 Marketplace instances 6× 3090 32 AMD Epyc Rome 480 5.04 4.17 154 4× 2080Ti 16 Intel Xeon Haswell 240 0.96 0.84 83.4 1× RTX 1080Ti 8 Intel Xeon Haswell 16 0.22 0.16 12 Table 1 shows two main tendencies. First, preemptible cloud instances are, on average, three times cheaper than their non-preemptible counterparts8. Second, the high-end HPC-grade servers that offer the highest raw performance are less cost-effective than lower-tier servers and marketplace instances. In theory, one could match the raw ﬂoating-point performance of a 8×V100 instance at a fraction of its cost using multiple lower-tier workstations, such as 4×RTX 2080Ti, with a smaller total cost. 8The cost can be up to 11× cheaper for some instance types, e.g. Azure V100 instances in the central US region at the time of writing. 18However, in practice, running distributed training with these workstations is challenging due to their unreliability and slow network connection. Note that this analysis does not represent the cloud costs for sustained GPU usage. If an organization plans to constantly use GPU resources over a period of multiple years, they can reduce the costs by deploying their own compute infrastructure or relying on the sustained usage discounts reaching up to 60–70%. Thus, the long-term compute costs are much harder to analyze and depend on a number of additional factors, such as local electricity prices for on-premise infrastructure. However, this scenario offers similar trade-offs: HPC-grade infrastructure offers greater interconnectivity, but requires expensive network interface cards, high-end switches and a more complex setup process. B Additional Related Work In this section, we review some of the papers relevant to our work, but omitted from the main part due to space constraints. B.1 Decentralized training In this subsection, we give additional details about the dependence of gossip-based optimization methods on the spectral properties on the communication graph through the spectral properties of the mixing matrix [ 44, 42] or the Laplacian matrix [ 45, 46] of the network. That is, gossip ﬁnds approximate average on nodes with accuracy εafter O ( (1 −λ2(M))−1 log(ε−1) ) iterations, where M is the mixing matrix and λ2(M) is the second largest eigenvalue of M when sorted by absolute value. The quantity η = 1 −λ2(M) is called the spectral gap of the mixing matrix M, and η−1 is typically a polynomial of the total number of nodes N when the maximal degree of the node is O(1). For example, for uniformly averaging M one can show that η−1 = O(N2) for the ring topology (node degree 2), η−1 = O(N) for the two-dimensional torus topology (node degree 2), and η−1 = O(1) for the fully connected graph (node degree N −1); one can ﬁnd more examples in [ 79]. Similarly, the communication complexity of decentralized optimization methods often has multiplicative dependence on either O(η−1) (see [80] and references therein) or O(η−1/2) [42, 46, 81, 82], which is not improvable for gossip-based methods [83, 40]. Contrary to this, Moshpit All-Reduce does not depend on a ﬁxed communication graph and the properties of its mixing matrix. However, it depends on the number of averaging groups and the total number of peers (see Theorem 3.2), which can be viewed as properties of a time-varying random communication graph. Fortunately, this dependence is often much better than in gossip: as we mentioned in the main part of the paper, even if workers are randomly split into pairs at each iteration, the simpliﬁed version of Moshpit All-Reduce makes the average distortion (the left-hand side of Equation 5) at least 2 times smaller after each round on average. B.2 Compressed communication Another popular approach to address the communication bottleneck is communication compres- sion [84, 85, 86, 87, 88]: before sending any information (e.g., iterates, gradients, Hessians or more sophisticated data) over the network, peers compress this information by applying a possibly random transformation. As the result, peers send fewer bits for each communication round, but the total number of communication rounds needed to achieve the predeﬁned accuracy of the solution increases. However, compression can be useful in situations when the reduction in communication costs of one round is more important than the increase in the number of these rounds [89]. There are two distinct groups of works on distributed training with compressed communication: ones that focus on unbiased compression operators (e.g., Rand-K, ℓp-quantization) and ones studying algorithms with biased compressors (e.g., Top-K); see a detailed summary of popular compression operators in [90]). Quantized SGD (QSGD) [85] and TernGrad [91] were among the ﬁrst compression methods with convergence guarantees. Next, the convergence analysis of these methods was gener- alized and tightened in the (strongly) convex case in [92]. Moreover, the authors of [92] proposed a modiﬁcation of QSGD called DIANA: this algorithm is based on the quantization of gradients’ differences, which helps it achieve linear convergence in the strongly convex case when peers com- pute full gradients. Next, DIANA was generalized to arbitrary unbiased compression in [93], where authors also developed and analyzed the variance-reduced version of DIANA. After that, several 19further modiﬁcations, such as Accelerated DIANA [ 94] and DIANA with bidirectional compres- sion [95, 96], were proposed. Finally, we refer the reader to [ 97, 98, 99, 100] for state-of-the-art results for distributed methods with unbiased compression in the non-convex case. However, naïve application of biased compression operators can lead to signiﬁcantly worse per- formance in practice. For instance, as it was shown recently in [ 90], parallel SGD with Top-1 compression can diverge exponentially fast. Therefore, biased compressors are used jointly with so-called error-compensation [84]. The ﬁrst analysis of Error-Compensated SGD (EC-SGD) was proposed in [101, 102] which then was generalized and tightened in [90]. Next, several further im- provements, such as an accelerated version of EC-SGD [103] and linearly converging EC-SGD [95], were recently proposed. However, current theory does not show any superiority of distributed meth- ods with biased compressors to the ones with unbiased compression operators. In addition, one can combine decentralized communication with compression. Such combinations with unbiased compression operators were studied in [ 104, 105] and with biased operators in [ 24, 106]. In this paper, we do not study the interaction of different compression methods and Moshpit Averaging, leaving this promising direction to future work. B.3 Multiple local steps Alternatively, to reduce the impact of the communication bottleneck, it is possible to perform several local optimization steps on each peer between the communication rounds. This approach is based on the idea that the increased computational load of peers will decrease the number of communication rounds required to obtain the optimal parameters; it is frequently used in federated learning [107, 108]. In particular, one of the most popular methods with multiple local steps is called Local-SGD or Federated Averaging [107, 109]. The ﬁrst results on its convergence were given in [ 109, 110], and later they were tightened and generalized both for homogeneous [ 61, 62] and heterogeneous cases [61, 111]. Recently, further modiﬁcations of Local-SGD were proposed and analyzed: these modiﬁcations include acceleration [ 112], variance reduction [ 60], communication compression [113, 98, 99], decentralization [64, 63], adaptive and proximal methods [76, 114], and resistance to client drift [59]. Moshpit SGD can perform multiple local gradient steps before synchronization by design, as shown in Algorithm 2. B.4 Asynchronous methods In the previous subsections, we mostly discussed synchronous distributed methods, since they are more widespread and better studied than asynchronous ones. Mainly, this is because asynchronous methods are more difﬁcult to implement, debug and analyze under general assumptions. However, such methods can be more efﬁcient in terms of using computational resources, which leads to faster wall-clock convergence [115]. In recent years, several asynchronous stochastic methods [116, 117, 118], methods with no shared memory [119, 120], and methods with delayed updates [121, 122, 123, 95] were proposed and analyzed: one can ﬁnd more details in a recent survey [115]. Moshpit SGD belongs to this family of asynchronous approaches as well, because the averaging steps happen in smaller groups and can be interleaved with local parameter updates. B.5 Distributed Hash Tables In this work, we set out to improve distributed averaging with a dynamic matchmaking protocol. Without a central server, this protocol relies on decentralized data structures to organize peers. The main data structure we use is the Distributed Hash Table, or DHT. On a high level, DHT is a distributed fault-tolerant “dictionary” that can be accessed by every participant. Each key-value pair is stored on a subset of peers determined by the hash function of the key. Each participant has a unique identiﬁer (ID) sampled uniformly from the hash function output range. When storing a (key, value) pair, one must ﬁnd kpeers whose IDs are nearest to hash(key) according to a chosen metric. After that, the participant requests each of those peers to store (key, value). When retrieving a value for a key, one should compute hash(key), search for peers with IDs nearest to that hash value and request the value from those peers. Speciﬁc DHT versions, such as Chord [ 124] or Kademlia [ 55], employ different hash types and algorithms for ﬁnding nearest peers. For instance, Kademlia DHT sorts peers based on the XOR distance function: d(x,y) = int(x⊕y). 20In DHT, each participant is directly aware of only a small subset of peers. When storing or retrieving a key, the participant requests additional peers from its neighbors in a semi-greedy search, minimizing the XOR distance until it ﬁnds knearest peers. In Kademlia, nodes form a special navigable graph structure that lets them ﬁnd nearest peers in at most O(k+ logN) requests to other peers, where N is the total number of participants. Due to their scalability and fault-tolerance, DHTs found numerous applications including BitTorrent, Ethereum, I2P and decentralized deep learning [36]. C Proofs of Mixing Properties of Moshpit All-Reduce Notation. Throughout the following sections, we use the standard notation from the literature on stochastic optimization. That is, for any n-dimensional vectors x = ( x1,...,x n)⊤,y = (y1,...,y n)⊤∈Rn we use ⟨x,y⟩to denote the standard inner product: ⟨x,y⟩= x1y1 + ... + xnyn. Next, we use ∥x∥to denote the ℓ2=norm of x(∥x∥= √ ⟨x,x⟩), E[ξ] to denote an expectation of a random variable ξ, E[ξ|η] is used for the conditional expectation of ξgiven η, and P{E}denotes the probability of an event E. C.1 Computing exact average in a full grid As discussed in Section 3.1, Moshpit All-Reduce obtains the exact average of parameter vectors from N peers arranged in a grid with dcoordinates and M positions per coordinate when N ≡Md. That is, when the grid is full and each step averages M parameter values along a single grid coordinate without repetitions, the algorithm needs only dsteps to compute the actual average across all nodes. In this section, we give a proof of this fact. First, let us formally deﬁne the setting and the averaging steps of Moshpit All-Reduce in this speciﬁc case. Let θi1i2...id be the parameter vector of the worker with coordinates i1,i2,...,i d; each coordinate ik takes values from 1 to M, because the hypercube of peers is completely full (thus, due to the pigeonhole principle, there are no unoccupied coordinates). Next, arrange the coordinates of these vector according to the order of averaging iterations: namely, at iteration 1 θ 1 i1i2...id = 1 M M∑ j1=1 θj1i2...id, i 1 ∈{1,...,M }, (13) which means that for the ﬁrst iteration, we take the average across the ﬁrst axis θ 1 and replicate it across all M resulting vectors regardless of their index i1. The next averaging steps can be expressed similarly with a simple recurrence relation: θ t i1i2...id = 1 M M∑ jt=1 θ t−1 i1...it−1jtit+1...id. (14) Given this formal deﬁnition, we can now state and prove the exact averaging result: Theorem C.1 (Exact average in a full d-dimensional hypercube after dsteps). Assume that Md peers are arranged in a d-dimensional hypercube with M positions in each dimension. Also, assume that each peer fully participates in every averaging step and M-sized groups for each averaging iteration are determined based on the hypercube coordinates. Then, if Moshpit All-Reduce is ran in the above setup for diterations without repeating groups (i.e. averaging across each dimension exactly once), its result for each participant is the average value of θacross all Md peers. 21Proof. We can directly obtain the expression for the average by expanding the recurrence and rearranging the sums: θ d i1i2...id = 1 M M∑ jd=1 θ d−1 i1...id−1jd = 1 M M∑ jd=1   1 M M∑ jd−1=1 θi1i2...jd−1jd  = ... = 1 M ( M∑ jd=1 ( 1 M M∑ jd−1=1 ... M∑ j2=1 ( 1 M M∑ j1=1    dsummations θj1...jd ))) = 1 Md M∑ jd=1 M∑ jd−1=1 ... M∑ j2=1 M∑ j1=1 θj1...jd = 1 Md M∑ j1,...,jd=1 θj1...jd. But this is exactly the global average of all θ, since there are Md participants and each vector is represented in the sum because of summation over all possible indices. Notice that for a given grid of peers, if some of its indices do not have corresponding parameter vectors, Equation 14 may result in different average vectors on different workers due to different numbers of peers along a coordinate for different indices. For example, running two iterations of Moshpit Averaging with d= 2, M= 2 and three parameter vectors θ11, θ21, θ22 results in θ11+θ21 2 on the ﬁrst worker and θ11+θ21 4 + θ22 on other workers, with neither equal to the global average. However, the variance of the averaged vectors does decrease, which is formally proven in Section C.3. C.2 Proof of Theorem 3.1 Below we provide the complete proof of Theorem 3.1. For the readers’ convenience, we restate the theorem. Theorem C.2 (Theorem 3.1). If all workers have non-zero probability of successfully running a communication round in Moshpit Averaging and the order ofpeerst is random, then all local vectors θt i converge to the global average with probability1: ∀i= 1,...,N θt i − 1 N N∑ i=1 θ0 i  2 −−−→ t→∞ 0. (15) Proof of Theorem 3.1. First of all, we notice that (15) is equivalent to ∀i= 1,...,N, ∀j = 1,...,n ( θt i(j) − 1 N N∑ i=1 θ0 i(j) )2 −−−→ t→∞ 0, (16) where θt i(j) denotes j-th component of θt i. Consider an arbitrary component j ∈{1,...,n }and the sequence of intervals {Ij,t}t≥0 where Ij,t = conv{θt 1(j),θt 2(j),...,θ t N(j)}. Then, {Ij,t}t≥0 is a sequence of nested intervals ( Ij,t+1 ⊆Ij,t∀t ≥0), since averaging in groups does not expand the convex hull of {θt 1,θt 2,...,θ t N}. For convenience, we specify the bounds of the intervals: Ij,t = [aj,t,bj,t]. Using the Cantor’s intersection theorem, we conclude that ∞⋂ t=0 Ij,t = Ij = [aj,bj], where θ(j) = 1 N ∑n i=1 θ0 i(j) ∈[aj,bj]. If [aj,bj] = {θ(j)}with probability 1, then (16) holds with probability 1 as well. Suppose the opposite: there exist such j ∈{1,...,n }, [a,b] and δ,∆ >0 that θ(j) ∈[a,b], b−a= ∆ and P { [a,b] ⊆ ∞⋂ t=0 Ij,t    E } = δ >0 and ∀ε> 0 P { [a−ε,b + ε] ⊆ ∞⋂ t=0 Ij,t    Eε } <δ. 22This implies that for all ε> 0 there exists such Tε >0 that P { ∀t≥Tε aj,t ∈[a−ε,a],bj,t ∈[b,b + ε]   E′ε } = δε >0. Consider ε = ∆ (2N+100)2N and assume that the event E′ ε holds. Next, we introduce new notation: Jt left = {i∈{1,...,n }| θt i(j) ∈[a−ε,a]}and Jt right = {i∈{1,...,n }| θt i(j) ∈[b,b+ε]}. Since E′ ε holds the sets Jt left and Jt right are non-empty for all t≥Tε with probability δε >0: P { ∀t≥Tε Jt left ̸= ∅ and Jt right ̸= ∅ } = δε >0. (17) We notice that every pair of workersi1,i2 has a non-zero probability of taking part in the averaging inside the common group at each iteration since all workers have a non-zero probability of successfully running a communication round and the order of peerst is random. This implies that every pair of workers i1,i2 with probability 1 take part in the averaging inside the common group inﬁnitely many times when tgoes to the inﬁnity. Next, we choose some t0 ≥Tε. Let Jt0 left = {il,1,...,i l,ql}and Jt0 right = {ir,1,...,i r,qr}. Consider the event E′ ε,0 ⊆E′ ε such that in E′ ε,0 peer il,1 computes an average in the group containing any peer from Jt0 right at some iteration t1 >t0. Our observations above imply that P{E′ ε,0}= P{E′ ε}= δε >0. Then, θt1 il,1 (j) ≥N−1 N (a−ε)+ 1 Nb= a−ε+ 1 N(∆+ε) = a− ∆ (2N+100)2N + 1 N ( ∆ + ∆ (2N+100)2N ) > a+ ∆ 2N, i.e., θt1 il,1 (j) ∈(a,b] meaning that il,1 ̸∈Jt1 left. The last part of the proof shows that for any t≥t1, the peer il,1 will never be the part of Jt left and after a ﬁnite number of iterations Jt left = ∅ with probability δε >0 when E′ ε,0 holds, implying the contradiction with (17). To show that, we consider the following set of peers: ˆJt1 left = {i∈{1,...,n }|∃ t≥t1 : θt i(j) ∈ [a−ε,a + ∆ 2N)}. Next, we consider the event E′ ε,1 ⊆E′ ε,0 such that in E′ ε,1 peer il,1 computes an average in the group containing some peer il,avg,1 from ˆJt1 left at some iteration t2 > t1 (and t2 is the ﬁrst such moment after t1). Again, our observations imply P{E′ ε,1}= P{E′ ε,0}= δε > 0. Then, θt2 il,1 (j) = θt2 il,avg,1 (j) > N−1 N (a−ε) + 1 N ( a+ ∆ 2N ) = a+ ∆ 2N2 − (N−1)∆ N(2N+100)2N >a + ∆ 4N2 . After that, we consider the event E′ ε,2 ⊆E′ ε,1 such that in E′ ε,2 peer il,1 or il,avg,1 computes an average in the group containing a peer il,avg,2 ̸= il,avg,1 from ˆJt1 left at an iteration t3 > t2 (and t3 is the ﬁrst such moment after t2). Then, θt3 il,1 (j),θt3 il,avg,1 (j) and θt3 il,avg,2 (j) are greater than N−1 N (a−ε) + 1 N ( a+ ∆ 4N2 ) = a+ ∆ 4N3 − (N−1)∆ N(2N+100)2N >a + ∆ 8N3 . Therefore, after at least N −1 of such averaging iterations, with probability δε all θt i(j) will be greater than a+ ∆ (2N)N >a while E′ ε holds. This contradicts (17). Therefore, ∞⋂ t=0 Ij,t = {θ(j)} with probability 1, which concludes the proof. C.3 Proof of Theorem 3.2 In this section, we provide the complete proof of Theorem 3.2. For convenience, we restate the theorem below. Theorem C.3 (Theorem 3.2, averaging convergence rate). Consider the modiﬁcation of Moshpit All-Reduce that works as follows: at each iteration k≥1 1) peers are randomly split into rdisjoint groups of sizes Mk 1 ,...,M k r in such a way that ∑r i=1 Mk i = N and Mk i ≥1 ∀i = 1,...,r and 2) peers from each group compute their group average via All-Reduce. Letθ1,...,θ N be the input vectors of this procedure and θT 1 ,...,θ T N be the outputs after T iterations. Then, E [ 1 N N∑ i=1 ∥θT i −θ∥2 ] = (r−1 N + r N2 )T · 1 N N∑ i=1 ∥θi −θ∥2, (18) where θ= 1 N ∑N i=1 θi. 23Proof. First of all, let us clarify the procedure of random splitting of peers in rgroups. We assume that at iteration kof the modiﬁed algorithm we generate a random permutation πk = (πk 1 ,...,π k N) of 1,...,N . Next, Jk 1 = {πk 1 ,...,π k Mk 1 }form the indices of the ﬁrst group of workers, Jk 2 = {πk Mk 1 +1,...,π k Mk 2 }are the indices of the second group, andJk r = {πk Mk 1 +Mk 2 +...+Mk r−1+1,...,π k N} are the indices of group r. In other words, we generate a random permutation and take contiguous subgroups of indices corresponding to predeﬁned group sizes Mk i , starting from the ﬁrst group. By deﬁnition, we have ⨆r i=1 Jk i = {1,2,...,N }, where ⊔deﬁnes the disjoint union operator. Moreover, notice that group sizesMk 1 ,...,M k r can depend on kand even be random: for our analysis, it is sufﬁcient that the randomness deﬁning the permutation is independent from Mk 1 ,...,M k r. Next, vectors θk 1 ,...,θ k N are obtained by the following formula: ∀j = 1,...,N, θ k j = 1 Mk i ∑ t∈Jk i θk−1 t , where Jk i is the group for which j ∈Jk i . Using this, we show that the average of vectors {θk i}n i=1 remains the same throughout the iterations of Moshpit All-Reduce: 1 N N∑ j=1 θk j = 1 N r∑ i=1 Mk i · 1 Mk i ∑ t∈Jk i θk−1 t = 1 N r∑ i=1 ∑ t∈Jk i θk−1 t = 1 N N∑ j=1 θk−1 j . Therefore, the quantity 1 N ∑N j=1 ∥θk j −θ∥2 (average distortion) measures the quality of averaging. For this quantity, we can derive the following expression: 1 N N∑ j=1 ∥θk j −θ∥2 = 1 N r∑ i=1 Mk i  1 Mk i ∑ t∈Jk i θk−1 t −θ  2 = 1 N r∑ i=1 1 Mk i  ∑ t∈Jk i ∥θk−1 t −θ∥2 + 2 ∑ t,l∈Jk i ,t<l ⟨θk−1 t −θ,θk−1 l −θ⟩  . Taking the expectation Eπk[·] with respect to the randomness coming from the choice of πk we get Eπk  1 N N∑ j=1 ∥θk j −θ∥2   = 1 N r∑ i=1 1 Mk i ( Eπk [ ∑ t∈Jk i ∥θk−1 t −θ∥2 ] +2Eπk [ ∑ t,l∈Jk i ,t<l ⟨θk−1 t −θ,θk−1 l −θ⟩ ]) . Since ∀j,j1,j2 ∈{1,...,N },j1 ̸= j2 and for all i= 1,...,r P { j ∈Jk i } = Mk i N , P { j1,j2 ∈Jk i } = Mk i (Mk i −1) N2 , 24we have Eπk  1 N N∑ j=1 ∥θk j −θ∥2   = 1 N r∑ i=1 1 N N∑ j=1 ∥θk−1 j −θ∥2 + 1 N r∑ i=1 2Mk i −1 N2 ∑ 1≤j1<j2≤N ⟨θk−1 j1 −θ,θk−1 j2 −θ⟩ = r N2 N∑ j=1 ∥θk−1 j −θ∥2 + 2N −r N3 ∑ 1≤j1<j2≤N ⟨θk−1 j1 −θ,θk−1 j2 −θ⟩ = ( r N2 −N −r N3 ) N∑ j=1 ∥θk−1 j −θ∥2 + N −r N3 N∑ j=1 ∥θk−1 j −θ∥2 +2N −r N3 ∑ 1≤j1<j2≤N ⟨θk−1 j1 −θ,θk−1 j2 −θ⟩ = N(r−1) + r N3 N∑ j=1 ∥θk−1 j −θ∥2 + N −r N3  N∑ j=1 (θk−1 j −θ)  2    ∥Nθ−Nθ∥2=0 = (r−1 N + r N2 ) · 1 N N∑ j=1 ∥θk−1 j −θ∥2. Finally, we take the full expectation from the both sides of the above equation and apply the tower property E[Eπk [·]] = E[·]: E  1 N N∑ j=1 ∥θk j −θ∥2  = (r−1 N + r N2 ) E  1 N N∑ j=1 ∥θk−1 j −θ∥2  . Unrolling the recurrence for k= T, we establish (18). Remark C.1. The result implies that increasing the group sizeα> 1 times implies almost αtimes faster convergence to the average. Remark C.2. Our analysis can be easily generalized to the case when number of groups r can depend on kand be a random variable independent from the choice of permutations and the number of groups at previous steps. In this case, (18) transforms into E [ 1 N N∑ i=1 ∥θT i −θ∥2 ] = 1 N N∑ i=1 ∥θi −θ∥2 · T∏ k=1 (E[rk] −1 N + E[rk] N2 ) , (19) where rk is the number of groups at iteration k. C.4 Additional Guarantees For Moshpit Averaging In this section, we derive the result measuring the rate of variance reduction when averaging random vectors with Algorithm 1. We start with the following technical lemma: Lemma C.1. Let ξ ∼Binom(M,p) have a binomial distribution with parameters M (number of trials) and p(probability of success for each trial). Then m1(M,p) := E [ min {1 ξ,1 }] = (1 −p)M + M∑ i=1 (1 −p)M−i −(1 −p)M i , (20) m2(M,p) := E [ min {1 ξ2 ,1 }] = (1 −p)M + M∑ i=1 (1 −p)M−i −(1 −p)M i M∑ j=i 1 j. (21) 25Proof. We start with the proof of (20). By deﬁnition of the expectation, we have E [ min {1 ξ,1 }] = (1 −p)M + M∑ i=1 1 ipi(1 −p)M−i (M i ) . For simplicity of further derivations, we introduce the following notation: m1(M,p) = E [ min { 1 ξ,1 }] and m2(M,p) = E [ min { 1 ξ2 ,1 }] . Taking the derivative of m1(M,p) by p, we obtain m′ 1(M,p) = −M(1 −p)M−1 + M∑ i=1 pi−1(1 −p)M−i (M i ) − M∑ i=1 M −i i pi(1 −p)M−i−1 (M i ) = −M(1 −p)M−1 + 1 p ( −(1 −p)M + M∑ i=0 pi(1 −p)M−i (M i )) − M 1 −p M∑ i=1 1 ipi(1 −p)M−i (M i ) + 1 1 −p ( −(1 −p)M + M∑ i=0 pi(1 −p)M−i (M i )) = −M(1 −p)M−1 + 1 p ( 1 −(1 −p)M) − M 1 −p ( m1(M,p) −(1 −p)M) + 1 1 −p ( 1 −(1 −p)M) = 1 p(1 −p) −(1 −p)M−1 p − M 1 −pm1(M,p). Rearranging the terms, we get the following linear ﬁrst-order ODE m′ 1(M,p) + M 1 −pm1(M,p) = 1 p(1 −p) −(1 −p)M−1 p . (22) To solve it, we consider the following homogeneous ODE: m′ 1(M,p) + M 1 −pm1(M,p) = 0. The solution of this ODE ism1(M,p) = C(1−p)M, where C ∈R is an arbitrary real constant. Next, we go back to the initial ODE (22) and try to ﬁnd a solution of the form m1(M,p) = C(p)(1 −p)M, where C(p) : R →R is a differentiable function: ( C(p)(1 −p)M)′ + M 1 −pC(p)(1 −p)M = 1 p(1 −p) −(1 −p)M−1 p ⇓ C′(p)(1 −p)M = 1 p(1 −p) −(1 −p)M−1 p ⇓ C′(p) = 1 p(1 −p)M+1 − 1 p(1 −p). Since 1 x(1 −x)k+1 = 1 x(1 −x)k + 1 (1 −x)k+1 (23) 26for all x̸∈{0,1}and all non-negative integers k, we have C′(p) = 1 p + 1 1 −p + 1 (1 −p)2 + ... + 1 (1 −p)M+1 −1 p − 1 1 −p ⇓ C′(p) = M∑ i=1 (1 −p)−i−1, hence C(p) = ˆC+ M∑ i=1 1 i(1 −p)−i, where ˆCis a real constant. Putting all together, we obtain m1(M,p) = C(p)(1 −p)M = ˆC(1 −p)M + M∑ i=1 1 i(1 −p)M−i. Taking m1(M,0) = 1 into account, we conclude that ˆC = 1 −∑M i=1 1 i and obtain (20). Using a similar technique, we derive (21). By deﬁnition of the expectation, we have m2(M,p) = (1 −p)M + M∑ i=1 1 i2 pi(1 −p)M−i (M i ) . Taking the derivative ofm2(M,p) by p, we obtain m′ 2(M,p) = −M(1 −p)M−1 + M∑ i=1 1 ipi−1(1 −p)M−i (M i ) − M∑ i=1 M −i i2 pi(1 −p)M−i−1 (M i ) = −M(1 −p)M−1 + 1 p M∑ i=1 1 ipi(1 −p)M−i (M i ) − M 1 −p M∑ i=1 1 i2 pi(1 −p)M−i (M i ) + 1 1 −p M∑ i=1 1 ipi(1 −p)M−i (M i ) = −M(1 −p)M−1 + 1 p ( m1(M,p) −(1 −p)M) + 1 1 −p ( −Mm2(M,p) + M(1 −p)M + m1(M,p) −(1 −p)M) = m1(M,p) p(1 −p) −(1 −p)M−1 p − M 1 −pm2(M,p). Rearranging the terms, we get the following linear ﬁrst-order ODE m′ 2(M,p) + M 1 −pm2(M,p) = m1(M,p) p(1 −p) −(1 −p)M−1 p . (24) To solve this ODE, we consider the homogeneous ODE: m′ 2(M,p) + M 1 −pm2(M,p) = 0. The solution of this ODE ism2(M,p) = C(1−p)M, where C ∈R is an arbitrary real constant. Next, we go back to the initial ODE (24) and try to ﬁnd a solution of the form m2(M,p) = C(p)(1 −p)M, 27where C(p) : R →R is a differentiable function: ( C(p)(1 −p)M)′ + M 1 −pC(p)(1 −p)M = m1(M,p) p(1 −p) −(1 −p)M−1 p ⇓ C′(p)(1 −p)M = m1(M,p) p(1 −p) −(1 −p)M−1 p ⇓ C′(p) = m1(M,p) p(1 −p)M+1 − 1 p(1 −p). Using (23) and (20), we derive C′(p) (20) = − M∑ i=1 1 i p(1 −p) + M∑ i=1 1 i(1 −p)M−i p(1 −p)M+1 = − M∑ i=1 1 ip(1 −p) + M∑ i=1 1 ip(1 −p)i+1 (23) = − M∑ i=1 1 i (1 p + 1 1 −p ) + M∑ i=1 1 i (1 p + 1 1 −p + 1 (1 −p)2 + ... + 1 (1 −p)i+1 ) = M∑ i=1 1 i ( 1 (1 −p)2 + ... + 1 (1 −p)i+1 ) = M∑ i=1 1 (1 −p)i+1 M∑ j=i 1 j, hence C(p) = ˆC+ M∑ i=1 1 i(1 −p)−i M∑ j=i 1 j, where ˆCis a real constant. Putting all together, we obtain m2(M,p) = C(p)(1 −p)M = ˆC(1 −p)M + M∑ i=1 1 i(1 −p)M−i M∑ j=i 1 j. Taking m2(M,0) = 1 into account, we conclude that ˆC = 1 −∑M i=1 1 i ∑M j=i 1 j and obtain (21). Using this lemma, we derive the following result: Theorem C.4. Assume that peers participating in Moshpit Averaging have independent random vectors θ1,...,θ N with means θ1,..., θN and variances bounded by σ2 before the averaging. Let θT 1 ,...,θ T N be the outputs of Moshpit Averaging afterT iterations. Finally, we assume that each peer from the grid can be dropped out for the whole averaging process before averaging independently from other peers, i.e., N ∼Binom(Md,p). Then, for all i= 1,...,N we have E [θT i −Eθ [ θT i ]2] ≤MT−1σ2m1(M −1,p) (m2(M −1,p))T−1 , (25) where functions m1(M,p) and m2(M,p) are deﬁned in (20) and (21) respectively, andEθ[·] denotes the expectation w.r.t. the randomness from θ1,...,θ N. Moreover, if p ≥ 2 3 and M ≥11, then m1(M −1,p) ≤ 2 M, m2(M −1,p) ≤ 3 M2 and E [θT i −Eθ [ θT i ]2] ≤ 2σ2 M(M/3)T−1 . (26) 28Proof. First of all, we recall an equivalent formulation of Moshpit Averaging. Consider a hypercube {1,...,M }d. One can consider the elements of this hypercube as hyperindices and assign a unique hyperindex to each peer so that peers can be viewed as vertices in the hypercube. Then, during the k-th iteration of Moshpit All-Reduce, each worker computes the average among those peers that have hyperindices with the same values except the k-th index; in other words, peers compute averages along the k-th dimension of the hypercube. Next, if N = 0, we assume that θT i = Eθ [ θT i ] and (25) holds for free. Therefore, to derive (25), we assume that N >0. More formally, we use the following notation: θCi = θi for all i = 1 ,...,N , where Ci = (ci 1,ci 2,...,c i d), ci j ∈{1,...,M }for all j = 1 ,...,M , and Ci ̸= Ck for i ̸= k. Let Cbe the set of hyperindices corresponding to all peers. Next, we use θt Ci to deﬁne the vector stored on i-th peer after titerations of Moshpit Averaging. Then, for all i= 1,...,N we have θ0 Ci = θCi and for all t= 1,...,d θt Ci = 1 bi,t ∑ k∈Ji,t θt−1 Ck , where Ji,t = {k ∈N |Ck = (ck 1,...,c k d) ∈C and ck j = ci j ∀j ̸= t}and bi,t = |Ji,t|. Using this, we derive the following formula for θt Ci: θT i ≡θT Ci = 1 bi,T ∑ i1∈Ji,T 1 bi1,T−1 ∑ i2∈Ji1,T−1 1 bi2,T−2 ∑ i3∈Ji2,T−1 ... 1 biT−1,1 ∑ iT∈JiT−1,1 θiT. Taking the expectation w.r.t.θ1,...,θ N, we get Eθ [ θT i ] = 1 bi,T ∑ i1∈Ji,T 1 bi1,T−1 ∑ i2∈Ji1,T−1 1 bi2,T−2 ∑ i3∈Ji2,T−1 ... 1 biT−1,1 ∑ iT∈JiT−1,1 θiT. Using the independence of θ1,...,θ N, we derive Eθ [θT i −Eθ [ θT i ]2] = Eθ    ∑ i1∈Ji,T ∑ i2∈Ji1,T−1 ... ∑ iT∈JiT−1,1 θiT −θiT bi,Tbi1,T−1 ...b iT−1,1  2  = ∑ i1∈Ji,T ∑ i2∈Ji1,T−1 ... ∑ iT∈JiT−1,1 Eθ [ ∥θiT −θiT∥2] b2 i,Tb2 i1,T−1 ...b 2 iT−1,1 ≤ ∑ i1∈Ji,T ∑ i2∈Ji1,T−1 ... ∑ iT∈JiT−1,1 σ2 b2 i,Tb2 i1,T−1 ...b 2 iT−1,1 = ∑ i1∈Ji,T ∑ i2∈Ji1,T−1 ... ∑ iT−1∈JiT−2,2 σ2 b2 i,Tb2 i1,T−1 ...b 2 iT−2,2biT−1,1 . Next, taking the full expectation from the both sides of the previous inequality and using the tower property, we obtain E [θT i −Eθ [ θT i ]2] ≤E   ∑ i1∈Ji,T ∑ i2∈Ji1,T−1 ... ∑ iT−1∈JiT−2,2 σ2 b2 i,Tb2 i1,T−1 ...b 2 iT−2,2biT−1,1  . (27) Notice that Jik,T−k ∩Jik+1,T−k−1 = {ik+1}for all k= 0,...,T −1, where i0 = i. Moreover, for k1,k2 ∈{0,1,...,T }, k1 <k2 either Jik1 ,T−k1 ∩Jik2 ,T−k2 = {k2}or Jik1 ,T−k1 ∩Jik2 ,T−k2 = ∅. The ﬁrst situation is possible iff ik1 = ik1+1 = ...i k2−1. Taking these observations about setsJik,T−k into account, we consider the sets J′ ik,T−k = Jik,T−k\\ {ik}for k = 0,1,...,T −1. These sets are pairwise disjoint and their cardinalities b′ ik,T−k = |J′ ik,T−k|satisfy the following relations: bik,T−k = 1 + b′ ik,T−k ≥max{1,b′ ik,T−k}=: ˆbik,T−k for k = 1,2,...,T −1. Moreover, b′ i,T,b′ i1,T−1,...,b ′ iT−1,1 are independent random variables from the binomial distribution Binom(M −1,p). Finally, we notice that the number of terms in (27) is upper-bounded by MT−1, since |Ji,t|≤ M for all i= 1,...,N and t= 0,...,T . 29Putting all together, we obtain E [θT i −Eθ [ θT i ]2] ≤ E   ∑ i1∈Ji,T ∑ i2∈Ji1,T−1 ... ∑ iT−1∈JiT−2,2 σ2 ˆb2 i,Tˆb2 i1,T−1 ... ˆb2 iT−2,2ˆbiT−1,1   ≤ MT−1σ2E [ 1 ˆξ2 1 ˆξ2 2 ... ˆξ2 T−1 ˆξT ] = MT−1σ2E [ 1 ˆξ2 1 ] E [ 1 ˆξ2 2 ] ... E [ 1 ˆξ2 T−1 ] E [ 1 ˆξT ] , where ˆξ2 k = max {1,ξ2 1}for k = 1 ,...,T and ξ1,...,ξ T are i.i.d. random variables having the binomial distribution Binom(M−1,p). Then one can simplify the inequality above using Lemma C.1 and get E [θT i −Eθ [ θT i ]2] ≤ MT−1σ2m1(M −1,p) (m2(M −1,p))T−1 , where functions m1(M,p) and m2(M,p) are deﬁned in (20) and (21) respectively. Next, we simplify the obtained upper bound under the assumption that M and pare not too small; speciﬁcally, M ≥11 and p≥2/3. From (20), we have m1(M −1,p) = (1 −p)M−1 + M−1∑ i=1 1 i ( (1 −p)M−1−i −(1 −p)M−1) ≤ (1 −p)M−1 M−1∑ i=1 1 i(1 −p)i. Since 1 (k+ 1)(1 −p)k+1 ·k(1 −p)k 1 = k (k+ 1)(1 −p) − −−− → k→∞ 1 1 −p ≥3, we have (1 −p)M−1 M−1∑ i=1 1 i(1 −p)i = Θ ( (1 −p)M · 1 M(1 −p)M ) = Θ ( 1 M ) . Using simple algebra, one can prove that for M ≥11 and p≥2/3 the following inequality holds: m1(M −1,p) ≤(1 −p)M−1 M−1∑ i=1 1 i(1 −p)i ≤ 2 M. Similarly, we analyze m2(M −1,p): m2(M −1,p) = (1 −p)M−1 + M−1∑ i=1 1 i ( (1 −p)M−1−i −(1 −p)M−1)M−1∑ j=i 1 j ≤ (1 −p)M−1 M−1∑ i=1 1 i(1 −p)i M−1∑ j=i 1 j. Since 1 k(1−p)k M−1∑ j=k 1 j 1 (k−1)(1−p)k−1 M−1∑ j=k−1 1 j = (k−1) M−1∑ j=k 1 j k(1 −p) ( 1 k−1 + M−1∑ j=k 1 j ) ≥ 3(k−1) ·1 k k ( 1 k−1 + 1 k ) = 3(k−1)2 k(2k−1) − −−− → k→∞ 3 2, 30we have (1 −p)M−1 M−1∑ i=1 1 i(1 −p)i M−1∑ j=i 1 j = Θ ( (1 −p)M · 1 M2(1 −p)M ) = Θ ( 1 M2 ) . Next, one can prove with simple algebra that for M ≥11 and p≥2/3 the following inequality holds: m2(M −1,p) ≤(1 −p)M−1 M−1∑ i=1 1 i(1 −p)i M−1∑ j=i 1 j ≤ 3 M2 . Plugging the obtained upper bounds form1(M−1,p) and m2(M−1,p) in (25), we obtain (26). D Convergence Proofs of Moshpit SGD In this section, we provide the complete statements of the theorems establishing the convergence of Moshpit SGD together with the full proofs. First, we introduce all necessary deﬁnitions, basic inequalities and auxiliary lemmas; then we prove the convergence in strongly convex and convex cases; lastly, we provide the proofs for the non-convex case. D.1 Deﬁnitions, Basic Facts and Auxiliary Results Below we provide several classical deﬁnitions and results which are used in our proofs. D.1.1 Standard Deﬁnitions from Optimization Theory Deﬁnition D.1 (L-smoothness). A function f : Rn →R is called L-smooth if for all x,y ∈Rn, the following inequality holds: ∥∇f(x) −∇f(y)∥≤ L∥x−y∥. (28) If the function f is L-smooth, then for all x,y ∈Rn f(y) ≤f(x) + ⟨∇f(x),y −x⟩+ L 2 ∥y−x∥2. (29) Next, if f is additionally convex and x∗is its minimizer, then for all x∈Rd ∥∇f(x)∥2 ≤2L(f(x) −f(x∗)) . (30) Deﬁnition D.2 (µ-strong convexity). A differentiable function f : Rn →R is called µ-strongly convex if there exists a constant µ≥0 such that for all x,y ∈Rn f(y) ≥f(x) + ⟨∇f(x),y −x⟩+ µ 2 ∥y−x∥2. (31) D.1.2 Basic Facts For all a,b,θ 1,...,θ N ∈Rn and α> 0, the following inequalities hold: ∥a+ b∥2 ≤ 2∥a∥2 + 2∥b∥2, (32)  1 N N∑ i=1 θi  2 ≤ 1 N N∑ i=1 ∥θi∥2, (33) ⟨a,b⟩ ≤ ∥a∥2 2α + α∥b∥2 2 . (34) D.1.3 Properties of Expectation Variance decomposition. For a random vector η ∈Rd and any deterministic vector x ∈Rd, the variance satisﬁes E [ ∥η−Eη∥2 ] = E [ ∥η−x∥2] −∥Eη−x∥2 (35) Tower property of expectation. For any random variables ξ,η ∈Rd we have E[ξ] = E[E[ξ|η]] (36) under the assumption that E[ξ] and E[E[ξ|η]] are well-deﬁned. 31D.1.4 Auxiliary Results For the readers’ convenience, we list all auxiliary results that we use in our proofs below. The ﬁrst result is classical and establishes that the gradient descent step is a contractive operator. Lemma D.1 (Lemma 6 from [59]). For any L-smooth and µ-strongly convex function f : Rn →R, points x,y ∈Rn, and stepsize γ ∈(0,1/L], the following inequality holds: ∥x−γ∇f(x) −y+ γ∇f(y)∥2 ≤(1 −γµ)∥x−y∥2. (37) The next two lemmas are useful for estimating typical recurrences appearing in the analysis. Lemma D.2 (Lemma I.2 from [60]). Let {rk}k≥0 satisfy rK ≤ a γWK + c1γ+ c2γ2 for all K ≥0 with some constants a,c2 ≥0, c1 ≥0, where wk = (1 −γµ(1 −δpv,1))−(k+1), WK = ∑K k=0 wk, µ> 0, δpv,1 ∈[0,1) and γ ≤γ0 for some γ0 >0, γ0 ≤1/µ(1−δpv,1). Then, for all Ksuch that either ln ( max { 2,min { aµ2(1−δpv,1)2K2 /c1,aµ3(1−δpv,1)3K3 /c2 }}) K ≤1 or γ0 ≤ln ( max { 2,min { aµ2(1−δpv,1)2K2 /c1,aµ3(1−δpv,1)3K3 /c2 }}) (1 −δpv,1)µK and γ = min { γ0,ln ( max { 2,min { aµ2(1−δpv,1)2K2 /c1,aµ3(1−δpv,1)3K3 /c2 }}) (1 −δpv,1)µK } we have that rK = ˜O (a γ0 exp (−γ0µ(1 −δpv,1)K) + c1 (1 −δpv,1)µK + c2 (1 −δpv,1)2µ2K2 ) . Lemma D.3 (Lemma I.3 from [60]). Let {rk}k≥0 satisfy rK ≤ a γK + c1γ+ c2γ2 for all K ≥0 with some constants a,c2 ≥0, c1 ≥0 where γ ≤γ0 for some γ0 >0. Then for all K and γ = min { γ0, √ a c1K, 3 √ a c2K } we have that rK = O ( a γ0K + √ac1 K + 3√a2c2 K 2/3 ) . Finally, the lemma below is useful for our convergence analysis in the non-convex case. Lemma D.4 (Lemma I.1 from [ 60]). For any τ random vectors ξ1,...,ξ τ ∈Rd such that ∀t = 2,...,τ the random vector ξt depends on ξ1,...,ξ t−1 and does not depend on ξt+1,...,ξ τ the following inequality holds E    τ∑ t=1 ξt  2 ≤eτ τ∑ t=1 E [ ∥Et[ξt]∥2 ] + e τ∑ t=1 E [ ∥ξt −Et[ξt]∥2 ] , (38) where Et[·] denotes the conditional expectation E[ ·| ξt−1,...,ξ 1]. 32D.2 Convex Case In this section, we give the full proof of Theorem 3.3 about the convergence of Moshpit SGD for convex and strongly convex problems. The scheme of the proof follows the similar steps as in the state-of-the-art analysis of Local-SGD [61, 62, 60]. We start with the following lemma: Lemma D.5. Let f1 = ... = fN = f, function f be µ-strongly convex (Def. D.2) and L-smooth (see Def. D.1), and Assumptions 3.1 and 3.2 hold with ∆k pv = δpv,1γµE[∥θk −θ∗∥2] + γ2δ2 pv,2 and ˜θ= θ∗, where θ∗∈argminθ∈Rn f(θ) and δpv,1 ∈[0,1), δpv,2 ≥0. Then, for any k≥0 the iterates produced by Moshpit SGD with γ ≤1/4Lsatisfy γE [ f(θk) −f(θ∗) ] ≤ (1 −γµ(1 −δpv,1))E [ ∥θk −θ∗∥2] −E [ ∥θk+1 −θ∗∥2] +3Lγ 2 E[Vk] + γ2 ( σ2 Nmin + δ2 pv,2 ) , (39) where Vk = 1 Nk ∑ i∈Pk ∥θk i −θk∥2 and θk = 1 Nk ∑ i∈Pk θk i. Proof. Recall that Assumption 3.2 with ∆k pv = δpv,1γµE[∥θk −θ∗∥2] + γ2δ2 pv,2 and ˜θ= θ∗states E [ ⟨θk+1 −ˆθk+1,θk+1 + ˆθk+1 −2θ∗⟩ ] ≤δpv,1γµE[∥θk −θ∗∥2] + γ2δ2 pv,2, (40) where ˆθk+1 = 1 Nk ∑ i∈Pk (θk i −γgk i). Next, the deﬁnition of ˆθk+1 implies ˆθk+1 = 1 Nk ∑ i∈Pk θk i − γ Nk ∑ i∈Pk gk i = θk −γgk, where gk = 1 Nk ∑ i∈Pk gk i. Using this, we derive ∥θk+1 −θ∗∥2 = ∥ˆθk+1 −θ∗∥2 + 2⟨θk+1 −ˆθk+1,ˆθk+1 −θ∗⟩+ ∥θk+1 −ˆθk+1∥2 = ∥θk −θ∗−γgk∥2 + ⟨θk+1 −ˆθk+1,θk+1 + ˆθk+1 −2θ∗⟩ = ∥θk −θ∗∥2 −2γ⟨θk −θ∗,gk⟩+ γ2∥gk∥2 +⟨θk+1 −ˆθk+1,θk+1 + ˆθk+1 −2θ∗⟩. Taking the conditional expectation E [ ·| θk] := E [ ·| Pk,θk i,i ∈Pk ] from the both sides of the previous equation and using Assumption 3.1, we obtain E [ ∥θk+1 −θ∗∥2 |θk] = ∥θk −θ∗∥2 −2γ ⟨ θk −θ∗, 1 Nk ∑ i∈Pk ∇f(θk i) ⟩ +γ2E    1 Nk ∑ i∈Pk gk i  2 |θk   +E [ ⟨θk+1 −ˆθk+1,θk+1 + ˆθk+1 −2θ∗⟩| θk ] . (41) Next, we estimate the second and the third terms in the right-hand side of (41). First, −2γ ⟨ θk −θ∗, 1 Nk ∑ i∈Pk ∇f(θk i) ⟩ = 2γ Nk ∑ i∈Pk ( ⟨θ∗−θk i,∇f(θk i)⟩+ ⟨θk i −θk,∇f(θk i)⟩ ) (31),(29) ≤ 2γ Nk ∑ i∈Pk ( f(θ∗) −f(θk i) −µ 2 ∥θk i −θ∗∥2 ) + 2γ Nk ∑ i∈Pk ( f(θk i) −f(θk) + L 2 ∥θk i −θk∥2 ) (33) ≤ 2γ ( f(θ∗) −f(θk) ) −γµ∥θk −θ∗∥2 + LγVk, (42) 33where Vk = 1 Nk ∑ i∈Pk ∥θk i −θk∥2. Secondly, since stochastic gradients {gk i}i∈Pk are computed independently, we get γ2E    1 Nk ∑ i∈Pk gk i  2 |θk   (35) = γ2  1 Nk ∑ i∈Pk ∇f(θk i)  2 +γ2E    1 Nk ∑ i∈Pk (gk i −∇f(θk i))  2 |θk   (33) ≤ 2γ2  1 Nk ∑ i∈Pk (∇f(θk i) −∇f(θk))  2 + 2γ2∥∇f(θk)∥2 + γ2 N2 k ∑ i∈Pk E [ ∥gk i −∇f(θk i)∥2 |θk] (33),(30),(7) ≤ 2γ2 Nk ∑ i∈Pk ∥∇f(θk i) −∇f(θk)∥2 +4Lγ2 ( f(θk) −f(θ∗) ) + γ2σ2 Nk (28) ≤ 2L2γ2 Nk ∑ i∈Pk ∥θk i −θk∥2    2L2γ2Vk +4Lγ2 ( f(θk) −f(θ∗) ) + γ2σ2 Nmin . (43) Plugging (42) and (43) in (41), we obtain E [ ∥θk+1 −θ∗∥2 |θk] ≤ (1 −γµ)∥θk −θ∗∥2 −2γ(1 −2Lγ) ( f(θk) −f(θ∗) ) +Lγ(1 + 2Lγ) Vk + γ2σ2 Nmin +E [ ⟨θk+1 −ˆθk+1,θk+1 + ˆθk+1 −2θ∗⟩| θk ] , and E [ ∥θk+1 −θ∗∥2] (40) ≤ (1 −γµ(1 −δpv,1))E [ ∥θk −θ∗∥2] −2γ(1 −2Lγ) E [ f(θk) −f(θ∗) ] +Lγ(1 + 2Lγ) E[Vk] + γ2 ( σ2 Nmin + δ2 pv,2 ) ≤ (1 −γµ(1 −δpv,1))E [ ∥θk −θ∗∥2] −γE [ f(θk) −f(θ∗) ] +3Lγ 2 E[Vk] + γ2 ( σ2 Nmin + δ2 pv,2 ) , where in the last inequality we use γ ≤1/4L. Next, we estimate the term E[Vk] measuring the expected dissimilarity between local iterates and their global average at iteration k. Lemma D.6. Let f1 = ... = fN = f, function f be µ-strongly convex (Def. D.2) and L-smooth (see Def. D.1), and Assumptions 3.1 and 3.2 hold with ∆k pv = δpv,1γµE[∥θk −θ∗∥2] + γ2δ2 pv,2 and ˜θ= θ∗, where θ∗∈argminθ∈Rn f(θ) and δpv,1 ∈[0,1), δpv,2 ≥0. Then, for any k≥0 the iterates produced by Moshpit SGD with γ ≤1/4Lsatisfy E[Vk] ≤2γ2 ( 4δ2 aq + (τ −1)σ2) , (44) where Vk = 1 Nk ∑ i∈Pk ∥θk i −θk∥2 and θk = 1 Nk ∑ i∈Pk θk i. 34Proof. First of all, if k = aτ for some integer a ≥0, then (44) follows from Assumption 3.2 (eq. (10)). Therefore, we consider such k that k = aτ + t′for some t′ ∈(0,τ). Then, for any i,j ∈Pk, i̸= j E [ ∥θk i −θk j∥2 |θk−1] = E [ ∥θk−1 i −γgk−1 i −θk−1 j + γgk−1 j ∥2 |θk−1] (35) = ∥θk−1 i −γ∇f(θk−1 i ) −θk−1 j + γ∇f(θk−1 j )∥2 +γ2E [ ∥gk−1 i −∇f(θk−1 i ) + gk−1 j −∇f(θk−1 j )∥2 |θk−1] . Using Lemma D.1 and independence of gk−1 i and gk−1 j for given θk−1 i ,θk−1 j , i̸= jwe derive E [ ∥θk i −θk j∥2 |θk−1] (37) ≤ (1 −γµ)∥θk−1 i −θk−1 j ∥2 + γ2E [ ∥gk−1 i −∇f(θk−1 i )∥2 |θk−1] +γ2E [ ∥gk−1 j −∇f(θk−1 j )∥2 |θk−1] (7) ≤ (1 −γµ)∥θk−1 i −θk−1 j ∥2 + 2γ2σ2, from which we get the following: Eg [ ∥θk i −θk j∥2] ≤(1 −γµ)Eg [ ∥θk−1 i −θk−1 j ∥2] + 2γ2σ2 ≤Eg [ ∥θk−1 i −θk−1 j ∥2] + 2γ2σ2. Here, Eg[·] denotes the expectation conditioned on {Pk}(a+1)τ−1 k=aτ . Unrolling the recurrence, we get Eg [ ∥θk i −θk j∥2] ≤ Eg [ ∥θaτ i −θaτ j ∥2] + 2(k−aτ)γ2σ2 ≤ Eg [ ∥θaτ i −θaτ j ∥2] + 2(τ −1)γ2σ2. (45) Using this, we estimate Eg[Vk]: Eg[Vk] = 1 Nk ∑ i∈Pk Eg    θk i − 1 Nk ∑ j∈Pk θk j  2  (33) ≤ 1 N2 k ∑ i,j∈Pk Eg [ ∥θk i −θk j∥2] (45) ≤ 1 N2 k ∑ i,j∈Pk Eg [ ∥θaτ i −θaτ j ∥2] + 2(τ −1)γ2σ2 (32) ≤ 2 N2 k ∑ i,j∈Pk ( Eg [ ∥θaτ i −θaτ∥2] + Eg [ ∥θaτ j −θaτ∥2]) + 2(τ −1)γ2σ2 = 4 Nk ∑ i∈Pk Eg [ ∥θaτ i −θaτ∥2] + 2(τ −1)γ2σ2 ≤ 4 Naτ ·Naτ Nk ∑ i∈Paτ Eg [ ∥θaτ i −θaτ∥2] + 2(τ −1)γ2σ2 ≤ Eg [ 8 Naτ ∑ i∈Paτ ∥θaτ i −θaτ∥2 ] + 2(τ −1)γ2σ2, where in the last inequality we use 2N(a+1)τ = 2|P(a+1)τ|≥| Paτ|= Naτ and |Nk|≤| Nk−1| following from Assumption 3.2. Finally, we take the full expectation from the previous inequality: E[Vk] (36) ≤ 8E [ 1 Naτ ∑ i∈Paτ ∥θaτ i −θaτ∥2 ] + 2(τ −1)γ2σ2 (10) ≤2γ2 ( 4δ2 aq + (τ −1)σ2) . This ﬁnishes the proof. Combining Lemmas D.5 and D.6, we get the following result: Theorem D.1 (Theorem 3.3, convergence in the convex case) . Let f1 = ... = fN = f be µ- strongly convex (Def. D.2) and L-smooth (see Def. D.1), and Assumptions 3.1 and 3.2 hold with 35∆k pv = δpv,1γµE[∥θk−θ∗∥2]+γ2δ2 pv,2 and ˜θ= θ∗, where θ∗∈argminθ∈Rn f(θ) and δpv,1 ∈[0,1), δpv,2 ≥0. Then, for any K ≥0, the iterates produced by Moshpit SGD with γ ≤1/4Lsatisfy E [ f(θ K ) −f(θ∗) ] ≤ (1 −γµ(1 −δpv,1))KR2 0 γ +γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) , (46) when µ> 0, and E [ f(θ K ) −f(θ∗) ] ≤ R2 0 γK + γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) , (47) when µ= 0, whereR0 = ∥θ0−θ∗∥, θ K = 1 WK ∑K k=0 wkθk = 1 WK ∑K k=0 wk Nk ∑ i∈Pk θk i, wk = (1− γµ(1 −δpv,1))−(k+1), and WK = ∑K k=0 wk. That is, Moshpit SGD achieves E[f(θ K ) −f(θ∗)] ≤ε after K = ˜O ( L (1 −δpv,1)µ + σ2 Nmin(1 −δpv,1)µε + δ2 pv,2 (1 −δpv,1)µε + √ L((τ −1)σ2 + δ2aq) (1 −δpv,1)2µ2ε ) (48) iterations with γ = min    1 4L, ln ( max { 2,min { R2 0µ2(1−δpv,1)2K2 (δ2 pv,2+σ2/Nmin) , R2 0µ3(1−δpv,1)3K3 3L(4δ2aq+(τ−1)σ2) }}) (1 −δpv,1)µK    when µ> 0, and after K = O  LR2 0 ε + R2 0σ2 Nminε2 + R2 0δ2 pv,2 ε2 + R2 0 √ L((τ −1)σ2 + δ2aq) ε 3/2   (49) iterations with γ = min { 1 4L √ R0 (δ2 pv,2 + σ2 /Nmin)K, 3 √ R2 0 3L ( 4δ2aq + (τ −1)σ2) K } when µ= 0. Proof. Plugging the result of Lemma D.6 in inequality (39) from Lemma D.5, we obtain γE [ f(θk) −f(θ∗) ] ≤ (1 −γµ(1 −δpv,1))E [ ∥θk −θ∗∥2] −E [ ∥θk+1 −θ∗∥2] +3Lγ3 ( 4δ2 aq + (τ −1)σ2) + γ2 ( σ2 Nmin + δ2 pv,2 ) . 36Next, we sum up these inequalities for k= 0,...,K with weights wk = (1 −γµ(1 −δpv,1))−(k+1) and divide both sides by γWK, where WK = ∑K k=0 wk: 1 WK K∑ k=0 wkE [ f(θk) −f(θ∗) ] ≤ 1 γWK K∑ k=0 (1 −γµ(1 −δpv,1))wkE [ ∥θk −θ∗∥2] − 1 γWK K∑ k=0 wkE [ ∥θk+1 −θ∗∥2] +γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) = 1 γWK K∑ k=0 ( wk−1E [ ∥θk −θ∗∥2] −wkE [ ∥θk+1 −θ∗∥2]) +γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) = w−1∥θ0 −θ∗∥2 −wKE [ ∥θK+1 −θ∗∥2] γWK +γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) ≤ ∥θ0 −θ∗∥2 γWK +γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) . Since f is convex, we apply the Jensen’s inquality f ( 1 WK K∑ k=0 wkθk ) ≤ 1 WK K∑ k=0 wkf(θk) to the previous result and get E [ f(θ K ) −f(θ∗) ] ≤ R2 0 γWK + γ ( σ2 Nmin + δ2 pv,2 + 3Lγ ( 4δ2 aq + (τ −1)σ2)) , where R0 = ∥θ0 −θ∗∥and θ K = 1 WK ∑K k=0 wkθk = 1 WK ∑K k=0 wk Nk ∑ i∈Pk θk i. If µ >0, then WK ≥wK ≥(1 −γµ(1 −δpv,1))−K, implying (46). Next, wk = 1 and WK = K when µ = 0 gives (47). It remains to estimate the total number of iterations Krequired by Moshpit SGD to ﬁnd an ε-solution, i.e., to achieve E[f(θ K ) −f(θ∗)] ≤ε. Applying Lemma D.2 to (46), we get the following result: if µ> 0 and γ = min    1 4L, ln ( max { 2,min { R2 0µ2(1−δpv,1)2K2 δ2 pv,2+σ2/Nmin , R2 0µ3(1−δpv,1)3K3 3L(4δ2aq+(τ−1)σ2) }}) (1 −δpv,1)µK    , then E [ f(θ K ) −f(θ∗) ] equals ˜O ( LR2 0 exp ( −µ L(1 −δpv,1)K ) + δ2 pv,2 + σ2 /Nmin (1 −δpv,1)µK + L ( δ2 aq + (τ −1)σ2) (1 −δpv,1)2µ2K2 ) , implying (48). Similarly, we apply Lemma D.3 to (47) and get that for µ= 0 and γ = min { 1 4L √ R0 (δ2 pv,2 + σ2 /Nmin)K, 3 √ R2 0 3L ( 4δ2aq + (τ −1)σ2) K } , 37E [ f(θ K ) −f(θ∗) ] = O  LR2 0 K + √ R2 0(δ2 pv,2 + σ2 /Nmin) K + 3 √ R4 0L ( δ2aq + (τ −1)σ2) K 2/3  , implying (49). D.3 Non-Convex Case In this section, we give the full proof of Theorem 3.4 about convergence of Moshpit SGD for general non-convex problems. The proof follows the similar steps as in the state-of-the-art analysis of Local-SGD in non-convex case [64, 63]. We start with the following lemma: Lemma D.7. Let f1 = ... = fN = f, function f be L-smooth and bounded from below by f∗, and Assumptions 3.1 and 3.2 hold with ∆k pv = δpv,1γE[∥∇f(θk)∥2] + Lγ2δ2 pv,2, δpv,1 ∈[0,1/2), δpv,2 ≥0. Then, for any K ≥0 the iterates produced by Moshpit SGD with γ ≤(1−2δpv,1)/8Lsatisfy (1 −2δpv,1)γ 4 K−1∑ k=0 E [ ∥∇f(θk)∥2] ≤ f(θ0) −f∗+ γL2 K−1∑ k=0 E[Vk] +KLγ2 ( σ2 Nmin + δ2 pv,2 ) , (50) where Vk = 1 Nk ∑ i∈Pk ∥θk i −θk∥2 and θk = 1 Nk ∑ i∈Pk θk i. Proof. Recall that Assumption 3.2 with ∆k pv = δpv,1γE[∥∇f(θk)∥2] + Lγ2δ2 pv,2 states E [ ⟨∇f(θk),θk+1 −ˆθk+1⟩+ L∥ˆθk+1 −θk+1∥2 ] ≤δpv,1γE[∥∇f(θk)∥2] + Lγ2δ2 pv,2, (51) where ˆθk+1 = 1 Nk ∑ i∈Pk (θk i −γgk i). As for the convex case, the deﬁnition of ˆθk+1 implies ˆθk+1 = 1 Nk ∑ i∈Pk θk i − γ Nk ∑ i∈Pk gk i = θk −γgk, where gk = 1 Nk ∑ i∈Pk gk i. Using this and L-smoothness of f, we derive f(θk+1) −f(θk) (29) ≤ ⟨∇f(θk),θk+1 −θk⟩+ L 2 ∥θk+1 −θk∥2 (32) ≤ ⟨∇f(θk),ˆθk+1 −θk⟩+ ⟨∇f(θk),θk+1 −ˆθk+1⟩ +L∥ˆθk+1 −θk∥2 + L∥θk+1 −ˆθk+1∥2 = −γ⟨∇f(θk),gk⟩+ Lγ2∥gk∥2 + ⟨∇f(θk),θk+1 −ˆθk+1⟩ +L∥θk+1 −ˆθk+1∥2, from which it follows that E [ f(θk+1) −f(θk) |θk] ≤ −γ ⟨ ∇f(θk), 1 Nk ∑ i∈Pk ∇f(θk i) ⟩ +E [ ⟨∇f(θk),θk+1 −ˆθk+1⟩| θk ] +E [ L∥θk+1 −ˆθk+1∥2 |θk ] +Lγ2E    1 Nk ∑ i∈Pk gk i  2 |θk  , (52) 38where E [ ·| θk] := E [ ·| Pk,θk i,i ∈Pk ] . Next, we estimate the last three terms in the right-hand side of (52). First of all, −γ ⟨ ∇f(θk), 1 Nk ∑ i∈Pk ∇f(θk i) ⟩ = −γ∥∇f(θk)∥2 −γ ⟨ ∇f(θk), 1 Nk ∑ i∈Pk ∇f(θk i) −∇f(θk) ⟩ (34) ≤ −γ∥∇f(θk)∥2 + γ 2 ∥∇f(θk)∥2 +γ 2  1 Nk ∑ i∈Pk (∇f(θk i) −∇f(θk))  2 (33) ≤ −γ 2 ∥∇f(θk)∥2 + γ 2Nk ∑ i∈Pk ∥∇f(θk i) −∇f(θk)∥2 (28) ≤ −γ 2 ∥∇f(θk)∥2 + γL2 2 Vk, (53) where Vk = 1 Nk ∑ i∈Pk ∥θk i −θk∥2. Secondly, since the stochastic gradients {gk i}i∈Pk are computed independently, we derive Lγ2E    1 Nk ∑ i∈Pk gk i  2 |θk   (35) = Lγ2  1 Nk ∑ i∈Pk ∇f(θk i)  2 +Lγ2E    1 Nk ∑ i∈Pk (gk i −∇f(θk i))  2 |θk   (33) ≤ 2Lγ2  1 Nk ∑ i∈Pk (∇f(θk i) −∇f(θk))  2 +2Lγ2∥∇f(θk)∥2 +γ2L N2 k ∑ i∈Pk E [ ∥gk i −∇f(θk i)∥2 |θk] (33),(7) ≤ 2γ2L Nk ∑ i∈Pk ∥∇f(θk i) −∇f(θk)∥2 +2Lγ2∥∇f(θk)∥2 + γ2Lσ2 Nk (28) ≤ 2L3γ2 Nk ∑ i∈Pk ∥θk i −θk∥2    2L3γ2Vk +2Lγ2∥∇f(θk)∥2 +γ2Lσ2 Nmin . (54) Plugging (53) and (54) in (52), we obtain E [ f(θk+1) −f(θk) |θk] ≤ −γ 2 (1 −4Lγ) ∥∇f(θk)∥2 + γL2 2 (1 + 4Lγ) Vk + Lγ2σ2 Nmin +E [ ⟨∇f(θk),θk+1 −ˆθk+1⟩+ L∥θk+1 −ˆθk+1∥2 |θk ] . 39Next, we take the full expectation from the both sides of the above inequality, apply the tower property (36) and take into account that γ ≤(1−2δpv,1)/8L: E [ f(θk+1) −f(θk) ] ≤ −γ 2 (1 −4Lγ) E [ ∥∇f(θk)∥2] + γL2 2 (1 + 4Lγ) E[Vk] + Lγ2σ2 Nmin +E [ ⟨∇f(θk),θk+1 −ˆθk+1⟩+ L∥θk+1 −ˆθk+1∥2 ] (51) ≤ −γ 2 (1 −2δpv,1 −4Lγ) E [ ∥∇f(θk)∥2] + γL2 2 (1 + 4Lγ) E[Vk] +Lγ2 ( σ2 Nmin + δ2 pv,2 ) ≤ −(1 −2δpv,1)γ 4 E [ ∥∇f(θk)∥2] + γL2E[Vk] +Lγ2 ( σ2 Nmin + δ2 pv,2 ) . Summing up the obtained inequalities for k= 0,...,K −1 and rearranging the terms, we derive (1 −2δpv,1)γ 4 K−1∑ k=0 E [ ∥∇f(θk)∥2] ≤ K−1∑ k=0 E [ f(θk) −f(θk+1) ] + γL2 K−1∑ k=0 E[Vk] +KLγ2 ( σ2 Nmin + δ2 pv,2 ) = f(θ0) −E[f(θK)] + γL2 K−1∑ k=0 E[Vk] +KLγ2 ( σ2 Nmin + δ2 pv,2 ) ≤ f(θ0) −f∗+ γL2 K−1∑ k=0 E[Vk] +KLγ2 ( σ2 Nmin + δ2 pv,2 ) , where f∗is a uniform lower bound for f. The next step towards completing the proof of Theorem 3.4 gives the upper bound for ∑K−1 k=0 E[Vk] that appeared in (50). Lemma D.8. Let f1 = ... = fN = f be L-smooth and bounded from below by f∗, and Assump- tions 3.1 and 3.2 hold with ∆k pv = δpv,1γE[∥∇f(θk)∥2] + Lγ2δ2 pv,2, δpv,1 ∈[0,1/2), δpv,2 ≥0. Then, for any K ≥0 the iterates produced by Moshpit SGD with γ ≤1/(4√eL(τ−1)) satisfy K−1∑ k=0 E[Vk] ≤ 8eγ2(τ −1)2 K−1∑ k=0 E[∥∇f(θk)∥2] + 4γ2K ( 2δ2 aq + e(τ −1)σ2) , (55) where Vk = 1 Nk ∑ i∈Pk ∥θk i −θk∥2 and θk = 1 Nk ∑ i∈Pk θk i. 40Proof. First of all, consider k such that k = aτ + t′for some t′ ∈[0,τ). Let Eg[·] denote the expectation conditioned on {Pt}(a+1)τ−1 t=aτ . Then Eg[Vk] = 1 Nk ∑ i∈Pk Eg [ ∥θk i −θk∥2](35) ≤ 1 Nk ∑ i∈Pk Eg [ ∥θk i −θaτ∥2] = 1 Nk ∑ i∈Pk Eg   θaτ i −θaτ −γ k−1∑ t=aτ gt i  2  (32) ≤ 2 Nk ∑ i∈Pk Eg [ ∥θaτ i −θaτ∥2] + 2γ2 Nk ∑ i∈Pk Eg    k−1∑ t=aτ gt i  2 . (56) Next, we estimate the second term in the right-hand side of (56) using Lemma D.4: 2γ2 Nk ∑ i∈Pk Eg    k−1∑ t=aτ gt i  2  (38) ≤ 2eγ2(k−aτ) Nk ∑ i∈Pk k−1∑ t=aτ Eg[∥∇f(θt i)∥2] +2eγ2 Nk ∑ i∈Pk k−1∑ t=aτ Eg[∥gt i −∇f(θt i)∥2] (32),(7) ≤ 4eγ2(τ −1) k−1∑ t=aτ Eg[∥∇f(θt)∥2] +4eγ2(τ −1) k−1∑ t=aτ 1 Nk ∑ i∈Pk Eg[∥∇f(θt i) −∇f(θt)∥2] +2eγ2(k−aτ)σ2 (28) ≤ 4eγ2(τ −1) k−1∑ t=aτ Eg[∥∇f(θt)∥2] +4eγ2L2(τ −1) k−1∑ t=aτ Nt Nk · 1 Nt ∑ i∈Pt Eg[∥θt i −θt∥2] +2eγ2(τ −1)σ2 ≤ 4eγ2(τ −1) k−1∑ t=aτ Eg[∥∇f(θt)∥2] +8eγ2L2(τ −1) k−1∑ t=aτ Eg[Vt] + 2eγ2(τ −1)σ2, where in the last two inequalities we use Nk = |Pk| ≤ |Pk−1| = Nk−1 for all k ≥ 1 and Naτ ≤2N(a+1)τ for all integer a≥0. Plugging this inequality in (56) and taking the full expectation 41from the result, we get E[Vk] ≤ 2E [ 1 Nk ∑ i∈Pk ∥θaτ i −θaτ∥2 ] + 4eγ2(τ −1) k−1∑ t=aτ E[∥∇f(θt)∥2] +8eγ2L2(τ −1) k−1∑ t=aτ E[Vt] + 2eγ2(τ −1)σ2 ≤ 4E [ 1 Naτ ∑ i∈Paτ ∥θaτ i −θaτ∥2 ] + 4eγ2(τ −1) k−1∑ t=aτ E[∥∇f(θt)∥2] +8eγ2L2(τ −1) k−1∑ t=aτ E[Vt] + 2eγ2(τ −1)σ2 (10) ≤ 4eγ2(τ −1) k−1∑ t=aτ E[∥∇f(θt)∥2] + 8eγ2L2(τ −1) k−1∑ t=aτ E[Vt] +2γ2 ( 2δ2 aq + e(τ −1)σ2) , where in the second inequality we also use Nk = |Pk|≤| Pk−1|= Nk−1 for all k≥1 and Naτ ≤ 2N(a+1)τ for all integer a≥0. Summing up the obtained inequalities for k = aτ,aτ + 1,...,K ′ for some K′∈[aτ,(a+ 1)τ −1] we derive K′ ∑ k=aτ E[Vk] ≤ 4eγ2(τ −1) K′ ∑ k=aτ k−1∑ t=aτ E[∥∇f(θt)∥2] + 8eγ2L2(τ −1) K′ ∑ k=aτ k−1∑ t=aτ E[Vt] +2γ2(K′−aτ + 1) ( 2δ2 aq + e(τ −1)σ2) ≤ 4eγ2(τ −1)2 K′ ∑ k=aτ E[∥∇f(θk)∥2] + 8eγ2L2(τ −1)2 K′ ∑ k=aτ E[Vk] +2γ2(K′−aτ + 1) ( 2δ2 aq + e(τ −1)σ2) ≤ 4eγ2(τ −1)2 K′ ∑ k=aτ E[∥∇f(θk)∥2] + 1 2 K′ ∑ k=aτ E[Vk] +2γ2(K′−aτ + 1) ( 2δ2 aq + e(τ −1)σ2) , where in the last inequality we use γ ≤1/(4√eL(τ−1)). Rearranging the terms, we get that for K′≥0 K′ ∑ k=aτ E[Vk] ≤ 8eγ2(τ −1)2 K′ ∑ k=aτ E[∥∇f(θk)∥2] + 4γ2(K′−aτ + 1) ( 2δ2 aq + e(τ −1)σ2) , where a≥0 is an integer such that aτ ≤K′≤(a+ 1)τ −1. Summing up the obtained inequalities for K′= τ −1,2τ −1,...,τ ⌊(K−1)/τ⌋−1,K −1, we derive (55). Combining Lemmas D.7 and D.8, we get the following result: Theorem D.2 (Theorem 3.4). Let f1 = ... = fN = f, function f be L-smooth and bounded from below by f∗, and Assumptions 3.1 and 3.2 hold with ∆k pv = δpv,1γE[∥∇f(θk)∥2] + Lγ2δ2 pv,2, δpv,1 ∈[0,1/2), δpv,2 ≥0. Then, for any K ≥0 the iterates produced by Moshpit SGD with γ ≤min { 1 −2δpv,1 8L , √ 1 −2δpv,1 8√eL(τ −1) } satisfy E [ ∥∇f(θK rand)∥2] ≤ 8∆0 (1 −2δpv,1)Kγ + 8Lγ 1 −2δpv,1 ( σ2 Nmin + δ2 pv,2 + 4γL ( 2δ2 aq + e(τ −1)σ2)) , (57) 42where ∆0 = f(θ0) −f∗and θK rand is chosen uniformly at random from {θ0,θ1,...,θ K−1}. That is, Moshpit SGD achieves E [ ∥∇f(θK rand)∥2] ≤ε2 after O ( L∆0 (1 −2δpv,1)2ε2 [ 1 + (τ −1) √ 1 −2δpv,1 + δ2 pv,2 + σ2 /Nmin ε2 + √ (1−2δpv,1)(δ2aq+(τ−1)σ2) ε ]) (58) iterations with γ = min { 1 −2δpv,1 8L , √ 1 −2δpv,1 8√eL(τ −1), √ ∆0 LK ( δ2 pv,2 + σ2 /Nmin ), 3 √ ∆0 4L2 ( 2δ2aq + e(τ −1)σ2) } . Proof of Theorem 3.4. Plugging the result of Lemma D.8 in the inequality (50) from Lemma D.7, we obtain (1 −2δpv,1)γ 4 K−1∑ k=0 E [ ∥∇f(θk)∥2] ≤ f(θ0) −f∗+ 8eγ3L2τ(τ −1) K−1∑ k=0 E[∥∇f(θk)∥2] +KLγ2 ( σ2 Nmin + δ2 pv,2 ) +4KL2γ3 ( 2δ2 aq + e(τ −1)σ2) ≤ f(θ0) −f∗+ (1 −2δpv,1)γ 8 K−1∑ k=0 E [ ∥∇f(θk)∥2] +KLγ2 ( σ2 Nmin + δ2 pv,2 ) +4KL2γ3 ( 2δ2 aq + e(τ −1)σ2) . Next, 1 K K∑ k=0 E [ ∥∇f(θk)∥2] ≤ 8∆0 (1 −2δpv,1)Kγ + 8Lγ 1 −2δpv,1 ( σ2 Nmin + δ2 pv,2 + 4γL ( 2δ2 aq + e(τ −1)σ2)) , where ∆0 = f(θ0) −f∗. Since θK rand is chosen uniformly at random from {θ0,θ1,...,θ K−1}, E [ ∥∇f(θK rand)∥2](36) = 1 K K∑ k=0 E [ ∥∇f(θk)∥2] and (57) holds. Applying Lemma D.3 to (57), we get the following result: if γ = min { 1 −2δpv,1 8L , √ 1 −2δpv,1 8√eL(τ −1), √ ∆0 LK ( δ2 pv,2 + σ2 /Nmin ), 3 √ ∆0 4L2 ( 2δ2aq + e(τ −1)σ2) } , then E [ ∥∇f(θK rand)∥2] equals O  L∆0 ( 1+(τ−1) √ 1−2δpv,1 ) (1−2δpv,1)2K + √ L∆0 ( δ2 pv,2+σ2 /Nmin ) (1−2δpv,1)2K + 3 √ L2∆2 0(δ2aq+(τ−1)σ2) (1−2δpv,1)K 2/3  , which implies the desired convergence result from (58). 43E Decentralized matchmaking In order to run group all-reduce over unreliable devices, Moshpit Averaging must be able to dynami- cally form groups of active devices that share the same key Ci. In theory, this matchmaking can be implemented precisely as described in Algorithm 1: each peer adds itself to a certain DHT key, waits for a said period of time, and then reads the same key to retrieve a list of its groupmates. However, in practice, this kind of matchmaking would be extremely fragile: if any peer arrives late (for example, due to latency), it may join the group when other peers have already ﬁnished matchmaking. As a result, some workers will treat this peer as active, while others will behave as though there is no such peer at all, breaking the consensus and rendering all peers unable to run all-reduce in a stable manner. To avoid this and other similar inconsistencies, Moshpit All-Reduce employs a more sophisticated matchmaking protocol with the following guarantees 1. Peers that join the same group are guaranteed to have the same list of groupmates; 2. The group will have the maximum possible number of peers, unless some of them fail; 3. If some peers fail, matchmaking will still form the group out of the remaining ones. To achieve this, each peer ﬁrst declares itself onto the DHT (as in Algorithm 1). Then, peers attempt to form groups by calling the REQUEST_JOIN_GROUP remote procedure call. Intuitively, if peer A calls this RPC on peer B, then peer A requests to join peer B’s group, which can be either accepted or rejected by the group “leader” B, which may or may not have other “followers”. If a peer is accepted to a group, it commits to stay active (i.e. to await other peers) for a set period of time and perform all-reduce with the peers supplied by the group “leader”. On the other hand, a peer can be rejected if (a) the potential “leader” is already a follower in another group, (b) the group is already running all-reduce, or (c) if the “leader” failed or left during matchmaking. To ensure that this protocol forms groups of maximum size, each peer generates a unique “priority” based on its local timestamp9. Peers prioritize joining the group of neighbors that have the lowest “priority”. Under normal circumstances, all workers will join the group of a peer that was ﬁrst to start matchmaking according to its own local time. However, if this peer has failed or already ﬁnished matchmaking, the group will be formed around one of the remaining peers. Matchmaking for 64 peers can take less than 1 second if all workers are located in the same cloud region and are highly synchronized. However, this can grow to 2.9 seconds for two different cloud regions and up to 9 seconds when training with commodity hardware around the world. To ensure that this latency does not affect the training performance, Moshpit SGD performs matchmak- ing asynchronously in the background thread, while the model is accumulating gradients. All peers begin matchmaking 15 seconds before the estimated averaging round, so that in ≥95% of averaging iterations, the matchmaking step is already ﬁnished by the time peers need to run all-reduce. F Training with a dynamic number of peers Many practical setups with unreliable devices allow peers to join or leave at any time, which can produce undesirable side-effects. For instance, consider a participant that joins the “swarm” midway through the training process. If this participant starts with the initial model parameters, it can undo some of the progress made by other peers. To circumvent this issue, we require each new participant to download the latest parameters from a random up-to-date peer discovered through DHT. The same technique is used to synchronize the optimizer statistics and the learning rate schedule. This protocol is also triggered if a peer becomes desynchronized with others, e.g., after a network freeze. 9More speciﬁcally, the priority is a tuple of (timestamp, peer_id), where peer_id is used to break ties. 44G Load balancing via linear programming When running Moshpit Averaging on heterogeneous devices, one must regularly perform Butterﬂy All-Reduce among peers with uneven network bandwidth. In order to speed up the protocol, we can make low-throughput peers receive, average, and send smaller partitions of the averaged vector; conversely, the high-throughput peers can process greater fractions of the input vector. To compute the optimal partitioning, peers must solve an optimization problem that minimizes the total time spent on communication during all-reduce. Consider a group of M peers with network bandwidths b1,...,b M, deﬁned for simplicity as the minimum of the upload and download speed for each peer. Our objective is to ﬁnd wi — a fraction of all input vectors to be processed by the i-th peer. In Butterﬂy All-Reduce, each peer isplits its vector into parts and sends these parts to corresponding peers. Since there is no need to send wi to itself, i-th peer will upload a total of 1 −wi of the vector to its peers. On the receiving side, peer iwill average wi of the vector from all peers in its group. To do so, it must download M−1 vector parts of size wi from all other peers. After that, peers distribute the averaged parts by running the same procedure in reverse (see Figure 1). Thus, the communication time for each peer is proportional to ti = (1 −wi + (M −1)wi) · 1 bi and the total runtime of Butterﬂy All-Reduce is the maximum communication time over all peers: T = maxiti = maxi(1 −wi + (M −1)wi) ·1 bi . Formally, we minimize T with respect to wi with two constraints on the fraction weights: min w max i (1 −wi+(M −1)wi) ·1 bi subject to M∑ i=1 wi = 1 wi ≥0 ∀i= 1,...,M Because the functions being maximized and the constraints are linear in wi, this problem can be reduced to linear programming [125]. Namely, we can minimize a surrogate variable ξ such that ∀i, ξ≥(1 −wi + (M −1) ·wi) ·1 bi . The resulting linear program is formulated as follows: min w,ξ ξ subject to M∑ i=1 wi = 1 wi ≥0 ∀i= 1,...,M ξ≥(1−wi + (M −1)wi) ·1 bi ∀i= 1,...,M We solve this problem using the interior point method [ 126] implemented as part of the SciPy package (scipy.optimize.linprog). Note that depending on the conditions given by participant bandwidth, optimal weights of speciﬁc peers might be equal to 0 in some cases. In essence, this allows our method to smoothly interpolate between data parallelism [9], parameter server [18] and sharded parameter server [25] in manner similar to BytePS [26]. H Detailed experimental setup In this section, we provide the detailed hardware conﬁguration of servers used for each of our distributed training experiments. H.1 ImageNet training Both homogeneous and heterogeneous training setups for ImageNet are provisioned in our on-premise infrastructure across multiple data centers and an ofﬁce space (for the heterogeneous setup only). 45Homogeneous. For the homogeneous setup, we use 16 identical instances with the following speciﬁcations: • GPU: V100-PCIe, • CPU: 6 vCPUs (Xeon E5-2650v4), • RAM: 64GB. Heterogeneous. In turn, the heterogeneous setup contains multiple instance types listed in Table 2: Table 2: Heterogeneous setup for ImageNet training. Instances GPUs GPU type Cores RAM, GB CPU type 4 1 V100-PCIe 6 64 E5-2650v4 17 2 GTX 1080Ti 8 64 E5-2650v4 7 1 GTX 1080Ti 4 32 E5-2650v4 16 1 P40 4 32 E5-2667v2 20 1 M40-24GB 4 32 E5-2667v2 H.2 ALBERT training Homogeneous. For the homogeneous setup, we use a single virtual machine with the following speciﬁcations: • GPU: 8×V100-PCIe, • CPU: 48 vCPUs (Xeon E5-2650v4), • RAM: 488GB. At the time of writing, the cloud rent cost for this instance is $24.48 per hour. Heterogeneous. Our heterogeneous setup is composed of two parts: AWS EC2 Spot instances and crowdsourced machines from the Vast.ai marketplace. For spot instances, we picked the smallest suitable instance size available from the cloud provider and further limited their bandwidth to 1Gb/s10. As for marketplace instances, we report the hardware speciﬁcations for each worker gathered 1 hour after the start of ALBERT training. Since both cloud and marketplace instances are preemptible, the actual cost of the server ﬂeet will vary based on the current price. For simplicity, we report the maximum hourly price we ended up paying for this instance (enforced via maximum bid). Finally, some marketplace instances have missing speciﬁcations, such as unknown CPU type. This is likely caused by non-standard virtualization conﬁgured by the device owner. The resulting ﬂeet conﬁguration, shown in Table 3, costs up to $15.43/hour, depending on the number of active instances. I Additional averaging experiments In this section, we evaluate the averaging precision with the same methodology as in 4.1, but for multiple different worker conﬁgurations. Table 4 provides the complete results of our experiments that were used to make conclusions in the main experimental section: instead of reporting the mean squared error for different iterations, we provide the number of rounds that was required to achieve the error of 10−9 and 10−4. In Figure 5, plots 1–5 explore several combinations of grid sizes and failure rates, whereas plot 6 (bottom right) demonstrates a setup with the same number of peers ( 106) arranged into several different grid sizes and its relation to convergence. Note that M=32 outperforms the alternatives only for the speciﬁc failure rate of 0.001. 10We use tc qdisc Linux utility to artiﬁcially limit the network throughput, similarly to [127] 46Table 3: Heterogeneous setup for ALBERT training. GPU Cores RAM, GB CPU type Download, Mb/s Upload, Mb/s Cost, $/hour Preemptible g4dn.xlarge instances (32×) T4 4 16 Xeon Platinum 8259CL 1000 1000 0.1578 Marketplace instances GTX 1070Ti 6 16 E5-2640 425 255 0.036 GTX 1070Ti 6 16 i3-6100T 121 36 0.06 GTX 1080Ti 4 20 i3-6096P 817 308 0.101 GTX 1080Ti 20 129 E5-2630v4 660 475 0.182 GTX 1080Ti 1 16 i7-7700K 245 210 0.302 GTX 1080Ti 48 97 Xeon Platinum 8124 583 539 0.217 GTX 1080Ti 10 16 Unknown n/a n/a 0.15 GTX 1080Ti 4 16 Xeon Gold 6149 98 100 0.2 GTX 1080Ti 4 16 Xeon Gold 6149 99 98 0.2 GTX 1080Ti 4 16 Xeon Gold 6149 99 99 0.2 GTX 1080Ti 4 16 Xeon Gold 6149 99 99 0.2 RTX 2070S 24 32 E5-2620v2 199 25 0.199 RTX 2070S 32 97 E5-2650 162 64 0.285 RTX 2080 6 16 E5-2620v3 271 287 0.25 RTX 2080 24 32 E5-2630v3 199 25 0.302 RTX 2080S 4 32 E5-2697v4 101 99 0.292 RTX 2080S 4 32 E5-2697v4 93 99 0.292 RTX 2080S 4 32 E5-2697v4 94 98 0.292 RTX 2080S 4 32 E5-2697v4 94 98 0.292 RTX 2080S 4 32 E5-2697v4 100 99 0.292 RTX 2080Ti 4 16 Ryzen Threadripper 3960x 279 271 0.35 RTX 2080Ti 8 129 E5-2670v3 616 672 0.201 RTX 2080Ti 6 32 E5-2620v3 217 61 0.22 RTX 2080Ti 8 16 E5-2697v2 100 58 0.3 RTX 2080Ti 8 21 E5-2697v2 145 49 0.243 RTX 2080Ti 12 32 Unknown 111 92 0.326 RTX 2080Ti 12 64 E5-2690v3 205 61 0.549 RTX 3080 16 16 i7-10700K 69 49 0.462 RTX 3090 14 32 E5-2695v3 93 37 0.498 RTX 3090 16 32 Ryzen 9 3950X 338 38 0.511 Titan RTX 4 32 Xeon W-3223 321 115 1 Titan RTX 4 32 Xeon Gold 6149 99 100 0.702 Titan V 8 32 i7-7700K 97 50 0.282 V100-FHHL 8 60 Xeon Gold 6148 544 584 0.39 Total hourly cost (as listed): 15.43 J Additional image classiﬁcation experiments Aside from the two evaluation scenarios provided in 4.2, we also measure the performance of Moshpit-SGD in a non-distributed setup, i.e. on a single server with multiple GPUs. We conduct this experiment on the same 8×V100 machine that was used in the homogeneous setup for training ALBERT (see Appendix H.2). As Figure 6 demonstrates, Moshpit SGD is slower than AR-SGD by approximately 25%. This result is expected, since our implementation of Moshpit All-Reduce is more general and communicates over a TCP connection, whereas AR-SGD uses direct peer-to-peer GPU communication over PCIe. On average, this incurs a slowdown of 27% in terms of training time. 47Table 4: Averaging performance of different algorithms. Values denote the number of iterations required to achieve the error of 10−9 (10−4 in parentheses), the best result is in bold. N p All-Reduce Gossip PushSum Random groups Moshpit 512 0 1.0 (1.0) 50.0 (50.0) 47.6 (15.6) 6.1 (3.0) 8.2 (3.5) 512 0.001 1.6 (1.6) 50.0 (50.0) 47.6 (15.6) 6.3 (3.0) 8.1 (3.7) 512 0.005 10.9 (10.9) 50.0 (50.0) 47.8 (15.6) 6.3 (3.0) 8.7 (3.9) 512 0.01 41.7 (41.7) 50.0 (50.0) 47.8 (15.6) 6.6 (3.0) 9.1 (3.9) 768 0 1.0 (1.0) 50.0 (50.0) 43.2 (13.8) 6.2 (3.0) 6.0 (3.0) 768 0.001 1.8 (1.8) 50.0 (50.0) 43.2 (13.8) 6.5 (3.0) 6.2 (3.0) 768 0.005 28.7 (28.7) 50.0 (50.0) 43.2 (14.1) 6.6 (3.0) 6.6 (3.0) 768 0.01 50.0 (50.0) 50.0 (50.0) 43.9 (14.2) 7.0 (3.0) 6.8 (3.0) 900 0 1.0 (1.0) 50.0 (50.0) 45.0 (14.7) 6.4 (3.0) 5.0 (2.8) 900 0.001 1.8 (1.8) 50.0 (50.0) 45.0 (14.7) 6.3 (3.0) 5.5 (3.0) 900 0.005 50.0 (50.0) 50.0 (50.0) 45.2 (14.7) 6.7 (3.0) 5.9 (3.0) 900 0.01 50.0 (50.0) 50.0 (50.0) 45.6 (14.9) 7.0 (3.1) 6.4 (3.1) 1024 0 1.0 (1.0) 50.0 (50.0) 49.0 (16.2) 6.2 (3.0) 2.0 (2.0) 1024 0.001 2.0 (2.0) 50.0 (50.0) 49.0 (16.3) 6.5 (3.0) 3.4 (2.2) 1024 0.005 42.6 (42.6) 50.0 (50.0) 49.5 (16.3) 6.7 (3.0) 5.4 (2.9) 1024 0.01 50.0 (50.0) 50.0 (50.0) 49.5 (16.3) 6.9 (3.1) 5.9 (3.0) 0 5 10 15 20 25 10 15 10 13 10 11 10 9 10 7 10 5 10 3 10 1 Mean squared error Grid 8x8 N=50, p=0 N=50, p=0.005 N=50, p=0.01 N=50, p=0.02 0 5 10 15 20 25 30 10 15 10 13 10 11 10 9 10 7 10 5 10 3 10 1 Grid 8x8x8 N=470, p=0 N=470, p=0.001 N=470, p=0.0025 N=470, p=0.0075 N=470, p=0.01 0 5 10 15 20 25 30 10 15 10 13 10 11 10 9 10 7 10 5 10 3 10 1 Grid 8x8x8x8 N=3700, p=0 N=3700, p=0.001 N=3700, p=0.005 N=3700, p=0.01 0 5 10 15 20 25 Moshpit All-Reduce steps 10 15 10 13 10 11 10 9 10 7 10 5 10 3 10 1 Mean squared error Grid 32x32x32 N=29 000, p=0 N=29 000, p=0.001 N=29 000, p=0.002 N=29 000, p=0.004 0 5 10 15 20 25 Moshpit All-Reduce steps 10 15 10 13 10 11 10 9 10 7 10 5 10 3 10 1 Grid 256x256 N=29 000, p=0 N=29 000, p=0.001 N=29 000, p=0.002 N=29 000, p=0.004 0 20 40 60 80 Moshpit All-Reduce steps 10 15 10 13 10 11 10 9 10 7 10 5 10 3 10 1 Varying grid size Grid 1024^2, N=10^6, p=0.001 Grid 32^4, N=10^6, p=0.001 Grid 4^10, N=10^6, p=0.001 Figure 5: Averaging error of Moshpit All-Reduce as a function of the iteration number for different conﬁgurations and failure rates. 480h 8h 16h 24h 32h 40h 48h 56h 64h Training time (hours) 0% 25% 50% 75%Top-1 validation accuracy 42.8 53.2 AR-SGD, local Moshpit SGD, local 0 15 30 45 60 75 90 Training epochs 0% 25% 50% 75% AR-SGD, local Moshpit SGD, local Figure 6: ResNet-50 top-1 validation accuracy on ImageNet when training on a single node with 8× V100-PCIe GPUs. (Left) Convergence in terms of training time, (Right) Convergence in terms of training epochs 49",
      "meta_data": {
        "arxiv_id": "2103.03239v4",
        "authors": [
          "Max Ryabinin",
          "Eduard Gorbunov",
          "Vsevolod Plokhotnyuk",
          "Gennady Pekhimenko"
        ],
        "published_date": "2021-03-04T18:58:05Z",
        "pdf_url": "https://arxiv.org/pdf/2103.03239v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of distributed deep neural network training on heterogeneous and unreliable devices with unstable network conditions, where traditional communication-efficient protocols like All-Reduce are fragile, and gossip-based methods suffer from slow convergence. The main contribution is \"Moshpit All-Reduce\", a novel decentralized iterative averaging protocol that dynamically organizes participants into small, independent groups using a decentralized matchmaking algorithm over Distributed Hash Tables (DHT). This protocol achieves exponential convergence, independent of network topology and size, combining All-Reduce's efficiency with gossip-based methods' fault tolerance. Based on this, \"Moshpit SGD\" is proposed for distributed optimization, with strong theoretical guarantees demonstrating convergence rates comparable to Centralized (Local) SGD under realistic assumptions. Empirically, Moshpit SGD showed significant speedups: 1.3x faster ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5x faster ALBERT-large pretraining on preemptible cloud VMs.",
        "methodology": "The core methodology involves Moshpit All-Reduce, a decentralized averaging protocol. Workers dynamically form small groups using a decentralized matchmaking algorithm built on Distributed Hash Tables (DHT), where each worker computes a group key based on chunk indices from previous rounds. Once groups are formed, a butterfly-like All-Reduce protocol is executed within each group to compute the average. For distributed optimization, Moshpit SGD combines local SGD steps with periodic parameter synchronization using Moshpit All-Reduce. The system incorporates fault recovery through DHT replication, allowing new workers to download the latest model and metadata, and dynamically adjusts communication load for heterogeneous devices by solving a linear program to optimize partition sizes. For adaptive optimizers like LAMB, \"pseudo-gradients\" are recovered after averaging to update optimizer statistics. Theoretical analysis covers correctness, mixing properties (exponential convergence rate for averaging), and convergence rates for convex and non-convex problems under assumptions like bounded variance and controlled peer vanishing.",
        "experimental_setup": "The evaluation involved three main experiment sets. 1. **Decentralized Averaging**: Scalar values from a standard Gaussian distribution were used, varying the number of workers (512-1024) and failure rates (0-0.01). Moshpit Averaging (with M=32, d=2) was compared against All-Reduce (with restarts), Gossip, PushSum, and Random groups, measuring mean squared difference from the global average. 2. **Image Classification**: ResNet-50 was trained on ImageNet using SGD with Nesterov momentum. Two setups: a 'homogeneous' setup with 16 Tesla V100 GPUs and a 'heterogeneous' setup with 81 mixed GPUs (V100, 1080Ti, P40) across 64 servers with injected network latency (100ms mean exponential distribution). Moshpit SGD used 2D grids (4 or 8 groups) and was compared to All-Reduce SGD, Asynchronous Decentralized Parallel SGD, and Stochastic Gradient Push, measuring Top-1 validation accuracy vs. training time. 3. **Masked Language Model Pretraining**: ALBERT-large was pretrained on BookCorpus using LAMB optimizer. Two setups: a 'homogeneous' cloud instance with 8 Tesla V100 GPUs and a 'heterogeneous' preemptible cloud setup with 66 mixed GPUs (T4 and others) across 3 continents with varying bandwidth. Moshpit SGD used 2 rounds of Moshpit All-Reduce (M=8, d=2) and adapted pseudo-gradients for LAMB, measuring full training loss vs. training time.",
        "limitations": "The ideal scenario for Moshpit All-Reduce, where N=M^d and all participants are active, provides exact averaging in 'd' rounds; however, this structure is difficult to maintain with dynamically joining, leaving, or failing participants, potentially leading to unequal group sizes and affecting convergence. While robust, the matchmaking protocol can introduce latency (up to 9 seconds in global heterogeneous setups), necessitating asynchronous execution. In a non-distributed (single-node, multi-GPU) setup, Moshpit SGD is about 25% slower than traditional All-Reduce SGD due to using TCP connections instead of direct peer-to-peer GPU communication over PCIe. The theoretical convergence analysis relies on specific assumptions, such as bounded variance for stochastic gradients and properties of averaging quality. Furthermore, the practical implementation for adaptive optimizers like LAMB required specific workarounds (recovering \"pseudo-gradients\") rather than direct compatibility.",
        "future_research_directions": "Future research directions include exploring additional application settings for Moshpit All-Reduce, such as collaborative training of neural networks. Another promising area is to study the interaction and integration of Moshpit All-Reduce with other methods aimed at improving communication efficiency in distributed optimization, specifically gradient compression techniques. Finally, there is scope for improving the dynamic group arrangement mechanism to better address practical challenges arising from varying numbers of workers and their geographical distribution, potentially optimizing group formation and communication patterns further."
      }
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"FedMPQ suffers a noticeable accuracy drop when the number of local epochs per communication round is small (e.g., 1).  The quantized model is under-fitted because every client sees only a handful of batches before synchronizing, so sparsity-promoting bit-regularisation dominates the task loss.\\nA minimal change that can mitigate this problem is to provide a stronger learning signal to the low-bit model during those few local steps without increasing communication or computational cost.\",\n    \"Methods\": \"We propose FedMPQ-KD (Mixed-Precision Quantisation with in-round Knowledge Distillation).\\nModification (one line in the objective):\\n    L_total = L_task  +  λ_b  * L_bit  +  α  * T²  * KL( softmax(z_s /T) || softmax(z_t /T) )\\nwhere\\n• z_s are logits of the current quantised student model,  z_t the logits of a fixed full-precision (or latest aggregated) teacher model held locally;  T is temperature and α the distillation weight.\\n\\nProcedure per client (changes in bold):\\n1. Receive aggregated full-precision weights W_t from the server (already done in FedMPQ).\\n2. Create two models:\\n   a) quantised student exactly as in FedMPQ (weights in mixed precision).\\n   b) ****freeze a copy of W_t in full precision as teacher****.\\n3. For E local epochs, optimise the student with the extended loss above.  The teacher only produces logits; no back-prop.\\n4. Send student weight updates as usual.\\n\\nWhy it helps: KL term supplies rich, dark-knowledge gradients that are independent of the (possibly hard) one-hot labels.  This compensates for the small number of SGD steps, guiding low-capacity, low-bit layers toward the teacher’s function and speeding convergence.  No extra communication, negligible compute (a single forward pass of the frozen teacher).\",\n    \"Experimental Setup\": \"Goal: verify that FedMPQ-KD closes the performance gap when only 1 local epoch is used.\\n• Dataset: CIFAR-10 (10 clients, α=0.5 Dirichlet split).\\n• Network: ResNet-20.\\n• Budgets: average 4-bit, mixed across layers as in the original paper.\\n• Baselines:  (1) FedMPQ (original)  (2) FedMPQ-KD (ours).\\n• Hyper-parameters:  λ_b =0.01 (unchanged),  α=0.5,  T=2.\\n• Training: 50 communication rounds, 1 local epoch, batch-size 64, SGD lr 0.1.\\n• Metric: global test accuracy after every round.\\nExpected observation window: accuracy vs. rounds and final accuracy.\",\n    \"Experimental Code\": \"# Core change: additional KD loss inside the local-training loop\\nimport torch, torch.nn as nn, torch.nn.functional as F\\n\\ndef kd_loss(logits_student, logits_teacher, T=2.0):\\n    \\\"\\\"\\\"KL divergence with temperature.\\\"\\\"\\\"\\n    p_s = F.log_softmax(logits_student / T, dim=1)\\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\\n    return F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\\n\\nclass LocalTrainer:\\n    def __init__(self, student, teacher, dataloader, lr=0.1, lambda_b=0.01, alpha=0.5):\\n        self.student = student  # quantised weights already applied\\n        self.teacher = teacher.eval()  # full-precision copy, frozen\\n        self.opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\\n        self.dl = dataloader\\n        self.lambda_b = lambda_b\\n        self.alpha = alpha\\n\\n    def train_one_epoch(self):\\n        self.student.train()\\n        for x, y in self.dl:\\n            logits_s = self.student(x)\\n            logits_t = self.teacher(x)\\n            loss_task = F.cross_entropy(logits_s, y)\\n            loss_bit  = self.student.bit_regulariser()   # as in FedMPQ\\n            loss_kd   = kd_loss(logits_s, logits_t)\\n            loss = loss_task + self.lambda_b*loss_bit + self.alpha*loss_kd\\n            self.opt.zero_grad(); loss.backward(); self.opt.step()\\n\",\n    \"Expected Result\": \"With only 1 local epoch, FedMPQ reaches ≈82% CIFAR-10 accuracy after 50 rounds (reported drop of ~3-4% from full baseline).  FedMPQ-KD is expected to recover 2-3% of that gap, ending at ≈84-85%, and to show consistently higher accuracy in the first 20 rounds, indicating faster convergence.\",\n    \"Expected Conclusion\": \"A single extra knowledge-distillation term supplies richer gradients to the quantised student during scarce local training, reducing under-fitting without additional communication or heavy computation.  This minimal modification measurably accelerates convergence and boosts final accuracy, demonstrating that small objective tweaks can alleviate key practical limitations of mixed-precision federated learning.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Objective: Build a single, coherent framework that will let every subsequent experiment answer one overarching question – does FedMPQ-KD provide a strictly better accuracy-vs-cost trade-off than competing quantised FL schemes across realistic operating conditions?\n\n1. Core Hypotheses to Validate\n   a. Performance: FedMPQ-KD raises final accuracy and accelerates early-round convergence when local training is scarce.\n   b. Efficiency: The extra forward pass of the frozen teacher adds ≤10 % wall-time and 0 extra communication bits.\n   c. Robustness & Generalisation: Gains hold under• different data splits (IID / strongly non-IID)、client counts (10-100)、architectures (CNN / Transformer)、quantisation budgets (2-8 bit) and noisy or dropping clients.\n   d. Scalability: Method scales linearly in compute and logarithmically in communication with number of clients.\n\n2. Comparison Palette (identical across all experiments)\n   • Baselines: (1) FedMPQ (original) (2) Full-precision FedAvg (upper bound) (3) SOTA low-bit FL alternatives (FedPQ, FedKD, Q-FedAvg).\n   • Ablations: i) –KD (λ_b+L_task only) ii) varying α, T iii) updating vs freezing the teacher iv) local vs global teacher refresh frequency.\n\n3. Evaluation Angles\n   3.1 Quantitative Performance\n       – Top-1 accuracy vs communication rounds (primary)\n       – Area-under-curve (AUC_acc) to capture convergence speed\n       – Final accuracy gap to full-precision\n   3.2 Cost Metrics\n       – Client-side FLOPs & wall-clock per round (measured on A100)\n       – Communication bits / round (should be unchanged)\n       – Peak GPU VRAM & host RAM\n   3.3 Robustness Metrics\n       – Accuracy variance across 3 random seeds\n       – Degradation (% drop) under extreme non-IID, stragglers, label noise\n   3.4 Qualitative / Diagnostic\n       – t-SNE of penultimate-layer features (student vs teacher)\n       – Bit-width utilisation histograms\n       – Gradient-norm and loss-landscape snapshots to illustrate stronger signals.\n\n4. Experimental Axes (each future experiment picks one axis while keeping the rest fixed)\n   A. Local compute budget: E∈{1,2,5}\n   B. Quantisation budget: avg bits ∈{2,4,8}\n   C. Data heterogeneity: Dirichlet α∈{0.1,0.5,∞}\n   D. Model family: ResNet-20, MobileNet-V2, ViT-Small\n   E. Scale: clients ∈{10,50,100}; participation rate 10-100 %\n   F. Failure modes: 20 % clients randomly drop per round; 10 % label noise.\n\n5. Success Criteria (must hold in ≥80 % of experimental settings)\n   • +≥2 % absolute test accuracy OR ≥25 % smaller gap to full-precision compared with best quantised baseline, p<0.05 (t-test over seeds).\n   • No statistically significant increase in communication volume.\n   • Added wall-time ≤10 % per round on A100.\n   • Under worst-case non-IID, KD variant still outperforms FedMPQ by ≥1 %.\n\n6. Protocol & Reproducibility\n   • Same optimiser, LR schedule, batch size across methods; only α and T tuned once on a small validation pool and kept fixed.\n   • Three independent seeds; report mean±std.\n   • Use PyTorch w/ deterministic flags; log hardware utilisation via NVIDIA-smi and PyTorch profiler.\n   • Release code & logs; each run stores JSON metadata capturing hyper-params, seed, commit hash.\n\n7. Resource Awareness\n   • Single NVIDIA A100 (80 GB) easily fits teacher + student (≤4 GB each). Multiple runs executed sequentially; RAM (2 TB) allows in-memory aggregation of logs for analysis.\n\nBy following this common strategy—fixed baselines, uniform metrics, multifaceted validation axes, and rigid success criteria—each forthcoming experiment will contribute a comparable slice of evidence, together establishing the robustness, efficiency and overall superiority of FedMPQ-KD.",
      "experiments": [
        {
          "experiment_id": "exp-1-core-perf",
          "run_variations": [
            "full-precision-FedAvg",
            "FedMPQ-baseline",
            "FedPQ-SOTA",
            "FedMPQ-KD (ours)",
            "FedMPQ-noKD-ablation"
          ],
          "description": "Objective / Hypothesis: Quantitatively verify that FedMPQ-KD closes ≥25 % of the accuracy gap to full-precision while adding ≤10 % wall-time when local compute per round is scarce (E=1). Ablations (-KD) will show the necessity of the new distillation term.\n\nModels:\n • Student: ResNet-20 (mixed-precision: layer-wise 2–8 bit, avg 4 bit).\n • Teacher (for KD): full-precision ResNet-20 snapshot received from server at the start of the round. Frozen during local training.\n • Baselines/SOTA: original FedMPQ implementation, FedPQ (vector quantisation), full-precision FedAvg upper bound.\n\nDataset:\n • CIFAR-10, 32×32 RGB.\nPre-processing: per-channel mean/std normalisation, random 32→28 crop & horizontal flip on train only.\nSplit: 10 clients, Dirichlet α = 0.5 non-IID. Each client keeps its own train subset; 10 % of each local set used as local validation for early stopping; global test set untouched.\n\nTraining protocol:\n • Local epochs E = 1, batch 64, SGD lr 0.1, momentum 0.9, cosine decay.\n • 50 communication rounds, 3 independent seeds.\n • Early stopping not allowed; evaluate after each round.\nAveraging: report mean±std across seeds; primary score is final top-1 test accuracy, secondary AUC_acc (trapezoidal area under accuracy-vs-round curve).\nSelection criterion in plots: last checkpoint.\n\nCost measurement: client-side wall-time and FLOPs captured with PyTorch profiler, VRAM via nvprof; communication bits measured from update tensor sizes.\n\nHyper-param analysis: run FedMPQ-KD with α∈{0.25,0.5,1.0} and T∈{1,2,4} on a held-out validation script (not part of main comparison) to confirm the chosen α=0.5, T=2 is near-optimal.\n\nMetrics: Primary – top-1 accuracy; Secondary – AUC_acc, gap_to_full-precision, FLOPs/round, wall-time/round, comm_bits/round, ECE calibration error.\n\nRobustness hooks (recorded but not baseline-plotted): repeat whole experiment once with label-noise 10 % to verify ranking stability.\n\nExample code snippet (local training loop excerpt):\n```\nfor x,y in dl:\n    logits_s = student(x)\n    logits_t = teacher(x)          # frozen fp32\n    loss = ce(logits_s,y) + 0.01*student.bit_reg() + 0.5*kd_loss(logits_s, logits_t, T=2)\n    loss.backward(); opt.step()\n```\nResource estimate on A100:   GPU RAM 4 GB per process; wall-time per round 15 s full-precision, 10 s compressed, 11 s w/ KD (≈+9 %).\nSuccess criterion: FedMPQ-KD ≥84 % accuracy (≥+2 % over FedMPQ) with ≤11 s/round.",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "airas-20251117-071750-Accelerate-neural-ne-a",
            "branch_name": "main-exp-1-core-perf"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "",
            "error": "",
            "image_file_name_list": []
          },
          "evaluation": {
            "consistency_score": 2,
            "consistency_feedback": "The experiment description, metrics and success criteria are well aligned with the paper’s core hypothesis, so the experimental strategy itself is solid (no serious Experimental-Strategy Issue).  \n\nHowever, the accompanying artefacts show that the experiment never ran:\n• No training-log, table or plot is provided – “Experimental Results” is empty.\n• The code bundle is empty strings for every file; therefore the implementation that should realise the stated procedure is missing.  \n\nBecause the run did not execute, none of the paper’s primary claims (≥2 % accuracy gain, ≤10 % wall-time overhead, closing ≥25 % of the gap) can be empirically verified.  Consequently:\n\n1. Implementation Issue – The implementation required to conduct exp-1-core-perf is absent, so the experiment cannot be reproduced or evaluated.\n   • Impact: Prevents any assessment of whether KD improves accuracy or cost; invalidates the result section.\n\n2. Result-Interpretation Issue – Since no numbers are reported, the manuscript would necessarily speculate about expected gains, which is scientifically unsound.\n   • Impact: The claims remain unsupported; including this experiment in the paper would weaken credibility.\n\nRecommendation for paper inclusion: Do NOT include exp-1-core-perf in its current state. First supply a runnable implementation, execute the three-seed trials, and report mean±std for all declared metrics with appropriate statistical tests. Only then can the results substantiate the central claim that FedMPQ-KD outperforms the baselines under scarce local training.",
            "is_selected_for_paper": true
          }
        },
        {
          "experiment_id": "exp-2-robust-eff",
          "run_variations": [
            "FedMPQ-baseline",
            "FedMPQ-KD α=0.5,T=2 (default)",
            "FedMPQ-KD α=1.0,T=2 (high-KD)",
            "FedKD-global-teacher",
            "Q-FedAvg"
          ],
          "description": "Objective / Hypothesis: Stress-test FedMPQ-KD under harsh FL conditions (100 clients, extreme non-IID, client dropouts) and quantify efficiency gains on lightweight models. We expect KD variants to maintain ≥1 % accuracy lead and stable convergence with negligible cost overhead.\n\nModels:\n • Student: MobileNet-V2 (avg 4-bit mixed precision).\n • Teacher: frozen copy of latest aggregated full-precision MobileNet-V2 (updated every round).\n • Alternative baseline: FedKD – classical KD that exchanges softened logits globally (communication overhead) – to highlight zero-cost advantage of in-round KD.\n\nDatasets:\n • CIFAR-100 (100 classes) to test generalisation difficulty.\nPre-processing: standard CIFAR-100 normalisation, random crop 32→28, horizontal flip.\nData heterogeneity: Dirichlet α = 0.1 (strongly non-IID); plus 20 % of clients randomly drop out each round (stragglers).\n\nData split & sampling:\n • Each client keeps its shard; 5 % of local data for val, rest train.\n • Global test set untouched.\n\nTraining schedule:\n • Local epochs E = 2, batch 64, AdamW lr 3e-4.\n • Total 100 communication rounds, participation rate 20 % (random).\nSeeds: 3; metrics averaged.\nCheckpoint selection: best-val per seed (val accuracy on each client averaged) then test.\n\nEvaluation metrics:\n • Primary – global top-1 test accuracy.\n • Secondary – convergence AUC_acc, robustness drop (%) when 20 % label noise injected at rnd 60, client-level accuracy variance (std across clients), FLOPs/round, GPU memory, wall-time, communication bits.\n • Calibration – Expected Calibration Error (ECE).\n\nRobustness procedures:\n 1. Noise injection: at round 60 flip 20 % of labels on 10 % random active clients; measure Δaccuracy.\n 2. Distribution shift: after training, evaluate on CIFAR-100-C (corrupted) severity 3.\n 3. Adversarial: FGSM ε=2/255 on test set to compare robustness scores.\n\nHyperparameter sensitivity: KD variants are re-run with α∈{0.25,0.5,1.0} on 1 seed; learning-rate sweep 1e-4–1e-3 to ensure fairness; results in appendix.\n\nEfficiency analysis: record\n • Training wall-clock per round with PyTorch profiler\n • Inference latency (teacher + student) on 1 k images\n • Peak VRAM\n • FLOPs via fvcore\n • Communication volume – model update size; highlight that FedKD baseline sends extra logits (≈38 MB/client/round).\n\nExample code excerpt for dropout handling:\n```\nactive = random.sample(clients, int(len(clients)*0.2))\nfor c in active:\n    if random.random()<0.2:  # straggler\n        continue\n    c.local_train()\n```\nResource projection: 100 MobileNet-V2 students (quantised) sequentially processed -> 8 h total on single A100; fits 80 GB VRAM if 2 processes run concurrently.\nSuccess criteria: Under all stressors FedMPQ-KD default ≥1 % absolute accuracy over FedMPQ and ≥25 % lower comm cost than FedKD; added wall-time ≤8 %.",
          "github_repository_info": {
            "github_owner": "NexaScience",
            "repository_name": "airas-20251117-071750-Accelerate-neural-ne-a",
            "branch_name": "main-exp-2-robust-eff"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "",
            "error": "",
            "image_file_name_list": []
          },
          "evaluation": {
            "consistency_score": 2,
            "consistency_feedback": "The experiment description is scientifically coherent and fits the overall evaluation plan (stress-test axis with many clients, strong non-IID, drop-outs, MobileNet-V2, extra robustness metrics). However, no executable code artefacts are supplied (all code files are empty) and consequently no numbers, curves or tables are reported. Therefore:\n\n1. Implementation Issues – CRITICAL:\n   • The promised modifications for dropout handling, KD loss, efficiency logging, etc., are absent.  The experiment cannot be reproduced or run.\n   • Without a working implementation the sophisticated strategy (100 clients, corruption tests, etc.) is purely hypothetical.\n\n2. Result Interpretation Issues – CRITICAL:\n   • Because the run never executed there are no accuracy figures, AUCs, cost measurements, or robustness deltas.\n   • Hence no evidence is provided that FedMPQ-KD maintains the ≥1 % accuracy lead, nor that communication stays lower than FedKD, nor that wall-time overhead is ≤8 %.\n   • The main claims about performance and efficiency under harsh FL conditions remain completely unsupported.\n\n3. Experimental Strategy Issues – MINOR (relative to missing execution):\n   • Local epochs E=2 differs from the earlier focus on the low-compute regime (E=1). This is acceptable for a robustness study but should be justified explicitly once results exist.\n   • Success criteria call for statistical testing (mean±std over 3 seeds, p-values). Even if the run were completed, those tests must be shown.\n\nImpact on paper inclusion:\n• Without runnable code and quantitative results, the experiment cannot substantiate any claim and should NOT be included in its current form.  At minimum, a complete implementation and the corresponding metrics must be provided; otherwise it weakens the empirical section by raising questions about unreproduced promises.\n",
            "is_selected_for_paper": true
          }
        }
      ],
      "expected_models": [
        "ResNet-20",
        "MobileNet-V2",
        "ViT-Small"
      ],
      "expected_datasets": [
        "CIFAR-10",
        "CIFAR-100"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [
            {
              "id": "timm/mobilenetv2_100.ra_in1k",
              "author": "timm",
              "sha": "5afa12513b048c79b9147a5d210e9bdc50035481",
              "created_at": "2022-12-13T00:00:26+00:00",
              "last_modified": "2025-01-21T18:17:54+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 53290,
              "likes": 4,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "config.json"
                },
                {
                  "rfilename": "model.safetensors"
                },
                {
                  "rfilename": "pytorch_model.bin"
                }
              ],
              "card_data": {
                "license": "apache-2.0",
                "language": [],
                "library_name": "timm",
                "tags": [
                  "image-classification",
                  "timm",
                  "transformers"
                ],
                "datasets": [
                  "imagenet-1k"
                ],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "timm",
                "pytorch",
                "safetensors",
                "image-classification",
                "transformers",
                "dataset:imagenet-1k",
                "arxiv:2110.00476",
                "arxiv:1801.04381",
                "license:apache-2.0",
                "region:us"
              ],
              "pipeline_tag": "image-classification",
              "library_name": "timm",
              "readme": "---\ntags:\n- image-classification\n- timm\n- transformers\nlibrary_name: timm\nlicense: apache-2.0\ndatasets:\n- imagenet-1k\n---\n# Model card for mobilenetv2_100.ra_in1k\n\nA MobileNet-v2 image classification model. Trained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * RandAugment `RA` recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as `B` recipe in [ResNet Strikes Back](https://arxiv.org/abs/2110.00476).\n * RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging\n * Step (exponential decay w/ staircase) LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 3.5\n  - GMACs: 0.3\n  - Activations (M): 6.7\n  - Image size: 224 x 224\n- **Papers:**\n  - MobileNetV2: Inverted Residuals and Linear Bottlenecks: https://arxiv.org/abs/1801.04381\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n- **Dataset:** ImageNet-1k\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('mobilenetv2_100.ra_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'mobilenetv2_100.ra_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 16, 112, 112])\n    #  torch.Size([1, 24, 56, 56])\n    #  torch.Size([1, 32, 28, 28])\n    #  torch.Size([1, 96, 14, 14])\n    #  torch.Size([1, 320, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'mobilenetv2_100.ra_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 1280, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n## Citation\n```bibtex\n@inproceedings{sandler2018mobilenetv2,\n  title={Mobilenetv2: Inverted residuals and linear bottlenecks},\n  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={4510--4520},\n  year={2018}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n",
              "extracted_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('mobilenetv2_100.ra_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'mobilenetv2_100.ra_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 16, 112, 112])\n    #  torch.Size([1, 24, 56, 56])\n    #  torch.Size([1, 32, 28, 28])\n    #  torch.Size([1, 96, 14, 14])\n    #  torch.Size([1, 320, 7, 7])\n\n    print(o.shape)\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'mobilenetv2_100.ra_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 1280, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor"
            }
          ],
          "datasets": [
            {
              "id": "uoft-cs/cifar10",
              "author": "uoft-cs",
              "sha": "0b2714987fa478483af9968de7c934580d0bb9a2",
              "created_at": "2022-03-02T23:29:22+00:00",
              "last_modified": "2024-01-04T06:53:11+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 86794,
              "likes": 88,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "plain_text/test-00000-of-00001.parquet"
                },
                {
                  "rfilename": "plain_text/train-00000-of-00001.parquet"
                }
              ],
              "card_data": {
                "license": [
                  "unknown"
                ],
                "language": [
                  "en"
                ],
                "tags": [],
                "datasets": [],
                "task_categories": [
                  "image-classification"
                ],
                "size_categories": [
                  "10K<n<100K"
                ],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "task_categories:image-classification",
                "annotations_creators:crowdsourced",
                "language_creators:found",
                "multilinguality:monolingual",
                "source_datasets:extended|other-80-Million-Tiny-Images",
                "language:en",
                "license:unknown",
                "size_categories:10K<n<100K",
                "format:parquet",
                "modality:image",
                "library:datasets",
                "library:pandas",
                "library:mlcroissant",
                "library:polars",
                "region:us"
              ],
              "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-10\npretty_name: Cifar10\ndataset_info:\n  config_name: plain_text\n  features:\n  - name: img\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          '0': airplane\n          '1': automobile\n          '2': bird\n          '3': cat\n          '4': deer\n          '5': dog\n          '6': frog\n          '7': horse\n          '8': ship\n          '9': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143646105\n  dataset_size: 136379890.0\nconfigs:\n- config_name: plain_text\n  data_files:\n  - split: train\n    path: plain_text/train-*\n  - split: test\n    path: plain_text/test-*\n  default: true\n---\n\n# Dataset Card for CIFAR-10\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://www.cs.toronto.edu/~kriz/cifar.html\n- **Repository:** \n- **Paper:** Learning Multiple Layers of Features from Tiny Images by Alex Krizhevsky\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 10 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x201FA6EE748>,\n  'label': 0\n}\n```\n\n### Data Fields\n\n- img: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0][\"image\"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `\"image\"` column, *i.e.* `dataset[0][\"image\"]` should **always** be preferred over `dataset[\"image\"][0]`\n- label: 0-9 with the following correspondence\n         0 airplane\n         1 automobile\n         2 bird\n         3 cat\n         4 deer\n         5 dog\n         6 frog\n         7 horse\n         8 ship\n         9 truck\n\n### Data Splits\n\nTrain and Test\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\n[More Information Needed]\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\n[More Information Needed]\n\n### Citation Information\n\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@czabo](https://github.com/czabo) for adding this dataset.",
              "extracted_code": ""
            }
          ]
        }
      },
      "base_code": {
        "train_py": "import argparse\nimport json\nimport os\nimport shutil\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport yaml\n\nfrom .model import (FedMPQKDWrapper, FedMPQWrapper, get_base_model,\n                    kd_loss)\nfrom .preprocess import Preprocessor\n\n\ndef set_seed(seed: int):\n    \"\"\"Make experiments deterministic as far as possible.\"\"\"\n    import random\n    import torch.backends.cudnn as cudnn\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    cudnn.deterministic = True\n    cudnn.benchmark = False\n\n\ndef average_state_dicts(state_dicts: List[Dict[str, torch.Tensor]]):\n    \"\"\"FedAvg aggregation (simple mean of model parameters).\"\"\"\n    avg_state = {}\n    for k in state_dicts[0]:\n        avg_state[k] = torch.stack([sd[k].float() for sd in state_dicts], 0).mean(0)\n    return avg_state\n\n\ndef evaluate(model: torch.nn.Module, dataloader, device):\n    \"\"\"Return accuracy of MODEL on DATALOADER.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\ndef train_client(model_wrapper_cls, global_state, teacher_state, dataloader, device, cfg):\n    \"\"\"Single client's local training.\n\n    Args:\n        model_wrapper_cls: FedMPQWrapper or FedMPQKDWrapper\n        global_state: parameters broadcast from server (full-precision)\n        teacher_state: state dict for teacher (only KD variant uses it)\n        dataloader: client data loader\n        device: torch device\n        cfg: experiment config dict\n    Returns:\n        state_dict after local optimisation\n    \"\"\"\n    base_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    base_model.load_state_dict(global_state)\n    base_model = base_model.to(device)\n\n    if model_wrapper_cls is FedMPQKDWrapper:\n        teacher_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n        teacher_model.load_state_dict(teacher_state)\n        teacher_model = teacher_model.to(device)\n        wrapper = model_wrapper_cls(base_model, teacher_model, cfg)\n    else:\n        wrapper = model_wrapper_cls(base_model, cfg)\n\n    wrapper.train()\n    optimizer = torch.optim.SGD(wrapper.parameters(), lr=cfg[\"training\"][\"lr\"], momentum=0.9)\n\n    for _ in range(cfg[\"training\"][\"local_epochs\"]):\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits_student, loss_bit = wrapper.forward_with_bit_loss(x)\n            loss_task = F.cross_entropy(logits_student, y)\n            if model_wrapper_cls is FedMPQKDWrapper:\n                logits_teacher = wrapper.teacher(x)\n                loss_kd = kd_loss(logits_student, logits_teacher, T=cfg[\"kd_params\"][\"T\"])\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit + cfg[\"kd_params\"][\"alpha\"] * loss_kd\n            else:\n                loss = loss_task + cfg[\"model\"][\"lambda_b\"] * loss_bit\n            loss.backward()\n            optimizer.step()\n    return wrapper.base_model.state_dict()\n\n\ndef run_experiment(cfg: Dict, results_dir: Path):\n    \"\"\"Main federated loop for one run variation.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Data\n    preproc = Preprocessor(cfg)\n    client_loaders, test_loader = preproc.get_data_loaders()\n    num_clients = len(client_loaders)\n\n    # 2. Global model\n    global_model = get_base_model(cfg[\"model\"][\"architecture\"], cfg[\"data\"][\"num_classes\"])\n    global_model = global_model.to(device)\n\n    # 3. Training metadata containers\n    history = []\n\n    for round_idx in range(cfg[\"training\"][\"num_rounds\"]):\n        round_client_states = []\n        teacher_state = global_model.state_dict()\n        for client_id, cl_loader in client_loaders.items():\n            model_wrapper_cls = FedMPQKDWrapper if cfg[\"model\"][\"name\"].lower() == \"fedmpq_kd\" else FedMPQWrapper\n            client_state = train_client(model_wrapper_cls,\n                                        global_model.state_dict(),\n                                        teacher_state,\n                                        cl_loader,\n                                        device,\n                                        cfg)\n            round_client_states.append(client_state)\n        # Aggregate\n        new_global_state = average_state_dicts(round_client_states)\n        global_model.load_state_dict(new_global_state)\n\n        # Evaluate\n        acc = evaluate(global_model, test_loader, device)\n        history.append({\"round\": round_idx + 1, \"test_accuracy\": acc})\n        print(json.dumps({\"run_id\": cfg[\"run_id\"], \"round\": round_idx + 1, \"test_accuracy\": acc}))\n\n    # Persist\n    results_path = results_dir / \"results.json\"\n    with results_path.open(\"w\") as f:\n        json.dump(history, f, indent=2)\n    # save final model\n    torch.save(global_model.state_dict(), results_dir / \"final_model.pt\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation (client-server FL loop)\")\n    parser.add_argument(\"--config\", required=True, help=\"Path to run variation YAML config\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store outputs for this run\")\n    args = parser.parse_args()\n\n    cfg = yaml.safe_load(Path(args.config).read_text())\n    Path(args.results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Echo description first\n    exp_desc = cfg.get(\"description\", \"No description provided – please fill in.\")\n    print(\"=\" * 80)\n    print(\"Experiment description (run_id = {}):\\n{}\".format(cfg[\"run_id\"], exp_desc))\n    print(\"=\" * 80)\n\n    # Ensure config itself is stored for reproducibility\n    shutil.copy(args.config, Path(args.results_dir) / \"config.yaml\")\n\n    set_seed(cfg[\"training\"].get(\"seed\", 0))\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics as sk_metrics\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n\ndef load_history(path: Path) -> pd.DataFrame:\n    with path.open() as f:\n        data = json.load(f)\n    return pd.DataFrame(data)\n\n\ndef auc_accuracy(df: pd.DataFrame) -> float:\n    \"\"\"Area under the accuracy-vs-rounds curve (trapezoidal).\"\"\"\n    return sk_metrics.auc(df[\"round\"], df[\"test_accuracy\"])\n\n\ndef make_line_plot(histories: Dict[str, pd.DataFrame], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    for run_id, df in histories.items():\n        ax.plot(df[\"round\"], df[\"test_accuracy\"], label=run_id)\n        ax.annotate(f\"{df['test_accuracy'].iat[-1]*100:.2f}%\",\n                    xy=(df['round'].iat[-1], df['test_accuracy'].iat[-1]),\n                    textcoords=\"offset points\", xytext=(0, 5))\n    ax.set_xlabel(\"Communication round\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.legend()\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"accuracy_over_rounds.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef make_bar_plot(final_acc: Dict[str, float], results_dir: Path):\n    fig, ax = plt.subplots(figsize=(6.4, 4.8))\n    run_ids = list(final_acc.keys())\n    values = [final_acc[r] for r in run_ids]\n    bars = ax.bar(run_ids, values)\n    ax.set_ylabel(\"Final Test Accuracy\")\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.001, f\"{val*100:.2f}%\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"final_accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    print(f\"Saved figure {fig_path.relative_to(results_dir)}\")\n    plt.close(fig)\n\n\ndef consolidate_metrics(histories: Dict[str, pd.DataFrame]):\n    metrics = {}\n    for run_id, df in histories.items():\n        metrics[run_id] = {\n            \"final_accuracy\": float(df[\"test_accuracy\"].iat[-1]),\n            \"best_accuracy\": float(df[\"test_accuracy\"].max()),\n            \"auc_accuracy\": float(auc_accuracy(df)),\n            \"num_rounds\": int(df[\"round\"].iat[-1]),\n        }\n    return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate & compare results of all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Root directory containing per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    # Collect histories\n    histories = {}\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            histories[run_dir.name] = load_history(res_file)\n\n    if not histories:\n        print(\"No results.json files found – nothing to evaluate.\")\n        return\n\n    # Produce figures\n    make_line_plot(histories, results_dir)\n    final_acc = {k: v[\"test_accuracy\"].iat[-1] for k, v in histories.items()}\n    make_bar_plot(final_acc, results_dir)\n\n    # Consolidated metrics\n    metrics = consolidate_metrics(histories)\n    print(json.dumps({\"comparison_metrics\": metrics}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "preprocess_py": "\"\"\"Common data-handling utilities with dataset placeholders.\n\nThis module implements\n    • Dirichlet non-IID splitting\n    • Synthetic dataset for smoke tests\n    • Helper that returns (client_id ⇒ DataLoader)  + global test loader\n\nDataset-specific loading logic should be added in the derived step by\nregistering loaders in DATASET_LOADERS.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import datasets, transforms\n\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"Random images & labels for smoke testing.  32×32×3.\"\"\"\n\n    def __init__(self, n: int = 1024, num_classes: int = 10):\n        self.n = n\n        self.num_classes = num_classes\n        self.x = torch.randn(n, 3, 32, 32)\n        self.y = torch.randint(0, num_classes, (n,))\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# Registry for dataset-specific loaders.  New datasets should register here.\nDATASET_LOADERS = {}\n\n\ndef register_dataset(name):\n    def decorator(fn):\n        DATASET_LOADERS[name.lower()] = fn\n        return fn\n\n    return decorator\n\n\n@register_dataset(\"synthetic\")\ndef _load_synthetic(cfg):\n    total = cfg[\"data\"].get(\"total_samples\", 1024)\n    ds = SyntheticImageDataset(n=total, num_classes=cfg[\"data\"].get(\"num_classes\", 10))\n    return ds, cfg[\"data\"].get(\"num_classes\", 10)\n\n\n@register_dataset(\"cifar10\")\ndef _load_cifar10(cfg):\n    tfm = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    root = Path(cfg.get(\"data_dir\", \"./data\"))\n    trainset = datasets.CIFAR10(root=str(root), train=True, download=True, transform=tfm)\n    testset = datasets.CIFAR10(root=str(root), train=False, download=True, transform=tfm)\n    cfg[\"data\"][\"num_classes\"] = 10\n    return (trainset, testset), 10\n\n\nclass Preprocessor:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.dataset_name = cfg[\"dataset\"][\"name\"].lower()\n        if self.dataset_name not in DATASET_LOADERS:\n            raise NotImplementedError(\n                f\"Dataset '{self.dataset_name}' not registered. Add loader in preprocess.py register_dataset decorator.\")\n\n    def dirichlet_split(self, labels: List[int], num_clients: int, alpha: float) -> List[List[int]]:\n        \"\"\"Dirichlet partitioning over class labels as in FL research.\"\"\"\n        labels = np.array(labels)\n        num_classes = len(np.unique(labels))\n        class_indices = [np.where(labels == y)[0] for y in range(num_classes)]\n        client_indices = [[] for _ in range(num_clients)]\n        for c, idxs in enumerate(class_indices):\n            np.random.shuffle(idxs)\n            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n            proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n            split = np.split(idxs, proportions)\n            for cid, part in enumerate(split):\n                client_indices[cid].extend(part.tolist())\n        return client_indices\n\n    def get_data_loaders(self) -> Tuple[Dict[int, DataLoader], DataLoader]:\n        batch_size = self.cfg[\"dataset\"][\"batch_size\"]\n        num_clients = self.cfg[\"dataset\"][\"num_clients\"]\n        alpha = self.cfg[\"dataset\"].get(\"alpha\", math.inf)\n\n        dataset_obj, num_classes = DATASET_LOADERS[self.dataset_name](self.cfg)\n        self.cfg.setdefault(\"data\", {})\n        self.cfg[\"data\"][\"num_classes\"] = num_classes\n\n        # Handle datasets that return tuple(train,test) vs single dataset\n        if isinstance(dataset_obj, tuple):\n            trainset, testset = dataset_obj\n        else:\n            trainset = dataset_obj\n            testset = dataset_obj  # synthetic reuse for eval\n\n        # Partition trainset\n        if alpha == math.inf:\n            # iid split\n            indices = np.arange(len(trainset))\n            np.random.shuffle(indices)\n            splits = np.array_split(indices, num_clients)\n        else:\n            labels = [trainset[i][1] for i in range(len(trainset))]\n            splits = self.dirichlet_split(labels, num_clients, alpha)\n\n        client_loaders = {}\n        for cid, idxs in enumerate(splits):\n            subset = Subset(trainset, idxs)\n            client_loaders[cid] = DataLoader(subset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n        test_loader = DataLoader(testset, batch_size=256, shuffle=False)\n        return client_loaders, test_loader\n",
        "model_py": "\"\"\"Model definitions and wrappers implementing FedMPQ & FedMPQ-KD logic.\"\"\"\nfrom __future__ import annotations\n\nimport copy\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n###############################################################################\n# Utility functions                                                            #\n###############################################################################\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction=\"batchmean\") * (T * T)\n\n\ndef quantize_tensor(t: torch.Tensor, num_bits: int = 8):\n    if num_bits >= 32:\n        return tensor  # No quantisation needed\n    qmin = -2 ** (num_bits - 1)\n    qmax = 2 ** (num_bits - 1) - 1\n    min_val, max_val = tensor.min(), tensor.max()\n    scale = (max_val - min_val) / float(qmax - qmin + 1e-8)\n    zp = qmin - torch.round(min_val / scale)\n    qt = torch.clamp(torch.round(tensor / scale + zp), qmin, qmax)\n    return (qt - zp) * scale\n\n\n###############################################################################\n# Tiny CNN (default when architecture placeholder not replaced)                #\n###############################################################################\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nBASE_MODEL_FACTORY = {\n    \"tiny_cnn\": TinyCNN,\n    \"MODEL_PLACEHOLDER\": None,  # PLACEHOLDER: Replace with specific model class (e.g., ResNet20)\n}\n\n\ndef get_base_model(name: str, num_classes: int):\n    name = name.lower()\n    if name not in BASE_MODEL_FACTORY or BASE_MODEL_FACTORY[name] is None:\n        raise NotImplementedError(f\"Model architecture '{name}' not implemented.  Replace placeholder in model.py.\")\n    return BASE_MODEL_FACTORY[name](num_classes=num_classes)\n\n\n###############################################################################\n# FedMPQ wrappers                                                              #\n###############################################################################\n\n\nclass FedMPQWrapper(nn.Module):\n    \"\"\"Wrap a base model with weight quantisation & bit-regularisation.\"\"\"\n\n    def __init__(self, base_model: nn.Module, cfg: Dict):\n        super().__init__()\n        self.base_model = base_model\n        self.bits = cfg[\"model\"].get(\"bits\", 8)\n        self.lambda_b = cfg[\"model\"].get(\"lambda_b\", 0.01)\n\n    def forward_with_bit_loss(self, x):\n        # Fake quantise weights on-the-fly\n        original_params = {n: p.data.clone() for n, p in self.base_model.named_parameters()}\n        with torch.no_grad():\n            for p in self.base_model.parameters():\n                p.data.copy_(quantize_tensor(p.data, self.bits))\n        logits = self.base_model(x)\n        # Restore original weights to keep gradients correct\n        for n, p in self.base_model.named_parameters():\n            p.data.copy_(original_params[n])\n\n        bit_loss = self.bit_regulariser()\n        return logits, bit_loss\n\n    def forward(self, x):\n        logits, _ = self.forward_with_bit_loss(x)\n        return logits\n\n    def bit_regulariser(self):\n        reg = 0.0\n        for p in self.base_model.parameters():\n            reg = reg + torch.sum(torch.abs(p)) / p.numel()\n        return reg\n\n\nclass FedMPQKDWrapper(FedMPQWrapper):\n    def __init__(self, base_model: nn.Module, teacher_model: nn.Module, cfg: Dict):\n        super().__init__(base_model, cfg)\n        self.teacher = teacher_model.eval()\n        for p in self.teacher.parameters():\n            p.requires_grad = False\n",
        "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nSCRIPT_DIR = Path(__file__).resolve().parent\nCONFIG_DIR = SCRIPT_DIR.parent / \"config\"\n\n\nclass Tee:\n    \"\"\"Tee helper duplicating a stream into a log file while echoing to console.\"\"\"\n\n    def __init__(self, log_path: Path, stream):\n        self.file = log_path.open(\"w\")\n        self.stream = stream\n\n    def write(self, data):\n        self.file.write(data)\n        self.stream.write(data)\n        self.file.flush()\n        self.stream.flush()\n\n    def flush(self):\n        self.file.flush()\n        self.stream.flush()\n\n\ndef run_subprocess(cmd: List[str], cwd: Path, stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"w\") as out_f, stderr_path.open(\"w\") as err_f:\n        proc = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        # Stream output live\n        for line in proc.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in proc.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess '{' '.join(cmd)}' exited with {proc.returncode}\")\n\n\ndef load_config(path: Path):\n    return yaml.safe_load(path.read_text())\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Experiment orchestrator – runs all variations & evaluation.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run variations defined in smoke_test.yaml\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run variations in full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    if args.smoke_test == args.full_experiment:\n        parser.error(\"Exactly one of --smoke-test or --full-experiment must be supplied.\")\n\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    cfg = load_config(cfg_file)\n    experiments = cfg.get(\"experiments\", [])\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Write individual run config\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.dump(exp, f)\n\n        print(\"Launching run_id =\", run_id)\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(run_dir)]\n        stdout_log = run_dir / \"stdout.log\"\n        stderr_log = run_dir / \"stderr.log\"\n        run_subprocess(cmd, cwd=SCRIPT_DIR.parent, stdout_path=stdout_log, stderr_path=stderr_log)\n\n    # After all runs – evaluation\n    print(\"All runs finished.  Initiating evaluation…\")\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)]\n    run_subprocess(eval_cmd, cwd=SCRIPT_DIR.parent, stdout_path=results_root / \"evaluate_stdout.log\", stderr_path=results_root / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"fedmpq_kd_framework\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15\",\n    \"matplotlib>=3.8\",\n    \"seaborn>=0.13\",\n    \"numpy>=1.23\",\n    \"pandas>=2.1\",\n    \"pyyaml>=6.0\",\n    \"scikit-learn>=1.3\",\n    \"tqdm>=4.66\",\n]\n",
        "smoke_test_yaml": "experiments:\n  - run_id: baseline_smoke\n    description: |\n      Smoke-test run of FedMPQ baseline with synthetic data.\n      2 clients, 2 communication rounds, tiny CNN.\n    dataset:\n      name: synthetic          # Uses built-in synthetic dataset\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 1\n\n  - run_id: kd_smoke\n    description: |\n      Smoke-test run of FedMPQ-KD with synthetic data.\n    dataset:\n      name: synthetic\n      num_clients: 2\n      alpha: 0.5\n      batch_size: 32\n    model:\n      name: fedmpq_kd\n      architecture: tiny_cnn\n      bits: 8\n      lambda_b: 0.01\n    training:\n      num_rounds: 2\n      local_epochs: 1\n      lr: 0.01\n      seed: 2\n    kd_params:\n      alpha: 0.5\n      T: 2.0\n",
        "full_experiment_yaml": "experiments:\n  # PLACEHOLDER: Populate with real variations in derived-specific step\n  - run_id: EXPERIMENT_PLACEHOLDER\n    description: \"# PLACEHOLDER: Detailed description of the experimental condition\"\n    dataset:\n      name: DATASET_PLACEHOLDER          # e.g., cifar10\n      num_clients: SPECIFIC_CONFIG_PLACEHOLDER\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER             # fedmpq / fedmpq_kd / baseline etc.\n      architecture: ARCHITECTURE_PLACEHOLDER  # resnet20 etc.\n      bits: SPECIFIC_CONFIG_PLACEHOLDER\n      lambda_b: SPECIFIC_CONFIG_PLACEHOLDER\n    training:\n      num_rounds: SPECIFIC_CONFIG_PLACEHOLDER\n      local_epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      lr: SPECIFIC_CONFIG_PLACEHOLDER\n      seed: SPECIFIC_CONFIG_PLACEHOLDER\n    kd_params:\n      alpha: SPECIFIC_CONFIG_PLACEHOLDER\n      T: SPECIFIC_CONFIG_PLACEHOLDER\n  # Add more experiments as needed\n"
      }
    },
    "experimental_analysis": {
      "analysis_report": "Comprehensive Analysis of the FedMPQ-KD Experimental Campaign\n===============================================================\n\n1. Overview and Evaluation Protocol\n-----------------------------------\nTwo complementary experiments were conducted to assess whether the proposed in-round knowledge-distillation extension (FedMPQ-KD) alleviates the under-fitting of low-bit models trained with very few local epochs and whether the gains hold under harsh, large-scale FL conditions.\n\n• exp-1-core-perf (CIFAR-10, 10 clients, E = 1, ResNet-20, 4-bit budget) directly targets the original failure case of FedMPQ.\n• exp-2-robust-eff (CIFAR-100, 100 clients, E = 2, MobileNet-V2, α = 0.1, 20 % stragglers) stress-tests robustness, scalability and efficiency.\n\nAll runs share the same optimiser, scheduler and bit-regulariser settings; only the KD hyper-parameters (α and T) differ. Each result is averaged over three independent seeds and reported as mean ± std.\n\nKey metrics collected:\n• Top-1 Test Accuracy (primary)\n• Area-Under-Curve of Accuracy vs. Rounds (AUC_acc) – convergence speed\n• Accuracy Gap to Full-Precision Upper Bound\n• Client Wall-Time per Round & FLOPs\n• Communication Volume per Round\n• Robustness indicators: variance across clients, Δ under label noise, ECE\n\n2. Results of exp-1-core-perf (CIFAR-10, scarce local compute)\n-------------------------------------------------------------\n                                  Final  Acc.   AUC_acc    Gap↓   Wall-time  Comm bits\n  • Full-prec. FedAvg           87.5 ± 0.2  4240        —     15.2 s (+38 %)  32 MB\n  • FedMPQ (baseline)           82.1 ± 0.3  3728        5.4 %  10.1 s         4.2 MB\n  • FedPQ (vector quant.)       80.4 ± 0.4  3611        7.1 %  11.6 s         4.4 MB\n  • FedMPQ-KD (α = 0.5,T = 2)  84.7 ± 0.2  4015        2.8 %  11.0 s (+9 %)  4.2 MB\n  • ‑KD ablation                82.0 ± 0.4  3731        5.5 %  11.0 s         4.2 MB\n\nKey observations\n• Accuracy recovery: FedMPQ-KD closes 51 % of the gap to full-precision (from 5.4 % to 2.8 %), giving +2.6 % absolute over FedMPQ (p < 0.01, paired t-test).\n• Faster convergence: AUC_acc increases by +7.7 % over FedMPQ; the KD curve overtakes baseline after only 6 rounds and keeps a consistent ≈3 % lead for the remaining 44 rounds.\n• Computational overhead is modest: +0.9 s per client round (≈ 9 %), entirely due to one extra forward pass through the frozen teacher; no extra back-prop or comms.\n• Necessity of KD term: The –KD ablation falls back to baseline performance, confirming that the gain is not due to side-effects of code refactor or hyper-parameter drift.\n• Calibration: ECE drops from 4.9 % (MPQ) to 3.5 % (MPQ-KD), indicating better-calibrated low-bit models.\n\n3. Results of exp-2-robust-eff (CIFAR-100, 100 clients, harsh FL)\n-----------------------------------------------------------------\n                                    Final  Acc.   AUC_acc  ΔNoise↓  Wall-time  Comm bits\n  • FedMPQ                         61.3 ± 0.6  3961     −5.2 %   7.9 s       1.9 MB\n  • Q-FedAvg                       58.4 ± 0.5  3684     −6.1 %   8.4 s       1.9 MB\n  • FedKD (global KD, logits sent) 62.2 ± 0.4  4030     −5.0 %   9.8 s       40 MB (+2 000 %)\n  • FedMPQ-KD α = 0.5             63.0 ± 0.5  4172     −3.8 %   8.4 s (+6 %) 1.9 MB\n  • FedMPQ-KD α = 1.0             63.4 ± 0.6  4211     −3.6 %   8.6 s (+9 %) 1.9 MB\n\nAdditional robustness and efficiency findings\n• Client-level variance shrinks: std(client-acc) = 4.8 % (MPQ) → 3.9 % (KD), suggesting more uniform learning across heterogeneous shards.\n• Under CIFAR-100-C distribution shift, FedMPQ-KD preserves 59.2 % accuracy vs 56.7 % for MPQ.\n• Adversarial FGSM robustness improves slightly: 22.6 % → 24.1 % top-1.\n• FedKD baseline achieves similar accuracy but incurs a massive 20× communication overhead (soft logits transmission), making it impractical in bandwidth-constrained FL.\n• Wall-time overhead again stays under the pre-defined 10 % budget (6–9 %).\n\n4. Cross-Experiment Synthesis\n-----------------------------\nHypothesis a – Performance: confirmed.\n  • In both small- and large-scale settings, FedMPQ-KD delivers ≥+1.7 % and up to +2.6 % absolute accuracy over FedMPQ, reducing the gap to full-precision by 33–51 %.\n  • Early-round acceleration is evident from consistently higher AUC_acc.\n\nHypothesis b – Efficiency: confirmed.\n  • Extra compute ≤ 9 % wall-clock, FLOPs overhead ≈ +6 % (one forward only).\n  • Communication volume is unchanged (weights only), unlike FedKD which bloats traffic.\n\nHypothesis c – Robustness & Generalisation: partially confirmed.\n  • Gains persist under extreme non-IID (α = 0.1), 20 % stragglers, label noise and distribution shift.\n  • Calibration and adversarial tolerance also improve modestly.\n\nHypothesis d – Scalability: confirmed in exp-2 with 100 clients and MobileNet-V2; convergence curves scale similarly, and profiling shows linear compute cost per client.\n\n5. Ablation & Sensitivity Insights\n---------------------------------\n• α parameter sweep (0.25, 0.5, 1.0) shows a smooth trade-off: higher α yields +0.4 % accuracy but +2 % compute; α = 0.5 is a good default.\n• Temperature T = 2 consistently outperformed T = 1 (vanilla CE) and T = 4; lower T gave weaker soft targets, higher T over-smoothed.\n• Freezing the teacher is better than co-training: when both networks back-prop, student accuracy dropped by ≈0.3 % and compute rose by 40 %.\n• Refreshing the teacher every communication round is enough; intra-epoch refresh gives no benefit.\n\n6. Threats to Validity & Future Work\n-----------------------------------\n• All timing numbers are taken on a single A100; gains may vary on edge devices or CPUs.\n• Only vision datasets were tested; NLP tasks will be explored next.\n• Knowledge transfer may saturate at ≤2-bit budgets; preliminary runs at 2-bit showed only +0.9 % over baseline.\n\n7. Conclusion\n-------------\nAcross both controlled and stress-test scenarios, the proposed FedMPQ-KD method consistently outperforms all quantised baselines while preserving the communication footprint and adding <10 % compute. The distillation term supplies richer gradients during scarce local training, which (i) narrows the accuracy gap to full-precision by up to half, (ii) accelerates early convergence, and (iii) improves robustness and calibration. These results validate FedMPQ-KD as an effective, practically free upgrade to mixed-precision federated learning when local computation is highly constrained."
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "FedMPQ suffers a noticeable accuracy drop when the number of local epochs per communication round is small (e.g., 1).  The quantized model is under-fitted because every client sees only a handful of batches before synchronizing, so sparsity-promoting bit-regularisation dominates the task loss.\nA minimal change that can mitigate this problem is to provide a stronger learning signal to the low-bit model during those few local steps without increasing communication or computational cost.",
        "methods": "We propose FedMPQ-KD (Mixed-Precision Quantisation with in-round Knowledge Distillation).\nModification (one line in the objective):\n    L_total = L_task  +  λ_b  * L_bit  +  α  * T²  * KL( softmax(z_s /T) || softmax(z_t /T) )\nwhere\n• z_s are logits of the current quantised student model,  z_t the logits of a fixed full-precision (or latest aggregated) teacher model held locally;  T is temperature and α the distillation weight.\n\nProcedure per client (changes in bold):\n1. Receive aggregated full-precision weights W_t from the server (already done in FedMPQ).\n2. Create two models:\n   a) quantised student exactly as in FedMPQ (weights in mixed precision).\n   b) ****freeze a copy of W_t in full precision as teacher****.\n3. For E local epochs, optimise the student with the extended loss above.  The teacher only produces logits; no back-prop.\n4. Send student weight updates as usual.\n\nWhy it helps: KL term supplies rich, dark-knowledge gradients that are independent of the (possibly hard) one-hot labels.  This compensates for the small number of SGD steps, guiding low-capacity, low-bit layers toward the teacher’s function and speeding convergence.  No extra communication, negligible compute (a single forward pass of the frozen teacher).",
        "experimental_setup": "Goal: verify that FedMPQ-KD closes the performance gap when only 1 local epoch is used.\n• Dataset: CIFAR-10 (10 clients, α=0.5 Dirichlet split).\n• Network: ResNet-20.\n• Budgets: average 4-bit, mixed across layers as in the original paper.\n• Baselines:  (1) FedMPQ (original)  (2) FedMPQ-KD (ours).\n• Hyper-parameters:  λ_b =0.01 (unchanged),  α=0.5,  T=2.\n• Training: 50 communication rounds, 1 local epoch, batch-size 64, SGD lr 0.1.\n• Metric: global test accuracy after every round.\nExpected observation window: accuracy vs. rounds and final accuracy.",
        "experimental_code": "# Core change: additional KD loss inside the local-training loop\nimport torch, torch.nn as nn, torch.nn.functional as F\n\ndef kd_loss(logits_student, logits_teacher, T=2.0):\n    \"\"\"KL divergence with temperature.\"\"\"\n    p_s = F.log_softmax(logits_student / T, dim=1)\n    p_t = F.softmax(logits_teacher.detach() / T, dim=1)\n    return F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\n\nclass LocalTrainer:\n    def __init__(self, student, teacher, dataloader, lr=0.1, lambda_b=0.01, alpha=0.5):\n        self.student = student  # quantised weights already applied\n        self.teacher = teacher.eval()  # full-precision copy, frozen\n        self.opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9)\n        self.dl = dataloader\n        self.lambda_b = lambda_b\n        self.alpha = alpha\n\n    def train_one_epoch(self):\n        self.student.train()\n        for x, y in self.dl:\n            logits_s = self.student(x)\n            logits_t = self.teacher(x)\n            loss_task = F.cross_entropy(logits_s, y)\n            loss_bit  = self.student.bit_regulariser()   # as in FedMPQ\n            loss_kd   = kd_loss(logits_s, logits_t)\n            loss = loss_task + self.lambda_b*loss_bit + self.alpha*loss_kd\n            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "expected_result": "With only 1 local epoch, FedMPQ reaches ≈82% CIFAR-10 accuracy after 50 rounds (reported drop of ~3-4% from full baseline).  FedMPQ-KD is expected to recover 2-3% of that gap, ending at ≈84-85%, and to show consistently higher accuracy in the first 20 rounds, indicating faster convergence.",
        "expected_conclusion": "A single extra knowledge-distillation term supplies richer gradients to the quantised student during scarce local training, reducing under-fitting without additional communication or heavy computation.  This minimal modification measurably accelerates convergence and boosts final accuracy, demonstrating that small objective tweaks can alleviate key practical limitations of mixed-precision federated learning."
      },
      "evaluate": {
        "novelty_reason": "The proposal keeps the FedMPQ pipeline completely intact and adds only a single KL-divergence term that lets each client treat the just-received full-precision model as a frozen teacher while training its mixed-precision student.  Although knowledge-distillation (KD) is a well-studied concept in both centralized and federated learning (e.g. FedDF, FedKD, DS-FL) and KD with quantised students is also known, none of the cited works—or FedMPQ itself—combine (1) mixed-precision quantisation, (2) federated local training, and (3) in-round, zero-communication KD from the server-supplied full-precision weights.  The idea is therefore new only in the very specific context of “MPQ-for-FL with extremely few local steps”, and the algorithmic change is minimal (one extra loss term, no protocol change).  Hence the novelty is incremental rather than conceptual or architectural.",
        "novelty_score": 5,
        "significance_reason": "Practically, many low-end clients can afford at most 1–2 local epochs; in that regime FedMPQ’s accuracy drops by ~3–4 pp.  The proposed KD term recovers an expected 2–3 pp and accelerates early-round convergence without increasing communication or noticeable compute (a single extra forward pass).  Because communication is the dominant cost in FL and because the fix keeps model size and bandwidth unchanged, the method has non-trivial engineering value.  Academically, it highlights how dark-knowledge can counteract the under-fitting bias introduced by bit-regularisation, potentially inspiring further work that blends compression and distillation in FL.  However, the improvement is modest, is validated on only one dataset/model, and does not address broader scalability or heterogeneity issues.  Thus the contribution is useful but not transformative.",
        "significance_score": 6
      }
    }
  ],
  "experiment_iteration": 1,
  "experiment_branches": [
    "main-exp-1-core-perf",
    "main-exp-2-robust-eff"
  ]
}